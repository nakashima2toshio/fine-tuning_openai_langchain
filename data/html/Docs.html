<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <link href="./favicon.png" rel="icon"/>
  <meta content="width=device-width" name="viewport"/>
  <link href="./_app/immutable/assets/0.uy2Fq64E.css" rel="stylesheet"/>
  <link href="./_app/immutable/assets/4.FWr7QTh8.css" rel="stylesheet"/>
  <link href="./_app/immutable/entry/start.DeGkUWpD.js" rel="modulepreload"/>
  <link href="./_app/immutable/chunks/entry.B_cgZbs-.js" rel="modulepreload"/>
  <link href="./_app/immutable/chunks/scheduler.CaqlsyK1.js" rel="modulepreload"/>
  <link href="./_app/immutable/entry/app.zeLHmYJV.js" rel="modulepreload"/>
  <link href="./_app/immutable/chunks/13.Ra4u4O7Y.js" rel="modulepreload"/>
  <link href="./_app/immutable/chunks/index.B6TI4_Zl.js" rel="modulepreload"/>
  <link href="./_app/immutable/nodes/0.ckPRERIZ.js" rel="modulepreload"/>
  <link href="./_app/immutable/chunks/index.iXbarjOM.js" rel="modulepreload"/>
  <link href="./_app/immutable/chunks/CopyButton.CumoJvrQ.js" rel="modulepreload"/>
  <link href="./_app/immutable/chunks/copy.DZUZlfk6.js" rel="modulepreload"/>
  <link href="./_app/immutable/nodes/2.CuUnvyZm.js" rel="modulepreload"/>
  <link href="./_app/immutable/nodes/4.I6MMIuYH.js" rel="modulepreload"/>
  <link href="./_app/immutable/chunks/each.DM4g3a_t.js" rel="modulepreload"/>
  <link href="./_app/immutable/chunks/MetaTags.BxA3G-ni.js" rel="modulepreload"/>
  <link href="./_app/immutable/chunks/stores.CCdqwuXp.js" rel="modulepreload"/>
  <link href="./_app/immutable/chunks/VersionDropdown.C2g0aReR.js" rel="modulepreload"/>
  <link href="./_app/immutable/chunks/version.BqEZdzj3.js" rel="modulepreload"/>
  <title>
   Gradio Documentation
  </title>
  <!-- HEAD_svelte-1a3766n_START -->
  <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&amp;display=swap" rel="stylesheet"/>
  <script async="" data-svelte-h="svelte-28fai2" src="https://www.googletagmanager.com/gtag/js?id=UA-156449732-1">
  </script>
  <script data-svelte-h="svelte-iepcrk">
   window.dataLayer = window.dataLayer || [];
		function gtag() {
			dataLayer.push(arguments);
		}
		gtag("js", new Date());
		gtag("config", "UA-156449732-1", {
			cookie_flags: "samesite=none;secure"
		});
  </script>
  <!-- HEAD_svelte-1a3766n_END -->
  <!-- HEAD_svelte-dkutrw_START -->
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1" name="viewport"/>
  <meta content="Documentation, tutorials and guides for the Gradio ecosystem." name="description"/>
  <meta content="Gradio Team" name="author"/>
  <meta content="Gradio Documentation" property="og:title"/>
  <meta content="website" property="og:type"/>
  <meta content="/docs" property="og:url"/>
  <meta content="Documentation, tutorials and guides for the Gradio ecosystem." property="og:description"/>
  <meta content="https://raw.githubusercontent.com/gradio-app/gradio/main/js/_website/src/lib/assets/img/header-image.jpg" property="og:image"/>
  <meta content="summary_large_image" name="twitter:card"/>
  <meta content="@Gradio" name="twitter:creator"/>
  <meta content="Gradio Documentation" name="twitter:title"/>
  <meta content="Documentation, tutorials and guides for the Gradio ecosystem.." name="twitter:description"/>
  <meta content="https://raw.githubusercontent.com/gradio-app/gradio/main/js/_website/src/lib/assets/img/header-image.jpg" name="twitter:image"/>
  <link href="/docs" rel="canonical"/>
  <!-- HEAD_svelte-dkutrw_END -->
  <!-- HEAD_svelte-kspch6_START -->
  <script src="https://gradio.s3-us-west-2.amazonaws.com/4.36.1/gradio.js" type="module">
  </script>
  <!-- HEAD_svelte-kspch6_END -->
 </head>
 <body data-sveltekit-preload-data="hover" style="min-height: 100vh; display: grid; grid-template-rows: auto 1fr auto">
  <div style="display: contents">
   <div class="main-header flex-row">
    <div class="relative isolate flex items-center gap-x-6 overflow-hidden bg-gradient-to-r from-white via-yellow-200 to-white px-6 py-2 sm:px-3.5 sm:before:flex-1 mx-auto" data-svelte-h="svelte-m604za">
     <div class="flex flex-wrap items-center gap-x-4 gap-y-2 flex-grow">
      <div class="flex flex-wrap items-center gap-x-4 gap-y-2 mx-auto">
       <p class="text-md leading-6 text-gray-700 text-center mx-auto">
        <strong class="font-semibold">
         Introducing Gradio Clients
        </strong>
       </p>
       <a class="mx-auto flex-none rounded-full px-3.5 py-1 text-sm font-semibold text-white bg-gradient-to-br from-orange-300 via-orange-500 to-orange-300 hover:drop-shadow-md" href="https://www.youtube.com/watch?v=44vi31hehw4" target="_blank">
        Watch
        <span aria-hidden="true">
         ‚Üí
        </span>
       </a>
      </div>
     </div>
     <div class="hidden justify-end flex-grow sm:flex">
     </div>
    </div>
    <div class="container mx-auto flex flex-wrap justify-between flex-row relative items-center px-4 py-5 gap-6 text-lg z-50">
     <a data-svelte-h="svelte-72mfco" href="/">
      <img alt="Gradio logo" src="/_app/immutable/assets/gradio.CHB5adID.svg"/>
     </a>
     <svg class="h-8 w-8 lg:hidden" viewbox="-10 -10 20 20">
      <rect height="2" width="14" x="-7" y="-6">
      </rect>
      <rect height="2" width="14" x="-7" y="-1">
      </rect>
      <rect height="2" width="14" x="-7" y="4">
      </rect>
     </svg>
     <nav class="w-full flex-col gap-3 lg:flex lg:w-auto lg:flex-row lg:gap-8 hidden">
      <a class="thin-link flex items-center gap-3" data-svelte-h="svelte-12q4mb7" href="/guides/quickstart">
       <span>
        ‚ö°
       </span>
       <span>
        Quickstart
       </span>
      </a>
      <a class="thin-link flex items-center gap-3" data-svelte-h="svelte-rj6sly" href="/docs">
       <span>
        ‚úçÔ∏è
       </span>
       <span>
        Docs
       </span>
      </a>
      <a class="thin-link flex items-center gap-3" data-svelte-h="svelte-b4pfpk" href="/playground">
       <span>
        üé¢
       </span>
       <span>
        Playground
       </span>
      </a>
      <a class="thin-link flex items-center gap-3" data-svelte-h="svelte-1wdzyuc" href="/custom-components/gallery">
       <span>
        üñºÔ∏è
       </span>
       <span>
        Custom Components
        <sup class="text-orange-500">
         NEW
        </sup>
       </span>
      </a>
      <div class="group relative flex cursor-pointer items-center gap-3">
       <span data-svelte-h="svelte-v9wlbr">
        üñê
       </span>
       <span data-svelte-h="svelte-1ufc5dh">
        Community
       </span>
       <svg class="h-4 w-4" viewbox="0 0 20 20" xmlns="http://www.w3.org/2000/svg">
        <path d="M9.293 12.95l.707.707L15.657 8l-1.414-1.414L10 10.828 5.757 6.586 4.343 8z">
        </path>
       </svg>
      </div>
      <a class="thin-link flex items-center gap-3" data-svelte-h="svelte-14vdzms" href="https://github.com/gradio-app/gradio">
       <img alt="Github logo" class="w-6" src="data:image/svg+xml,%3c?xml%20version='1.0'%20encoding='iso-8859-1'?%3e%3c!--%20Generator:%20Adobe%20Illustrator%2016.0.0,%20SVG%20Export%20Plug-In%20.%20SVG%20Version:%206.00%20Build%200)%20--%3e%3c!DOCTYPE%20svg%20PUBLIC%20'-//W3C//DTD%20SVG%201.1//EN'%20'http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd'%3e%3csvg%20version='1.1'%20id='Capa_1'%20xmlns='http://www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%20x='0px'%20y='0px'%20width='438.549px'%20height='438.549px'%20viewBox='0%200%20438.549%20438.549'%20style='enable-background:new%200%200%20438.549%20438.549;'%20xml:space='preserve'%3e%3cg%3e%3cpath%20d='M409.132,114.573c-19.608-33.596-46.205-60.194-79.798-79.8C295.736,15.166,259.057,5.365,219.271,5.365%20c-39.781,0-76.472,9.804-110.063,29.408c-33.596,19.605-60.192,46.204-79.8,79.8C9.803,148.168,0,184.854,0,224.63%20c0,47.78,13.94,90.745,41.827,128.906c27.884,38.164,63.906,64.572,108.063,79.227c5.14,0.954,8.945,0.283,11.419-1.996%20c2.475-2.282,3.711-5.14,3.711-8.562c0-0.571-0.049-5.708-0.144-15.417c-0.098-9.709-0.144-18.179-0.144-25.406l-6.567,1.136%20c-4.187,0.767-9.469,1.092-15.846,1c-6.374-0.089-12.991-0.757-19.842-1.999c-6.854-1.231-13.229-4.086-19.13-8.559%20c-5.898-4.473-10.085-10.328-12.56-17.556l-2.855-6.57c-1.903-4.374-4.899-9.233-8.992-14.559%20c-4.093-5.331-8.232-8.945-12.419-10.848l-1.999-1.431c-1.332-0.951-2.568-2.098-3.711-3.429c-1.142-1.331-1.997-2.663-2.568-3.997%20c-0.572-1.335-0.098-2.43,1.427-3.289c1.525-0.859,4.281-1.276,8.28-1.276l5.708,0.853c3.807,0.763,8.516,3.042,14.133,6.851%20c5.614,3.806,10.229,8.754,13.846,14.842c4.38,7.806,9.657,13.754,15.846,17.847c6.184,4.093,12.419,6.136,18.699,6.136%20c6.28,0,11.704-0.476,16.274-1.423c4.565-0.952,8.848-2.383,12.847-4.285c1.713-12.758,6.377-22.559,13.988-29.41%20c-10.848-1.14-20.601-2.857-29.264-5.14c-8.658-2.286-17.605-5.996-26.835-11.14c-9.235-5.137-16.896-11.516-22.985-19.126%20c-6.09-7.614-11.088-17.61-14.987-29.979c-3.901-12.374-5.852-26.648-5.852-42.826c0-23.035,7.52-42.637,22.557-58.817%20c-7.044-17.318-6.379-36.732,1.997-58.24c5.52-1.715,13.706-0.428,24.554,3.853c10.85,4.283,18.794,7.952,23.84,10.994%20c5.046,3.041,9.089,5.618,12.135,7.708c17.705-4.947,35.976-7.421,54.818-7.421s37.117,2.474,54.823,7.421l10.849-6.849%20c7.419-4.57,16.18-8.758,26.262-12.565c10.088-3.805,17.802-4.853,23.134-3.138c8.562,21.509,9.325,40.922,2.279,58.24%20c15.036,16.18,22.559,35.787,22.559,58.817c0,16.178-1.958,30.497-5.853,42.966c-3.9,12.471-8.941,22.457-15.125,29.979%20c-6.191,7.521-13.901,13.85-23.131,18.986c-9.232,5.14-18.182,8.85-26.84,11.136c-8.662,2.286-18.415,4.004-29.263,5.146%20c9.894,8.562,14.842,22.077,14.842,40.539v60.237c0,3.422,1.19,6.279,3.572,8.562c2.379,2.279,6.136,2.95,11.276,1.995%20c44.163-14.653,80.185-41.062,108.068-79.226c27.88-38.161,41.825-81.126,41.825-128.906%20C438.536,184.851,428.728,148.168,409.132,114.573z'/%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3c/svg%3e"/>
      </a>
     </nav>
    </div>
   </div>
   <div class="container mx-auto px-4 relative pt-8 mb-0">
    <div class="float-right">
     <select class="rounded-md border-gray-200 focus:placeholder-transparent focus:shadow-none focus:border-orange-500 focus:ring-0 text-xs mt-2 py-1 pl-2 pr-7 font-mono">
      <option value="4.36.1">
       4.36.1
      </option>
      <option value="main">
       main
      </option>
     </select>
    </div>
    <h1 class="mb-4 flex items-center text-2xl font-light" data-svelte-h="svelte-15pkjaq">
     <svg aria-hidden="true" class="mr-1.5 text-gray-400 text-3xl" height="1em" preserveaspectratio="xMidYMid meet" role="img" viewbox="0 0 32 32" width="1em" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
      <path d="M20.9022 5.10334L10.8012 10.8791L7.76318 9.11193C8.07741 8.56791 8.5256 8.11332 9.06512 7.7914L15.9336 3.73907C17.0868 3.08811 18.5002 3.26422 19.6534 3.91519L19.3859 3.73911C19.9253 4.06087 20.5879 4.56025 20.9022 5.10334Z" fill="currentColor" opacity="0.5">
      </path>
      <path d="M10.7999 10.8792V28.5483C10.2136 28.5475 9.63494 28.4139 9.10745 28.1578C8.5429 27.8312 8.074 27.3621 7.74761 26.7975C7.42122 26.2327 7.24878 25.5923 7.24756 24.9402V10.9908C7.25062 10.3319 7.42358 9.68487 7.74973 9.1123L10.7999 10.8792Z" fill="currentColor" fill-opacity="0.75">
      </path>
      <path clip-rule="evenodd" d="M21.3368 10.8499V6.918C21.3331 6.25959 21.16 5.61234 20.8346 5.03949L10.7971 10.8727L10.8046 10.874L21.3368 10.8499Z" fill="currentColor" fill-rule="evenodd">
      </path>
      <path d="M21.7937 10.8488L10.7825 10.8741V28.5486L21.7937 28.5234C23.3344 28.5234 24.5835 27.2743 24.5835 25.7335V13.6387C24.5835 12.0979 23.4365 11.1233 21.7937 10.8488Z" fill="currentColor" opacity="0.5">
      </path>
     </svg>
     Documentation
    </h1>
    <div class="grid grid-cols-1 gap-5 lg:grid-cols-2 mt-8">
     <div class="shadow-alternate hover:scale-[1.02] group group flex flex-col overflow-hidden md:first:row-span-3 rounded-xl bg-gradient-to-br px-3 pb-4 pt-6 from-orange-100 via-orange-50 hover:shadow-alternate to-white shadow-none transition-shadow" data-svelte-h="svelte-19kf460">
      <a class="mt-auto mb-2 flex-grow" href="./docs/gradio/" target="_self">
       <div class="text-lg">
        <p class="font-semibold">
         Gradio
        </p>
       </div>
       <code class="font-light text-md mb-2">
        gradio
       </code>
       <div class="relative pr-4 text-lg font-light mt-2">
        Build and share machine learning demos and web applications using the
					core Gradio Python library.
       </div>
      </a>
      <div class="hidden lg:flex flex-wrap mx-auto justify-center">
       <a class="shadow-alternate quick-link rounded-xl px-3.5 py-1 m-2 text-md font-semibold text-white bg-orange-300 hover:drop-shadow-md svelte-amcneo" href="./docs/gradio/interface" target="_self">
        Interface
       </a>
       <a class="shadow-alternate quick-link rounded-xl px-3.5 py-1 m-2 text-md font-semibold text-white bg-orange-300 hover:drop-shadow-md svelte-amcneo" href="./docs/gradio/blocks" target="_self">
        Blocks
       </a>
       <a class="shadow-alternate quick-link rounded-xl px-3.5 py-1 m-2 text-md font-semibold text-white bg-orange-300 hover:drop-shadow-md svelte-amcneo" href="./docs/gradio/chatinterface" target="_self">
        ChatInterface
       </a>
       <a class="shadow-alternate quick-link rounded-xl px-3.5 py-1 m-2 text-md font-semibold text-white bg-orange-300 hover:drop-shadow-md svelte-amcneo" href="./docs/gradio/textbox" target="_self">
        Textbox
       </a>
       <a class="shadow-alternate quick-link rounded-xl px-3.5 py-1 m-2 text-md font-semibold text-white bg-orange-300 hover:drop-shadow-md svelte-amcneo" href="./docs/gradio/image" target="_self">
        Image
       </a>
       <a class="shadow-alternate quick-link rounded-xl px-3.5 py-1 m-2 text-md font-semibold text-white bg-orange-300 hover:drop-shadow-md svelte-amcneo" href="./docs/gradio/audio" target="_self">
        Audio
       </a>
       <a class="shadow-alternate quick-link rounded-xl px-3.5 py-1 m-2 text-md font-semibold text-white bg-orange-300 hover:drop-shadow-md svelte-amcneo" href="./docs/gradio/dataframe" target="_self">
        Dataframe
       </a>
      </div>
     </div>
     <a class="shadow-alternate hover:scale-[1.02] group group relative flex flex-col overflow-hidden rounded-xl bg-gradient-to-r px-3 py-4 dark:border-gray-900 from-yellow-50 hover:shadow-alternate to-white shadow-none transition-shadow dark:from-gray-850 dark:to-gray-950 dark:hover:from-gray-800 svelte-amcneo" href="./docs/python-client" target="_self">
      <div class="relative mb-2 flex items-center text-lg font-semibold" data-svelte-h="svelte-t5lda9">
       <span>
        Python Client
       </span>
      </div>
      <code class="font-light text-md mb-2" data-svelte-h="svelte-1pw0pos">
       gradio-client
      </code>
      <div class="relative pr-4 text-lg font-light" data-svelte-h="svelte-k8176">
       Make programmatic requests to Gradio applications from Python
				environments.
      </div>
     </a>
     <a class="shadow-alternate hover:scale-[1.02] group group relative flex flex-col overflow-hidden md:first:row-span-2 rounded-xl bg-gradient-to-r px-3 py-4 dark:border-gray-900 from-green-50 hover:shadow-alternate to-white shadow-none transition-shadow dark:from-gray-850 dark:to-gray-950 dark:hover:from-gray-800 svelte-amcneo" href="./docs/js-client" target="_self">
      <div class="relative mb-2 flex items-center text-lg font-semibold" data-svelte-h="svelte-2w4loc">
       <span>
        Javascript Client
       </span>
      </div>
      <code class="text-md mb-2" data-svelte-h="svelte-1jr37zc">
       @gradio/client
      </code>
      <div class="relative pr-4 text-lg font-light" data-svelte-h="svelte-1yys1id">
       Make programmatic requests to Gradio applications in JavaScript
				(TypeScript) from the browser or server-side.
      </div>
     </a>
     <a class="shadow-alternate hover:scale-[1.02] group group relative flex flex-col overflow-hidden md:first:row-span-2 rounded-xl bg-gradient-to-r px-3 py-4 dark:border-gray-900 from-blue-50 hover:shadow-alternate to-white shadow-none transition-shadow dark:from-gray-850 dark:to-gray-950 dark:hover:from-gray-800 svelte-amcneo" href="./docs/js" target="_self">
      <div class="relative mb-2 flex items-center text-lg font-semibold" data-svelte-h="svelte-cmtn0p">
       <span>
        Javascript Components
       </span>
      </div>
      <div class="relative pr-4 text-lg font-light" data-svelte-h="svelte-1xpqfah">
       Use Gradio's UI components in standalone JavaScript applications outside
				of the Gradio environment.
      </div>
     </a>
     <a class="shadow-alternate hover:scale-[1.02] group group relative flex flex-col overflow-hidden md:first:row-span-2 rounded-xl bg-gradient-to-r px-3 py-4 dark:border-gray-900 from-purple-50 hover:shadow-alternate to-white shadow-none transition-shadow dark:from-gray-850 dark:to-gray-950 dark:hover:from-gray-800 svelte-amcneo" href="../guides/custom-components-in-five-minutes" target="_self">
      <div class="relative mb-2 flex items-center text-lg font-semibold" data-svelte-h="svelte-j32dk1">
       <span>
        Custom Components
       </span>
      </div>
      <div class="relative pr-4 text-lg font-light" data-svelte-h="svelte-15l6qfb">
       Create, use, and share your own custom components within a Gradio
				application.
      </div>
     </a>
     <a class="shadow-alternate hover:scale-[1.02] group group relative flex flex-col overflow-hidden md:first:row-span-2 rounded-xl bg-gradient-to-r px-3 py-4 dark:border-gray-900 from-pink-50 hover:shadow-alternate to-white shadow-none transition-shadow dark:from-gray-850 dark:to-gray-950 dark:hover:from-gray-800 svelte-amcneo" href="../guides/gradio-lite" target="_self">
      <div class="relative mb-2 flex items-center text-lg font-semibold" data-svelte-h="svelte-8cnxz2">
       <span>
        Gradio Lite
       </span>
      </div>
      <code class="font-light text-md mb-2" data-svelte-h="svelte-kgnwwb">
       @gradio/lite
      </code>
      <div class="relative pr-4 text-lg font-light" data-svelte-h="svelte-6dkfg1">
       Run Gradio's Python code serverless (entirely in your browser) using
				WebAssembly.
      </div>
     </a>
    </div>
    <h1 class="mb-4 flex items-center text-2xl font-light mt-8" data-svelte-h="svelte-b3ueke">
     <svg aria-hidden="true" class="mr-1.5 text-gray-400 text-3xl" fill="currentColor" height="0.75em" preserveaspectratio="xMidYMid meet" role="img" viewbox="0 0 24 24" width="0.75em" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
      <path d="M15,19H9c-0.6,0-1-0.4-1-1v-0.5c0-1.4-0.6-2.8-1.7-3.9C4.7,12,3.9,9.9,4,7.7C4.2,3.5,7.7,0.1,11.9,0L12,0     c4.4,0,8,3.6,8,8c0,2.1-0.8,4.2-2.4,5.7c-1.1,1-1.6,2.4-1.6,3.8V18C16,18.6,15.6,19,15,19z M10,17h4c0.1-1.8,0.9-3.4,2.2-4.8     C17.4,11.1,18,9.6,18,8c0-3.3-2.7-6-6-6l-0.1,0C8.8,2.1,6.1,4.6,6,7.8C5.9,9.4,6.6,11,7.7,12.2C9.1,13.6,9.9,15.3,10,17z">
      </path>
      <path d="M12,24L12,24c-2.2,0-4-1.8-4-4v-2c0-0.6,0.4-1,1-1h6c0.6,0,1,0.4,1,1v2C16,22.2,14.2,24,12,24z M10,19v1c0,1.1,0.9,2,2,2     H12c1.1,0,2-0.9,2-2v-1H10z">
      </path>
      <path d="M9,9C8.4,9,8,8.6,8,8c0-2.2,1.8-4,4-4c0.6,0,1,0.4,1,1s-0.4,1-1,1c-1.1,0-2,0.9-2,2C10,8.6,9.6,9,9,9z">
      </path>
     </svg>
     Guides
    </h1>
    <div class="category py-4">
     <h2 class="mb-4 text-xl font-normal block">
      Getting Started
     </h2>
     <div class="grid grid-cols-1 lg:grid-cols-3 gap-6">
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-green-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/quickstart/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Quickstart
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-green-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/key-features/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Key Features
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
     </div>
    </div>
    <div class="category py-4">
     <h2 class="mb-4 text-xl font-normal block">
      Building Interfaces
     </h2>
     <div class="grid grid-cols-1 lg:grid-cols-3 gap-6">
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-yellow-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/the-interface-class/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         The Interface Class
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-yellow-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/more-on-examples/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         More On Examples
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-yellow-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/flagging/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Flagging
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-yellow-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/interface-state/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Interface State
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-yellow-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/reactive-interfaces/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Reactive Interfaces
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-yellow-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/four-kinds-of-interfaces/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Four Kinds Of Interfaces
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
     </div>
    </div>
    <div class="category py-4">
     <h2 class="mb-4 text-xl font-normal block">
      Additional Features
     </h2>
     <div class="grid grid-cols-1 lg:grid-cols-3 gap-6">
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/queuing/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Queuing
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/streaming-outputs/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Streaming Outputs
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/alerts/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Alerts
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/styling/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Styling
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/progress_bars/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Progress_bars
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/batch-functions/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Batch Functions
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/sharing-your-app/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Sharing Your App
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
     </div>
    </div>
    <div class="category py-4">
     <h2 class="mb-4 text-xl font-normal block">
      Building With Blocks
     </h2>
     <div class="grid grid-cols-1 lg:grid-cols-3 gap-6">
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-blue-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/blocks-and-event-listeners/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Blocks And Event Listeners
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-blue-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/controlling-layout/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Controlling Layout
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-blue-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/state-in-blocks/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         State In Blocks
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-blue-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/dynamic-apps-with-render-decorator/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Dynamic Apps With Render Decorator
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-blue-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/custom-CSS-and-JS/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Custom CSS And JS
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-blue-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/using-blocks-like-functions/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Using Blocks Like Functions
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          TRANSLATION,¬†HUB,¬†SPACES
         </p>
        </div>
       </div>
      </a>
     </div>
    </div>
    <div class="category py-4">
     <h2 class="mb-4 text-xl font-normal block">
      Chatbots
     </h2>
     <div class="grid grid-cols-1 lg:grid-cols-3 gap-6">
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-pink-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/creating-a-chatbot-fast/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Creating A Chatbot Fast
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          NLP,¬†TEXT,¬†CHAT
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-pink-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/creating-a-custom-chatbot-with-blocks/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Creating A Custom Chatbot With Blocks
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          NLP,¬†TEXT,¬†CHAT
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-pink-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/creating-a-discord-bot-from-a-gradio-app/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Creating A Discord Bot From A Gradio App
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          NLP,¬†TEXT,¬†CHAT
         </p>
        </div>
       </div>
      </a>
     </div>
    </div>
    <div class="category py-4">
     <h2 class="mb-4 text-xl font-normal block">
      Custom Components
     </h2>
     <div class="grid grid-cols-1 lg:grid-cols-3 gap-6">
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-purple-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/custom-components-in-five-minutes/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Custom Components In Five Minutes
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-purple-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/key-component-concepts/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Key Component Concepts
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-purple-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/configuration/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Configuration
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-purple-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/backend/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Backend
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-purple-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/frontend/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Frontend
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-purple-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/frequently-asked-questions/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Frequently Asked Questions
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-purple-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/pdf-component-example/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Pdf Component Example
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-purple-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/multimodal-chatbot-part1/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Multimodal Chatbot Part1
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-purple-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/documenting-custom-components/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Documenting Custom Components
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
     </div>
    </div>
    <div class="category py-4">
     <h2 class="mb-4 text-xl font-normal block">
      Tabular Data Science And Plots
     </h2>
     <div class="grid grid-cols-1 lg:grid-cols-3 gap-6">
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-green-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/connecting-to-a-database/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Connecting To A Database
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          TABULAR,¬†PLOTS
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-green-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/creating-a-dashboard-from-bigquery-data/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Creating A Dashboard From Bigquery Data
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          TABULAR,¬†DASHBOARD,¬†PLOTS
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-green-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/creating-a-dashboard-from-supabase-data/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Creating A Dashboard From Supabase Data
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          TABULAR,¬†DASHBOARD,¬†PLOTS
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-green-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/creating-a-realtime-dashboard-from-google-sheets/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Creating A Realtime Dashboard From Google Sheets
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          TABULAR,¬†DASHBOARD,¬†PLOTS
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-green-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/plot-component-for-maps/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Plot Component For Maps
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          PLOTS,¬†MAPS
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-green-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/styling-the-gradio-dataframe/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Styling The Gradio Dataframe
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          DATAFRAME,¬†STYLE,¬†COLOR
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-green-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/using-gradio-for-tabular-workflows/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Using Gradio For Tabular Workflows
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
     </div>
    </div>
    <div class="category py-4">
     <h2 class="mb-4 text-xl font-normal block">
      Gradio Clients And Lite
     </h2>
     <div class="grid grid-cols-1 lg:grid-cols-3 gap-6">
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-yellow-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/getting-started-with-the-python-client/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Getting Started With The Python Client
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          CLIENT,¬†API,¬†SPACES
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-yellow-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/getting-started-with-the-js-client/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Getting Started With The Js Client
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          CLIENT,¬†API,¬†SPACES
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-yellow-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/querying-gradio-apps-with-curl/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Querying Gradio Apps With Curl
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          CURL,¬†API,¬†SPACES
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-yellow-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/gradio-and-llm-agents/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Gradio And Llm Agents
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-yellow-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/gradio-lite/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Gradio Lite
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          SERVERLESS,¬†BROWSER,¬†PYODIDE
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-yellow-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/gradio-lite-and-transformers-js/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Gradio Lite And Transformers Js
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          SERVERLESS,¬†BROWSER,¬†PYODIDE,¬†TRANSFORMERS
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-yellow-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/fastapi-app-with-the-gradio-client/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Fastapi App With The Gradio Client
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          CLIENT,¬†API,¬†WEB APP
         </p>
        </div>
       </div>
      </a>
     </div>
    </div>
    <div class="category py-4">
     <h2 class="mb-4 text-xl font-normal block">
      Other Tutorials
     </h2>
     <div class="grid grid-cols-1 lg:grid-cols-3 gap-6">
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/using-hugging-face-integrations/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Using Hugging Face Integrations
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          HUB,¬†SPACES,¬†EMBED
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/Gradio-and-Comet/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Gradio And Comet
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          COMET,¬†SPACES
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/Gradio-and-ONNX-on-Hugging-Face/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Gradio And ONNX On Hugging Face
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          ONNX,¬†SPACES
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/Gradio-and-Wandb-Integration/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Gradio And Wandb Integration
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          WANDB,¬†SPACES
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/create-your-own-friends-with-a-gan/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Create Your Own Friends With A Gan
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          GAN,¬†IMAGE,¬†HUB
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/deploying-gradio-with-docker/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Deploying Gradio With Docker
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          DEPLOYMENT,¬†DOCKER
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/developing-faster-with-reload-mode/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Developing Faster With Reload Mode
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/how-to-use-3D-model-component/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         How To Use 3D Model Component
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          VISION,¬†IMAGE
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/image-classification-in-pytorch/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Image Classification In Pytorch
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          VISION,¬†RESNET,¬†PYTORCH
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/image-classification-in-tensorflow/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Image Classification In Tensorflow
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          VISION,¬†MOBILENET,¬†TENSORFLOW
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/image-classification-with-vision-transformers/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Image Classification With Vision Transformers
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          VISION,¬†TRANSFORMERS,¬†HUB
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/installing-gradio-in-a-virtual-environment/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Installing Gradio In A Virtual Environment
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          INSTALLATION
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/named-entity-recognition/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Named Entity Recognition
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          NER,¬†TEXT,¬†HIGHLIGHT
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/real-time-speech-recognition/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Real Time Speech Recognition
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          ASR,¬†SPEECH,¬†STREAMING
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/running-background-tasks/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Running Background Tasks
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          TASKS,¬†SCHEDULED,¬†TABULAR,¬†DATA
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/running-gradio-on-your-web-server-with-nginx/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Running Gradio On Your Web Server With Nginx
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          DEPLOYMENT,¬†WEB SERVER,¬†NGINX
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/setting-up-a-demo-for-maximum-performance/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Setting Up A Demo For Maximum Performance
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          CONCURRENCY,¬†LATENCY,¬†PERFORMANCE
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/theming-guide/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Theming Guide
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          THEMES
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/using-flagging/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Using Flagging
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          FLAGGING,¬†DATA
         </p>
        </div>
       </div>
      </a>
      <a class="shadow-alternate hover:scale-[1.04] guide-box flex lg:col-span-1 flex-col group overflow-hidden relative rounded-xl bg-gradient-to-r px-3 py-4 from-red-50 hover:shadow-alternate to-white shadow-none transition-shadow svelte-amcneo" href="./guides/wrapping-layouts/">
       <div class="flex flex-col p-4 h-min">
        <h2 class="text-lg">
         Wrapping Layouts
        </h2>
        <div class="tags-holder">
         <p class="text-gray-600">
          LAYOUTS
         </p>
        </div>
       </div>
      </a>
     </div>
    </div>
   </div>
   <footer class="main-footer container mx-auto flex-row flex items-center px-4 py-6 justify-between" data-svelte-h="svelte-rbo7yn">
    <a href="/">
     <img alt="Gradio logo" src="/_app/immutable/assets/gradio.CHB5adID.svg"/>
    </a>
    <div class="flex gap-3">
     <a class="text-gray-400 hover:text-gray-500" href="https://status.gradio.app" target="_blank">
      <span>
       Status
      </span>
     </a>
     <a class="hover:opacity-75 transition" href="https://twitter.com/Gradio">
      <img alt="Twitter logo" class="w-6" src="data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20stroke='currentColor'%20fill='gray'%20height='32'%20width='32'%20viewBox='0%200%20512%20512'%3e%3cpath%20d='M389.2%2048h70.6L305.6%20224.2%20487%20464H345L233.7%20318.6%20106.5%20464H35.8L200.7%20275.5%2026.8%2048H172.4L272.9%20180.9%20389.2%2048zM364.4%20421.8h39.1L151.1%2088h-42L364.4%20421.8z'/%3e%3c/svg%3e"/>
     </a>
     <a class="hover:opacity-75 transition" href="https://github.com/gradio-app/gradio">
      <img alt="Github logo" class="w-6" src="data:image/svg+xml,%3c?xml%20version='1.0'%20encoding='iso-8859-1'?%3e%3c!--%20Generator:%20Adobe%20Illustrator%2016.0.0,%20SVG%20Export%20Plug-In%20.%20SVG%20Version:%206.00%20Build%200)%20--%3e%3c!DOCTYPE%20svg%20PUBLIC%20'-//W3C//DTD%20SVG%201.1//EN'%20'http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd'%3e%3csvg%20version='1.1'%20id='Capa_1'%20xmlns='http://www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%20x='0px'%20y='0px'%20width='438.549px'%20height='438.549px'%20viewBox='0%200%20438.549%20438.549'%20style='enable-background:new%200%200%20438.549%20438.549;'%20xml:space='preserve'%3e%3cg%3e%3cpath%20fill='gray'%20d='M409.132,114.573c-19.608-33.596-46.205-60.194-79.798-79.8C295.736,15.166,259.057,5.365,219.271,5.365%20c-39.781,0-76.472,9.804-110.063,29.408c-33.596,19.605-60.192,46.204-79.8,79.8C9.803,148.168,0,184.854,0,224.63%20c0,47.78,13.94,90.745,41.827,128.906c27.884,38.164,63.906,64.572,108.063,79.227c5.14,0.954,8.945,0.283,11.419-1.996%20c2.475-2.282,3.711-5.14,3.711-8.562c0-0.571-0.049-5.708-0.144-15.417c-0.098-9.709-0.144-18.179-0.144-25.406l-6.567,1.136%20c-4.187,0.767-9.469,1.092-15.846,1c-6.374-0.089-12.991-0.757-19.842-1.999c-6.854-1.231-13.229-4.086-19.13-8.559%20c-5.898-4.473-10.085-10.328-12.56-17.556l-2.855-6.57c-1.903-4.374-4.899-9.233-8.992-14.559%20c-4.093-5.331-8.232-8.945-12.419-10.848l-1.999-1.431c-1.332-0.951-2.568-2.098-3.711-3.429c-1.142-1.331-1.997-2.663-2.568-3.997%20c-0.572-1.335-0.098-2.43,1.427-3.289c1.525-0.859,4.281-1.276,8.28-1.276l5.708,0.853c3.807,0.763,8.516,3.042,14.133,6.851%20c5.614,3.806,10.229,8.754,13.846,14.842c4.38,7.806,9.657,13.754,15.846,17.847c6.184,4.093,12.419,6.136,18.699,6.136%20c6.28,0,11.704-0.476,16.274-1.423c4.565-0.952,8.848-2.383,12.847-4.285c1.713-12.758,6.377-22.559,13.988-29.41%20c-10.848-1.14-20.601-2.857-29.264-5.14c-8.658-2.286-17.605-5.996-26.835-11.14c-9.235-5.137-16.896-11.516-22.985-19.126%20c-6.09-7.614-11.088-17.61-14.987-29.979c-3.901-12.374-5.852-26.648-5.852-42.826c0-23.035,7.52-42.637,22.557-58.817%20c-7.044-17.318-6.379-36.732,1.997-58.24c5.52-1.715,13.706-0.428,24.554,3.853c10.85,4.283,18.794,7.952,23.84,10.994%20c5.046,3.041,9.089,5.618,12.135,7.708c17.705-4.947,35.976-7.421,54.818-7.421s37.117,2.474,54.823,7.421l10.849-6.849%20c7.419-4.57,16.18-8.758,26.262-12.565c10.088-3.805,17.802-4.853,23.134-3.138c8.562,21.509,9.325,40.922,2.279,58.24%20c15.036,16.18,22.559,35.787,22.559,58.817c0,16.178-1.958,30.497-5.853,42.966c-3.9,12.471-8.941,22.457-15.125,29.979%20c-6.191,7.521-13.901,13.85-23.131,18.986c-9.232,5.14-18.182,8.85-26.84,11.136c-8.662,2.286-18.415,4.004-29.263,5.146%20c9.894,8.562,14.842,22.077,14.842,40.539v60.237c0,3.422,1.19,6.279,3.572,8.562c2.379,2.279,6.136,2.95,11.276,1.995%20c44.163-14.653,80.185-41.062,108.068-79.226c27.88-38.161,41.825-81.126,41.825-128.906%20C438.536,184.851,428.728,148.168,409.132,114.573z'/%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3cg%3e%3c/g%3e%3c/svg%3e"/>
     </a>
    </div>
   </footer>
   <script>
    {
					__sveltekit_ypyqu1 = {
						base: new URL(".", location).pathname.slice(0, -1)
					};

					const element = document.currentScript.parentElement;

					const data = [{"type":"data","data":null,"uses":{"url":1}},{"type":"data","data":{docs:{gradio:{building:{simplecsvlogger:{class:null,name:"SimpleCSVLogger",description:"A simplified implementation of the FlaggingCallback abstract class provided for illustrative purposes.  Each flagged sample (both the input and output data) is logged to a CSV file on the machine running the gradio app.",tags:{},parameters:[],returns:{annotation:null},example:"import gradio as gr\ndef image_classifier(inp):\n    return {'cat': 0.3, 'dog': 0.7}\ndemo = gr.Interface(fn=image_classifier, inputs=\"image\", outputs=\"label\",\n                    flagging_callback=SimpleCSVLogger())",fns:[],parent:"gradio"},csvlogger:{class:null,name:"CSVLogger",description:"The default implementation of the FlaggingCallback abstract class. Each flagged sample (both the input and output data) is logged to a CSV file with headers on the machine running the gradio app.",tags:{guides:"using-flagging"},parameters:[{name:"simplify_file_data",annotation:"bool",doc:null,default:"True"}],returns:{annotation:null},example:"import gradio as gr\ndef image_classifier(inp):\n    return {'cat': 0.3, 'dog': 0.7}\ndemo = gr.Interface(fn=image_classifier, inputs=\"image\", outputs=\"label\",\n                    flagging_callback=CSVLogger())",fns:[],guides:[{name:"using-flagging",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:65,pretty_name:"Using Flagging",content:"# Using Flagging\n\n\n\n\n## Introduction\n\nWhen you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these \"hard\" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust.\n\nGradio simplifies the collection of this data by including a **Flag** button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`.\n\n## The **Flag** button in `gradio.Interface`\n\nFlagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a button marked **Flag**. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file.\n\nThere are [four parameters](https://gradio.app/docs/interface#initialization) in `gradio.Interface` that control how flagging works. We will go over them in greater detail.\n\n- `allow_flagging`: this parameter can be set to either `\"manual\"` (default), `\"auto\"`, or `\"never\"`.\n  - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked.\n  - `auto`: users will not see a button to flag, but every sample will be flagged automatically.\n  - `never`: users will not see a button to flag, and no sample will be flagged.\n- `flagging_options`: this parameter can be either `None` (default) or a list of strings.\n  - If `None`, then the user simply clicks on the **Flag** button and no additional options are shown.\n  - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\"Incorrect\", \"Ambiguous\"]`, then buttons labeled **Flag as Incorrect** and **Flag as Ambiguous** appear. This only applies if `allow_flagging` is `\"manual\"`.\n  - The chosen option is then logged along with the input and output.\n- `flagging_dir`: this parameter takes a string.\n  - It represents what to name the directory where flagged data is stored.\n- `flagging_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\n  - Using this parameter allows you to write custom code that gets run when the flag button is clicked\n  - By default, this is set to an instance of `gr.CSVLogger`\n  - One example is setting it to an instance of `gr.HuggingFaceDatasetSaver` which can allow you to pipe any flagged data into a HuggingFace Dataset. (See more below.)\n\n## What happens to flagged data?\n\nWithin the directory provided by the `flagging_dir` argument, a CSV file will log the flagged data.\n\nHere's an example: The code below creates the calculator interface embedded below it:\n\n```python\nimport gradio as gr\n\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        return num1 / num2\n\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\"\n)\n\niface.launch()\n```\n\n\u003Cgradio-app space=\"gradio/calculator-flag-basic/\">\u003C/gradio-app>\n\nWhen you click the flag button above, the directory where the interface was launched will include a new flagged subfolder, with a csv file inside it. This csv file includes all the data that was flagged.\n\n```directory\n+-- flagged/\n|   +-- logs.csv\n```\n\n_flagged/logs.csv_\n\n```csv\nnum1,operation,num2,Output,timestamp\n5,add,7,12,2022-01-31 11:40:51.093412\n6,subtract,1.5,4.5,2022-01-31 03:25:32.023542\n```\n\nIf the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example an `image` input to `image` output interface will create the following structure.\n\n```directory\n+-- flagged/\n|   +-- logs.csv\n|   +-- image/\n|   |   +-- 0.png\n|   |   +-- 1.png\n|   +-- Output/\n|   |   +-- 0.png\n|   |   +-- 1.png\n```\n\n_flagged/logs.csv_\n\n```csv\nim,Output timestamp\nim/0.png,Output/0.png,2022-02-04 19:49:58.026963\nim/1.png,Output/1.png,2022-02-02 10:40:51.093412\n```\n\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging_options` argument of Interface. Users will have to select one of these choices when flagging, and the option will be saved as an additional column to the CSV.\n\nIf we go back to the calculator example, the following code will create the interface embedded below it.\n\n```python\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"]\n)\n\niface.launch()\n```\n\n\u003Cgradio-app space=\"gradio/calculator-flagging-options/\">\u003C/gradio-app>\n\nWhen users click the flag button, the csv file will now include a column indicating the selected option.\n\n_flagged/logs.csv_\n\n```csv\nnum1,operation,num2,Output,flag,timestamp\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\n```\n\n## The HuggingFaceDatasetSaver Callback\n\nSometimes, saving the data to a local CSV file doesn't make sense. For example, on Hugging Face\nSpaces, developers typically don't have access to the underlying ephemeral machine hosting the Gradio\ndemo. That's why, by default, flagging is turned off in Hugging Face Space. However,\nyou may want to do something else with the flagged data.\n\nWe've made this super easy with the `flagging_callback` parameter.\n\nFor example, below we're going to pipe flagged data from our calculator example into a Hugging Face Dataset, e.g. so that we can build a \"crowd-sourced\" dataset:\n\n```python\nimport os\n\nHF_TOKEN = os.getenv('HF_TOKEN')\nhf_writer = gr.HuggingFaceDatasetSaver(HF_TOKEN, \"crowdsourced-calculator-demo\")\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    description=\"Check out the crowd-sourced dataset at: [https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"],\n    flagging_callback=hf_writer\n)\n\niface.launch()\n```\n\nNotice that we define our own\ninstance of `gradio.HuggingFaceDatasetSaver` using our Hugging Face token and\nthe name of a dataset we'd like to save samples to. In addition, we also set `allow_flagging=\"manual\"`\nbecause on Hugging Face Spaces, `allow_flagging` is set to `\"never\"` by default. Here's our demo:\n\n\u003Cgradio-app space=\"gradio/calculator-flagging-crowdsourced/\">\u003C/gradio-app>\n\nYou can now see all the examples flagged above in this [public Hugging Face dataset](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo).\n\n![flagging callback hf](https://github.com/gradio-app/gradio/blob/main/guides/assets/flagging-callback-hf.png?raw=true)\n\nWe created the `gradio.HuggingFaceDatasetSaver` class, but you can pass your own custom class as long as it inherits from `FLaggingCallback` defined in [this file](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py). If you create a cool callback, contribute it to the repo!\n\n## Flagging with Blocks\n\nWhat about if you are using `gradio.Blocks`? On one hand, you have even more flexibility\nwith Blocks -- you can write whatever Python code you want to run when a button is clicked,\nand assign that using the built-in events in Blocks.\n\nAt the same time, you might want to use an existing `FlaggingCallback` to avoid writing extra code.\nThis requires two steps:\n\n1. You have to run your callback's `.setup()` somewhere in the code prior to the\n   first time you flag data\n2. When the flagging button is clicked, then you trigger the callback's `.flag()` method,\n   making sure to collect the arguments correctly and disabling the typical preprocessing.\n\nHere is an example with an image sepia filter Blocks demo that lets you flag\ndata using the default `CSVLogger`:\n\n```python\nimport numpy as np\nimport gradio as gr\n\ndef sepia(input_img, strength):\n    sepia_filter = strength * np.array(\n        [[0.393, 0.769, 0.189], [0.349, 0.686, 0.168], [0.272, 0.534, 0.131]]\n    ) + (1-strength) * np.identity(3)\n    sepia_img = input_img.dot(sepia_filter.T)\n    sepia_img /= sepia_img.max()\n    return sepia_img\n\ncallback = gr.CSVLogger()\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            img_input = gr.Image()\n            strength = gr.Slider(0, 1, 0.5)\n        img_output = gr.Image()\n    with gr.Row():\n        btn = gr.Button(\"Flag\")\n        \n    # This needs to be called at some point prior to the first call to callback.flag()\n    callback.setup([img_input, strength, img_output], \"flagged_data_points\")\n\n    img_input.change(sepia, [img_input, strength], img_output)\n    strength.change(sepia, [img_input, strength], img_output)\n    \n    # We can choose which components to flag -- in this case, we'll flag all of them\n    btn.click(lambda *args: callback.flag(args), [img_input, strength, img_output], None, preprocess=False)\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/blocks_flag'>\u003C/gradio-app>\n\n## Privacy\n\nImportant Note: please make sure your users understand when the data they submit is being saved, and what you plan on doing with it. This is especially important when you use `allow_flagging=auto` (when all of the data submitted through the demo is being flagged)\n\n### That's all! Happy building :)\n",tags:["FLAGGING","DATA"],spaces:["https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced","https://huggingface.co/spaces/gradio/calculator-flagging-options","https://huggingface.co/spaces/gradio/calculator-flag-basic"],url:"/guides/using-flagging/",contributor:null}],parent:"gradio"},huggingfacedatasetsaver:{class:null,name:"HuggingFaceDatasetSaver",description:"A callback that saves each flagged sample (both the input and output data) to a HuggingFace dataset. \u003Cbr>",tags:{guides:"using-flagging"},parameters:[{name:"hf_token",annotation:"str",doc:"The HuggingFace token to use to create (and write the flagged sample to) the HuggingFace dataset (defaults to the registered one)."},{name:"dataset_name",annotation:"str",doc:"The repo_id of the dataset to save the data to, e.g. &quot;image-classifier-1&quot; or &quot;username/image-classifier-1&quot;."},{name:"private",annotation:"bool",doc:"Whether the dataset should be private (defaults to False).",default:"False"},{name:"info_filename",annotation:"str",doc:"The name of the file to save the dataset info (defaults to &quot;dataset_infos.json&quot;).",default:"\"dataset_info.json\""},{name:"separate_dirs",annotation:"bool",doc:"If True, each flagged item will be saved in a separate directory. This makes the flagging more robust to concurrent editing, but may be less convenient to use.",default:"False"}],returns:{annotation:null},example:"import gradio as gr\nhf_writer = gr.HuggingFaceDatasetSaver(HF_API_TOKEN, \"image-classification-mistakes\")\ndef image_classifier(inp):\n    return {'cat': 0.3, 'dog': 0.7}\ndemo = gr.Interface(fn=image_classifier, inputs=\"image\", outputs=\"label\",\n                    allow_flagging=\"manual\", flagging_callback=hf_writer)",fns:[],guides:[{name:"using-flagging",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:65,pretty_name:"Using Flagging",content:"# Using Flagging\n\n\n\n\n## Introduction\n\nWhen you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these \"hard\" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust.\n\nGradio simplifies the collection of this data by including a **Flag** button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`.\n\n## The **Flag** button in `gradio.Interface`\n\nFlagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a button marked **Flag**. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file.\n\nThere are [four parameters](https://gradio.app/docs/interface#initialization) in `gradio.Interface` that control how flagging works. We will go over them in greater detail.\n\n- `allow_flagging`: this parameter can be set to either `\"manual\"` (default), `\"auto\"`, or `\"never\"`.\n  - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked.\n  - `auto`: users will not see a button to flag, but every sample will be flagged automatically.\n  - `never`: users will not see a button to flag, and no sample will be flagged.\n- `flagging_options`: this parameter can be either `None` (default) or a list of strings.\n  - If `None`, then the user simply clicks on the **Flag** button and no additional options are shown.\n  - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\"Incorrect\", \"Ambiguous\"]`, then buttons labeled **Flag as Incorrect** and **Flag as Ambiguous** appear. This only applies if `allow_flagging` is `\"manual\"`.\n  - The chosen option is then logged along with the input and output.\n- `flagging_dir`: this parameter takes a string.\n  - It represents what to name the directory where flagged data is stored.\n- `flagging_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\n  - Using this parameter allows you to write custom code that gets run when the flag button is clicked\n  - By default, this is set to an instance of `gr.CSVLogger`\n  - One example is setting it to an instance of `gr.HuggingFaceDatasetSaver` which can allow you to pipe any flagged data into a HuggingFace Dataset. (See more below.)\n\n## What happens to flagged data?\n\nWithin the directory provided by the `flagging_dir` argument, a CSV file will log the flagged data.\n\nHere's an example: The code below creates the calculator interface embedded below it:\n\n```python\nimport gradio as gr\n\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        return num1 / num2\n\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\"\n)\n\niface.launch()\n```\n\n\u003Cgradio-app space=\"gradio/calculator-flag-basic/\">\u003C/gradio-app>\n\nWhen you click the flag button above, the directory where the interface was launched will include a new flagged subfolder, with a csv file inside it. This csv file includes all the data that was flagged.\n\n```directory\n+-- flagged/\n|   +-- logs.csv\n```\n\n_flagged/logs.csv_\n\n```csv\nnum1,operation,num2,Output,timestamp\n5,add,7,12,2022-01-31 11:40:51.093412\n6,subtract,1.5,4.5,2022-01-31 03:25:32.023542\n```\n\nIf the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example an `image` input to `image` output interface will create the following structure.\n\n```directory\n+-- flagged/\n|   +-- logs.csv\n|   +-- image/\n|   |   +-- 0.png\n|   |   +-- 1.png\n|   +-- Output/\n|   |   +-- 0.png\n|   |   +-- 1.png\n```\n\n_flagged/logs.csv_\n\n```csv\nim,Output timestamp\nim/0.png,Output/0.png,2022-02-04 19:49:58.026963\nim/1.png,Output/1.png,2022-02-02 10:40:51.093412\n```\n\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging_options` argument of Interface. Users will have to select one of these choices when flagging, and the option will be saved as an additional column to the CSV.\n\nIf we go back to the calculator example, the following code will create the interface embedded below it.\n\n```python\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"]\n)\n\niface.launch()\n```\n\n\u003Cgradio-app space=\"gradio/calculator-flagging-options/\">\u003C/gradio-app>\n\nWhen users click the flag button, the csv file will now include a column indicating the selected option.\n\n_flagged/logs.csv_\n\n```csv\nnum1,operation,num2,Output,flag,timestamp\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\n```\n\n## The HuggingFaceDatasetSaver Callback\n\nSometimes, saving the data to a local CSV file doesn't make sense. For example, on Hugging Face\nSpaces, developers typically don't have access to the underlying ephemeral machine hosting the Gradio\ndemo. That's why, by default, flagging is turned off in Hugging Face Space. However,\nyou may want to do something else with the flagged data.\n\nWe've made this super easy with the `flagging_callback` parameter.\n\nFor example, below we're going to pipe flagged data from our calculator example into a Hugging Face Dataset, e.g. so that we can build a \"crowd-sourced\" dataset:\n\n```python\nimport os\n\nHF_TOKEN = os.getenv('HF_TOKEN')\nhf_writer = gr.HuggingFaceDatasetSaver(HF_TOKEN, \"crowdsourced-calculator-demo\")\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    description=\"Check out the crowd-sourced dataset at: [https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"],\n    flagging_callback=hf_writer\n)\n\niface.launch()\n```\n\nNotice that we define our own\ninstance of `gradio.HuggingFaceDatasetSaver` using our Hugging Face token and\nthe name of a dataset we'd like to save samples to. In addition, we also set `allow_flagging=\"manual\"`\nbecause on Hugging Face Spaces, `allow_flagging` is set to `\"never\"` by default. Here's our demo:\n\n\u003Cgradio-app space=\"gradio/calculator-flagging-crowdsourced/\">\u003C/gradio-app>\n\nYou can now see all the examples flagged above in this [public Hugging Face dataset](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo).\n\n![flagging callback hf](https://github.com/gradio-app/gradio/blob/main/guides/assets/flagging-callback-hf.png?raw=true)\n\nWe created the `gradio.HuggingFaceDatasetSaver` class, but you can pass your own custom class as long as it inherits from `FLaggingCallback` defined in [this file](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py). If you create a cool callback, contribute it to the repo!\n\n## Flagging with Blocks\n\nWhat about if you are using `gradio.Blocks`? On one hand, you have even more flexibility\nwith Blocks -- you can write whatever Python code you want to run when a button is clicked,\nand assign that using the built-in events in Blocks.\n\nAt the same time, you might want to use an existing `FlaggingCallback` to avoid writing extra code.\nThis requires two steps:\n\n1. You have to run your callback's `.setup()` somewhere in the code prior to the\n   first time you flag data\n2. When the flagging button is clicked, then you trigger the callback's `.flag()` method,\n   making sure to collect the arguments correctly and disabling the typical preprocessing.\n\nHere is an example with an image sepia filter Blocks demo that lets you flag\ndata using the default `CSVLogger`:\n\n```python\nimport numpy as np\nimport gradio as gr\n\ndef sepia(input_img, strength):\n    sepia_filter = strength * np.array(\n        [[0.393, 0.769, 0.189], [0.349, 0.686, 0.168], [0.272, 0.534, 0.131]]\n    ) + (1-strength) * np.identity(3)\n    sepia_img = input_img.dot(sepia_filter.T)\n    sepia_img /= sepia_img.max()\n    return sepia_img\n\ncallback = gr.CSVLogger()\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            img_input = gr.Image()\n            strength = gr.Slider(0, 1, 0.5)\n        img_output = gr.Image()\n    with gr.Row():\n        btn = gr.Button(\"Flag\")\n        \n    # This needs to be called at some point prior to the first call to callback.flag()\n    callback.setup([img_input, strength, img_output], \"flagged_data_points\")\n\n    img_input.change(sepia, [img_input, strength], img_output)\n    strength.change(sepia, [img_input, strength], img_output)\n    \n    # We can choose which components to flag -- in this case, we'll flag all of them\n    btn.click(lambda *args: callback.flag(args), [img_input, strength, img_output], None, preprocess=False)\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/blocks_flag'>\u003C/gradio-app>\n\n## Privacy\n\nImportant Note: please make sure your users understand when the data they submit is being saved, and what you plan on doing with it. This is especially important when you use `allow_flagging=auto` (when all of the data submitted through the demo is being flagged)\n\n### That's all! Happy building :)\n",tags:["FLAGGING","DATA"],spaces:["https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced","https://huggingface.co/spaces/gradio/calculator-flagging-options","https://huggingface.co/spaces/gradio/calculator-flag-basic"],url:"/guides/using-flagging/",contributor:null}],parent:"gradio"},base:{class:null,name:"Base",description:"",tags:{},parameters:[{name:"primary_hue",annotation:"colors.Color | str",doc:"The primary hue of the theme. Load a preset, like gradio.themes.colors.green (or just the string &quot;green&quot;), or pass your own gradio.themes.utils.Color object.",default:"Color()"},{name:"secondary_hue",annotation:"colors.Color | str",doc:"The secondary hue of the theme. Load a preset, like gradio.themes.colors.green (or just the string &quot;green&quot;), or pass your own gradio.themes.utils.Color object.",default:"Color()"},{name:"neutral_hue",annotation:"colors.Color | str",doc:"The neutral hue of the theme, used . Load a preset, like gradio.themes.colors.green (or just the string &quot;green&quot;), or pass your own gradio.themes.utils.Color object.",default:"Color()"},{name:"text_size",annotation:"sizes.Size | str",doc:"The size of the text. Load a preset, like gradio.themes.sizes.text_sm (or just the string &quot;sm&quot;), or pass your own gradio.themes.utils.Size object.",default:"Size()"},{name:"spacing_size",annotation:"sizes.Size | str",doc:"The size of the spacing. Load a preset, like gradio.themes.sizes.spacing_sm (or just the string &quot;sm&quot;), or pass your own gradio.themes.utils.Size object.",default:"Size()"},{name:"radius_size",annotation:"sizes.Size | str",doc:"The radius size of corners. Load a preset, like gradio.themes.sizes.radius_sm (or just the string &quot;sm&quot;), or pass your own gradio.themes.utils.Size object.",default:"Size()"},{name:"font",annotation:"fonts.Font | str | Iterable[fonts.Font | str]",doc:"The primary font to use for the theme. Pass a string for a system font, or a gradio.themes.font.GoogleFont object to load a font from Google Fonts. Pass a list of fonts for fallbacks.",default:"(\u003Cgradio.themes.utils.fonts.GoogleFont (name='Source Sans Pro', weights=(400, 600))>, 'ui-sans-serif', 'system-ui', 'sans-serif')"},{name:"font_mono",annotation:"fonts.Font | str | Iterable[fonts.Font | str]",doc:"The monospace font to use for the theme, applies to code. Pass a string for a system font, or a gradio.themes.font.GoogleFont object to load a font from Google Fonts. Pass a list of fonts for fallbacks.",default:"(\u003Cgradio.themes.utils.fonts.GoogleFont (name='IBM Plex Mono', weights=(400, 600))>, 'ui-monospace', 'Consolas', 'monospace')"}],returns:{annotation:null},example:"",fns:[{fn:null,name:"push_to_hub",description:"Upload a theme to the HuggingFace hub. &lt;br&gt; This requires a HuggingFace account. &lt;br&gt;",tags:{},parameters:[{name:"repo_name",annotation:"str",doc:"The name of the repository to store the theme assets, e.g. &#x27;my_theme&#x27; or &#x27;sunset&#x27;."},{name:"org_name",annotation:"str | None",doc:"The name of the org to save the space in. If None (the default), the username corresponding to the logged in user, or h∆í_token is used.",default:"None"},{name:"version",annotation:"str | None",doc:"A semantic version tag for theme. Bumping the version tag lets you publish updates to a theme without changing the look of applications that already loaded your theme.",default:"None"},{name:"hf_token",annotation:"str | None",doc:"API token for your HuggingFace account",default:"None"},{name:"theme_name",annotation:"str | None",doc:"Name for the name. If None, defaults to repo_name",default:"None"},{name:"description",annotation:"str | None",doc:"A long form description to your theme.",default:"None"},{name:"private",annotation:"bool",doc:null,default:"False"}],returns:{},example:null,override_signature:null,parent:"gradio.Base"},{fn:null,name:"from_hub",description:"Load a theme from the hub. &lt;br&gt; This DOES NOT require a HuggingFace account for downloading publicly available themes. &lt;br&gt;",tags:{},parameters:[{name:"repo_name",annotation:"str",doc:"string of the form &lt;author&gt;/&lt;theme-name&gt;@&lt;semantic-version-expression&gt;.  If a semantic version expression is omitted, the latest version will be fetched."},{name:"hf_token",annotation:"str | None",doc:"HuggingFace Token. Only needed to download private themes.",default:"None"}],returns:{},example:null,override_signature:null,parent:"gradio.Base"},{fn:null,name:"load",description:"Load a theme from a json file. &lt;br&gt;",tags:{},parameters:[{name:"path",annotation:"str",doc:"The filepath to read."}],returns:{},example:null,override_signature:null,parent:"gradio.Base"},{fn:null,name:"dump",description:"Write the theme to a json file. &lt;br&gt;",tags:{},parameters:[{name:"filename",annotation:"str",doc:"The path to write the theme too"}],returns:{},example:null,override_signature:null,parent:"gradio.Base"},{fn:null,name:"from_dict",description:"Create a theme instance from a dictionary representation. &lt;br&gt;",tags:{},parameters:[{name:"theme",annotation:"dict[str, dict[str, str]]",doc:"The dictionary representation of the theme."}],returns:{},example:null,override_signature:null,parent:"gradio.Base"},{fn:null,name:"to_dict",description:"Convert the theme into a python dictionary.",tags:{},parameters:[],returns:{},example:null,override_signature:null,parent:"gradio.Base"}],parent:"gradio"},queue:{class:null,name:"queue",description:"By enabling the queue you can control when users know their position in the queue, and set a limit on maximum number of events allowed.",tags:{parameters:"status_update_rate: If \"auto\", Queue will send status estimations to all clients whenever a job is finished. Otherwise Queue will send status at regular intervals set by this parameter as the number of seconds.\u003Cbr>api_open: If True, the REST routes of the backend will be open, allowing requests made directly to those endpoints to skip the queue.\u003Cbr>max_size: The maximum number of events the queue will store at any given moment. If the queue is full, new events will not be added and a user will receive a message saying that the queue is full. If None, the queue size will be unlimited.\u003Cbr>concurrency_count: Deprecated. Set the concurrency_limit directly on event listeners e.g. btn.click(fn, ..., concurrency_limit=10) or gr.Interface(concurrency_limit=10). If necessary, the total number of workers can be configured via `max_threads` in launch().\u003Cbr>default_concurrency_limit: The default value of `concurrency_limit` to use for event listeners that don't specify a value. Can be set by environment variable GRADIO_DEFAULT_CONCURRENCY_LIMIT. Defaults to 1 if not set otherwise.\u003Cbr>with gr.Blocks() as demo:\u003Cbr>    button = gr.Button(label=\"Generate Image\")\u003Cbr>    button.click(fn=image_generator, inputs=gr.Textbox(), outputs=gr.Image())\u003Cbr>demo.queue(max_size=10)\u003Cbr>demo.launch()\u003Cbr>demo = gr.Interface(image_generator, gr.Textbox(), gr.Image())\u003Cbr>demo.queue(max_size=20)\u003Cbr>demo.launch()"},parameters:[{name:"status_update_rate",annotation:"float | Literal['auto']",doc:"If &quot;auto&quot;, Queue will send status estimations to all clients whenever a job is finished. Otherwise Queue will send status at regular intervals set by this parameter as the number of seconds.",default:"\"auto\""},{name:"api_open",annotation:"bool | None",doc:"If True, the REST routes of the backend will be open, allowing requests made directly to those endpoints to skip the queue.",default:"None"},{name:"max_size",annotation:"int | None",doc:"The maximum number of events the queue will store at any given moment. If the queue is full, new events will not be added and a user will receive a message saying that the queue is full. If None, the queue size will be unlimited.",default:"None"},{name:"concurrency_count",annotation:"int | None",doc:"Deprecated. Set the concurrency_limit directly on event listeners e.g. btn.click(fn, ..., concurrency_limit=10) or gr.Interface(concurrency_limit=10). If necessary, the total number of workers can be configured via `max_threads` in launch().",default:"None"},{name:"default_concurrency_limit",annotation:"int | None | Literal['not_set']",doc:"The default value of `concurrency_limit` to use for event listeners that don&#x27;t specify a value. Can be set by environment variable GRADIO_DEFAULT_CONCURRENCY_LIMIT. Defaults to 1 if not set otherwise.",default:"\"not_set\""}],returns:{annotation:null},example:"(\nI\nn\nt\ne\nr\nf\na\nc\ne\n)",fns:[],parent:"gradio"},blocks:{class:null,name:"Blocks",description:"Blocks is Gradio's low-level API that allows you to create more custom web applications and demos than Interfaces (yet still entirely in Python). \u003Cbr> \u003Cbr> Compared to the Interface class, Blocks offers more flexibility and control over: (1) the layout of components (2) the events that trigger the execution of functions (3) data flows (e.g. inputs can trigger outputs, which can trigger the next level of outputs). Blocks also offers ways to group together related demos such as with tabs. \u003Cbr> \u003Cbr> The basic usage of Blocks is as follows: create a Blocks object, then use it as a context (with the \"with\" statement), and then define layouts, components, or events within the Blocks context. Finally, call the launch() method to launch the demo. \u003Cbr>",tags:{demos:"blocks_hello, blocks_flipper, blocks_kinematics",guides:"blocks-and-event-listeners, controlling-layout, state-in-blocks, custom-CSS-and-JS, using-blocks-like-functions"},parameters:[{name:"theme",annotation:"Theme | str | None",doc:"A Theme object or a string representing a theme. If a string, will look for a built-in theme with that name (e.g. &quot;soft&quot; or &quot;default&quot;), or will attempt to load a theme from the Hugging Face Hub (e.g. &quot;gradio/monochrome&quot;). If None, will use the Default theme.",default:"None"},{name:"analytics_enabled",annotation:"bool | None",doc:"Whether to allow basic telemetry. If None, will use GRADIO_ANALYTICS_ENABLED environment variable or default to True.",default:"None"},{name:"mode",annotation:"str",doc:"A human-friendly name for the kind of Blocks or Interface being created. Used internally for analytics.",default:"\"blocks\""},{name:"title",annotation:"str",doc:"The tab title to display when this is opened in a browser window.",default:"\"Gradio\""},{name:"css",annotation:"str | None",doc:"Custom css as a string or path to a css file. This css will be included in the demo webpage.",default:"None"},{name:"js",annotation:"str | None",doc:"Custom js as a string or path to a js file. The custom js should be in the form of a single js function. This function will automatically be executed when the page loads. For more flexibility, use the head parameter to insert js inside &lt;script&gt; tags.",default:"None"},{name:"head",annotation:"str | None",doc:"Custom html to insert into the head of the demo webpage. This can be used to add custom meta tags, multiple scripts, stylesheets, etc. to the page.",default:"None"},{name:"fill_height",annotation:"bool",doc:"Whether to vertically expand top-level child components to the height of the window. If True, expansion occurs when the scale value of the child components &gt;= 1.",default:"False"},{name:"delete_cache",annotation:"tuple[int, int] | None",doc:"A tuple corresponding [frequency, age] both expressed in number of seconds. Every `frequency` seconds, the temporary files created by this Blocks instance will be deleted if more than `age` seconds have passed since the file was created. For example, setting this to (86400, 86400) will delete temporary files every day. The cache will be deleted entirely when the server restarts. If None, no cache deletion will occur.",default:"None"}],returns:{annotation:null},example:"import gradio as gr\ndef update(name):\n    return f\"Welcome to Gradio, {name}!\"\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Start typing below and then click **Run** to see the output.\")\n    with gr.Row():\n        inp = gr.Textbox(placeholder=\"What is your name?\")\n        out = gr.Textbox()\n    btn = gr.Button(\"Run\")\n    btn.click(fn=update, inputs=inp, outputs=out)\n\ndemo.launch()",fns:[{fn:null,name:"launch",description:"Launches a simple web server that serves the demo. Can also be used to create a public link used by anyone to access the demo from their browser by setting share=True. &lt;br&gt;",tags:{},parameters:[{name:"inline",annotation:"bool | None",doc:"whether to display in the gradio app inline in an iframe. Defaults to True in python notebooks; False otherwise.",default:"None"},{name:"inbrowser",annotation:"bool",doc:"whether to automatically launch the gradio app in a new tab on the default browser.",default:"False"},{name:"share",annotation:"bool | None",doc:"whether to create a publicly shareable link for the gradio app. Creates an SSH tunnel to make your UI accessible from anywhere. If not provided, it is set to False by default every time, except when running in Google Colab. When localhost is not accessible (e.g. Google Colab), setting share=False is not supported. Can be set by environment variable GRADIO_SHARE=True.",default:"None"},{name:"debug",annotation:"bool",doc:"if True, blocks the main thread from running. If running in Google Colab, this is needed to print the errors in the cell output.",default:"False"},{name:"max_threads",annotation:"int",doc:"the maximum number of total threads that the Gradio app can generate in parallel. The default is inherited from the starlette library (currently 40).",default:"40"},{name:"auth",annotation:"Callable | tuple[str, str] | list[tuple[str, str]] | None",doc:"If provided, username and password (or list of username-password tuples) required to access app. Can also provide function that takes username and password and returns True if valid login.",default:"None"},{name:"auth_message",annotation:"str | None",doc:"If provided, HTML message provided on login page.",default:"None"},{name:"prevent_thread_lock",annotation:"bool",doc:"By default, the gradio app blocks the main thread while the server is running. If set to True, the gradio app will not block and the gradio server will terminate as soon as the script finishes.",default:"False"},{name:"show_error",annotation:"bool",doc:"If True, any errors in the gradio app will be displayed in an alert modal and printed in the browser console log",default:"False"},{name:"server_name",annotation:"str | None",doc:"to make app accessible on local network, set this to &quot;0.0.0.0&quot;. Can be set by environment variable GRADIO_SERVER_NAME. If None, will use &quot;127.0.0.1&quot;.",default:"None"},{name:"server_port",annotation:"int | None",doc:"will start gradio app on this port (if available). Can be set by environment variable GRADIO_SERVER_PORT. If None, will search for an available port starting at 7860.",default:"None"},{name:"height",annotation:"int",doc:"The height in pixels of the iframe element containing the gradio app (used if inline=True)",default:"500"},{name:"width",annotation:"int | str",doc:"The width in pixels of the iframe element containing the gradio app (used if inline=True)",default:"\"100%\""},{name:"favicon_path",annotation:"str | None",doc:"If a path to a file (.png, .gif, or .ico) is provided, it will be used as the favicon for the web page.",default:"None"},{name:"ssl_keyfile",annotation:"str | None",doc:"If a path to a file is provided, will use this as the private key file to create a local server running on https.",default:"None"},{name:"ssl_certfile",annotation:"str | None",doc:"If a path to a file is provided, will use this as the signed certificate for https. Needs to be provided if ssl_keyfile is provided.",default:"None"},{name:"ssl_keyfile_password",annotation:"str | None",doc:"If a password is provided, will use this with the ssl certificate for https.",default:"None"},{name:"ssl_verify",annotation:"bool",doc:"If False, skips certificate validation which allows self-signed certificates to be used.",default:"True"},{name:"quiet",annotation:"bool",doc:"If True, suppresses most print statements.",default:"False"},{name:"show_api",annotation:"bool",doc:"If True, shows the api docs in the footer of the app. Default True.",default:"True"},{name:"allowed_paths",annotation:"list[str] | None",doc:"List of complete filepaths or parent directories that gradio is allowed to serve. Must be absolute paths. Warning: if you provide directories, any files in these directories or their subdirectories are accessible to all users of your app.",default:"None"},{name:"blocked_paths",annotation:"list[str] | None",doc:"List of complete filepaths or parent directories that gradio is not allowed to serve (i.e. users of your app are not allowed to access). Must be absolute paths. Warning: takes precedence over `allowed_paths` and all other directories exposed by Gradio by default.",default:"None"},{name:"root_path",annotation:"str | None",doc:"The root path (or &quot;mount point&quot;) of the application, if it&#x27;s not served from the root (&quot;/&quot;) of the domain. Often used when the application is behind a reverse proxy that forwards requests to the application. For example, if the application is served at &quot;https://example.com/myapp&quot;, the `root_path` should be set to &quot;/myapp&quot;. A full URL beginning with http:// or https:// can be provided, which will be used as the root path in its entirety. Can be set by environment variable GRADIO_ROOT_PATH. Defaults to &quot;&quot;.",default:"None"},{name:"app_kwargs",annotation:"dict[str, Any] | None",doc:"Additional keyword arguments to pass to the underlying FastAPI app as a dictionary of parameter keys and argument values. For example, `{&quot;docs_url&quot;: &quot;/docs&quot;}`",default:"None"},{name:"state_session_capacity",annotation:"int",doc:"The maximum number of sessions whose information to store in memory. If the number of sessions exceeds this number, the oldest sessions will be removed. Reduce capacity to reduce memory usage when using gradio.State or returning updated components from functions. Defaults to 10000.",default:"10000"},{name:"share_server_address",annotation:"str | None",doc:"Use this to specify a custom FRP server and port for sharing Gradio apps (only applies if share=True). If not provided, will use the default FRP server at https://gradio.live. See https://github.com/huggingface/frp for more information.",default:"None"},{name:"share_server_protocol",annotation:"Literal[('http', 'https')] | None",doc:"Use this to specify the protocol to use for the share links. Defaults to &quot;https&quot;, unless a custom share_server_address is provided, in which case it defaults to &quot;http&quot;. If you are using a custom share_server_address and want to use https, you must set this to &quot;https&quot;.",default:"None"},{name:"auth_dependency",annotation:"Callable[[fastapi.Request], str | None] | None",doc:"A function that takes a FastAPI request and returns a string user ID or None. If the function returns None for a specific request, that user is not authorized to access the app (they will see a 401 Unauthorized response). To be used with external authentication systems like OAuth. Cannot be used with `auth`.",default:"None"},{name:"max_file_size",annotation:"str | int | None",doc:"The maximum file size in bytes that can be uploaded. Can be a string of the form &quot;&lt;value&gt;&lt;unit&gt;&quot;, where value is any positive integer and unit is one of &quot;b&quot;, &quot;kb&quot;, &quot;mb&quot;, &quot;gb&quot;, &quot;tb&quot;. If None, no limit is set.",default:"None"},{name:"enable_monitoring",annotation:"bool",doc:null,default:"False"}],returns:{},example:"import gradio as gr\ndef reverse(text):\n    return text[::-1]\nwith gr.Blocks() as demo:\n    button = gr.Button(value=\"Reverse\")\n    button.click(reverse, gr.Textbox(), gr.Textbox())\ndemo.launch(share=True, auth=(\"username\", \"password\"))",override_signature:null,parent:"gradio.Blocks"},{fn:null,name:"queue",description:"By enabling the queue you can control when users know their position in the queue, and set a limit on maximum number of events allowed.",tags:{},parameters:[{name:"status_update_rate",annotation:"float | Literal['auto']",doc:"If &quot;auto&quot;, Queue will send status estimations to all clients whenever a job is finished. Otherwise Queue will send status at regular intervals set by this parameter as the number of seconds.",default:"\"auto\""},{name:"api_open",annotation:"bool | None",doc:"If True, the REST routes of the backend will be open, allowing requests made directly to those endpoints to skip the queue.",default:"None"},{name:"max_size",annotation:"int | None",doc:"The maximum number of events the queue will store at any given moment. If the queue is full, new events will not be added and a user will receive a message saying that the queue is full. If None, the queue size will be unlimited.",default:"None"},{name:"concurrency_count",annotation:"int | None",doc:"Deprecated. Set the concurrency_limit directly on event listeners e.g. btn.click(fn, ..., concurrency_limit=10) or gr.Interface(concurrency_limit=10). If necessary, the total number of workers can be configured via `max_threads` in launch().",default:"None"},{name:"default_concurrency_limit",annotation:"int | None | Literal['not_set']",doc:"The default value of `concurrency_limit` to use for event listeners that don&#x27;t specify a value. Can be set by environment variable GRADIO_DEFAULT_CONCURRENCY_LIMIT. Defaults to 1 if not set otherwise.",default:"\"not_set\""}],returns:{},example:"with gr.Blocks() as demo:\n    button = gr.Button(label=\"Generate Image\")\n    button.click(fn=image_generator, inputs=gr.Textbox(), outputs=gr.Image())\ndemo.queue(max_size=10)\ndemo.launch()",override_signature:null,parent:"gradio.Blocks"},{fn:null,name:"integrate",description:"A catch-all method for integrating with other libraries. This method should be run after launch()",tags:{},parameters:[{name:"comet_ml",annotation:"\u003Cclass 'inspect._empty'>",doc:"If a comet_ml Experiment object is provided, will integrate with the experiment and appear on Comet dashboard",default:"None"},{name:"wandb",annotation:"ModuleType | None",doc:"If the wandb module is provided, will integrate with it and appear on WandB dashboard",default:"None"},{name:"mlflow",annotation:"ModuleType | None",doc:"If the mlflow module  is provided, will integrate with the experiment and appear on ML Flow dashboard",default:"None"}],returns:{},example:null,override_signature:null,parent:"gradio.Blocks"},{fn:null,name:"load",description:"This listener is triggered when the Blocks initially loads in the browser.",tags:{},parameters:[{name:"block",annotation:"Block | None",doc:null},{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Blocks"},{fn:null,name:"unload",description:"This listener is triggered when the user closes or refreshes the tab, ending the user session. It is useful for cleaning up resources when the app is closed.",tags:{},parameters:[{name:"fn",annotation:"Callable",doc:"Callable function to run to clear resources. The function should not take any arguments and the output is not used."}],returns:{},example:"import gradio as gr\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# When you close the tab, hello will be printed to the console\")\n    demo.unload(lambda: print(\"hello\"))\ndemo.launch()",override_signature:null,parent:"gradio.Blocks"}],demos:[["blocks_hello","import gradio as gr\n\ndef welcome(name):\n    return f\"Welcome to Gradio, {name}!\"\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\n    \"\"\"\n    # Hello World!\n    Start typing below to see the output.\n    \"\"\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n    inp.change(welcome, inp, out)\n\nif __name__ == \"__main__\":\n    demo.launch()"],["blocks_flipper","import numpy as np\nimport gradio as gr\n\n\ndef flip_text(x):\n    return x[::-1]\n\n\ndef flip_image(x):\n    return np.fliplr(x)\n\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Flip text or image files using this demo.\")\n    with gr.Tab(\"Flip Text\"):\n        text_input = gr.Textbox()\n        text_output = gr.Textbox()\n        text_button = gr.Button(\"Flip\")\n    with gr.Tab(\"Flip Image\"):\n        with gr.Row():\n            image_input = gr.Image()\n            image_output = gr.Image()\n        image_button = gr.Button(\"Flip\")\n\n    with gr.Accordion(\"Open for More!\", open=False):\n        gr.Markdown(\"Look at me...\")\n        temp_slider = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=0.1,\n            step=0.1,\n            interactive=True,\n            label=\"Slide me\",\n        )\n        temp_slider.change(lambda x: x, [temp_slider])\n\n    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"],["blocks_kinematics","import pandas as pd\nimport numpy as np\n\nimport gradio as gr\n\n\ndef plot(v, a):\n    g = 9.81\n    theta = a / 180 * 3.14\n    tmax = ((2 * v) * np.sin(theta)) / g\n    timemat = tmax * np.linspace(0, 1, 40)\n\n    x = (v * timemat) * np.cos(theta)\n    y = ((v * timemat) * np.sin(theta)) - ((0.5 * g) * (timemat**2))\n    df = pd.DataFrame({\"x\": x, \"y\": y})\n    return df\n\n\ndemo = gr.Blocks()\n\nwith demo:\n    gr.Markdown(\n        r\"Let's do some kinematics! Choose the speed and angle to see the trajectory. Remember that the range $R = v_0^2 \\cdot \\frac{\\sin(2\\theta)}{g}$\"\n    )\n\n    with gr.Row():\n        speed = gr.Slider(1, 30, 25, label=\"Speed\")\n        angle = gr.Slider(0, 90, 45, label=\"Angle\")\n    output = gr.LinePlot(\n        x=\"x\",\n        y=\"y\",\n        overlay_point=True,\n        tooltip=[\"x\", \"y\"],\n        x_lim=[0, 100],\n        y_lim=[0, 60],\n        width=350,\n        height=300,\n    )\n    btn = gr.Button(value=\"Run\")\n    btn.click(plot, [speed, angle], output)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]],guides:[{name:"blocks-and-event-listeners",category:"building-with-blocks",pretty_category:"Building With Blocks",guide_index:1,absolute_index:15,pretty_name:"Blocks And Event Listeners",content:"# Blocks and Event Listeners\n\nWe briefly descirbed the Blocks class in the [Quickstart](/main/guides/quickstart#custom-demos-with-gr-blocks) as a way to build custom demos. Let's dive deeper. \n\n\n## Blocks Structure\n\nTake a look at the demo below.\n\n```python\nimport gradio as gr\n\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\n\nwith gr.Blocks() as demo:\n    name = gr.Textbox(label=\"Name\")\n    output = gr.Textbox(label=\"Output Box\")\n    greet_btn = gr.Button(\"Greet\")\n    greet_btn.click(fn=greet, inputs=name, outputs=output, api_name=\"greet\")\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/hello_blocks'>\u003C/gradio-app>\n\n- First, note the `with gr.Blocks() as demo:` clause. The Blocks app code will be contained within this clause.\n- Next come the Components. These are the same Components used in `Interface`. However, instead of being passed to some constructor, Components are automatically added to the Blocks as they are created within the `with` clause.\n- Finally, the `click()` event listener. Event listeners define the data flow within the app. In the example above, the listener ties the two Textboxes together. The Textbox `name` acts as the input and Textbox `output` acts as the output to the `greet` method. This dataflow is triggered when the Button `greet_btn` is clicked. Like an Interface, an event listener can take multiple inputs or outputs.\n\nYou can also attach event listeners using decorators - skip the `fn` argument and assign `inputs` and `outputs` directly:\n\n```python\nimport gradio as gr\n\n\nwith gr.Blocks() as demo:\n    name = gr.Textbox(label=\"Name\")\n    output = gr.Textbox(label=\"Output Box\")\n    greet_btn = gr.Button(\"Greet\")\n\n    @greet_btn.click(inputs=name, outputs=output)\n    def greet(name):\n        return \"Hello \" + name + \"!\"\n\n   \n\ndemo.launch()\n```\n\n## Event Listeners and Interactivity\n\nIn the example above, you'll notice that you are able to edit Textbox `name`, but not Textbox `output`. This is because any Component that acts as an input to an event listener is made interactive. However, since Textbox `output` acts only as an output, Gradio determines that it should not be made interactive. You can override the default behavior and directly configure the interactivity of a Component with the boolean `interactive` keyword argument.\n\n```python\noutput = gr.Textbox(label=\"Output\", interactive=True)\n```\n\n_Note_: What happens if a Gradio component is neither an input nor an output? If a component is constructed with a default value, then it is presumed to be displaying content and is rendered non-interactive. Otherwise, it is rendered interactive. Again, this behavior can be overridden by specifying a value for the `interactive` argument.\n\n## Types of Event Listeners\n\nTake a look at the demo below:\n\n```python\nimport gradio as gr\n\ndef welcome(name):\n    return f\"Welcome to Gradio, {name}!\"\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\n    \"\"\"\n    # Hello World!\n    Start typing below to see the output.\n    \"\"\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n    inp.change(welcome, inp, out)\n\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/blocks_hello'>\u003C/gradio-app>\n\nInstead of being triggered by a click, the `welcome` function is triggered by typing in the Textbox `inp`. This is due to the `change()` event listener. Different Components support different event listeners. For example, the `Video` Component supports a `play()` event listener, triggered when a user presses play. See the [Docs](http://gradio.app/docs#components) for the event listeners for each Component.\n\n## Multiple Data Flows\n\nA Blocks app is not limited to a single data flow the way Interfaces are. Take a look at the demo below:\n\n```python\nimport gradio as gr\n\ndef increase(num):\n    return num + 1\n\nwith gr.Blocks() as demo:\n    a = gr.Number(label=\"a\")\n    b = gr.Number(label=\"b\")\n    atob = gr.Button(\"a > b\")\n    btoa = gr.Button(\"b > a\")\n    atob.click(increase, a, b)\n    btoa.click(increase, b, a)\n\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/reversible_flow'>\u003C/gradio-app>\n\nNote that `num1` can act as input to `num2`, and also vice-versa! As your apps get more complex, you will have many data flows connecting various Components.\n\nHere's an example of a \"multi-step\" demo, where the output of one model (a speech-to-text model) gets fed into the next model (a sentiment classifier).\n\n```python\nfrom transformers import pipeline\n\nimport gradio as gr\n\nasr = pipeline(\"automatic-speech-recognition\", \"facebook/wav2vec2-base-960h\")\nclassifier = pipeline(\"text-classification\")\n\n\ndef speech_to_text(speech):\n    text = asr(speech)[\"text\"]\n    return text\n\n\ndef text_to_sentiment(text):\n    return classifier(text)[0][\"label\"]\n\n\ndemo = gr.Blocks()\n\nwith demo:\n    audio_file = gr.Audio(type=\"filepath\")\n    text = gr.Textbox()\n    label = gr.Label()\n\n    b1 = gr.Button(\"Recognize Speech\")\n    b2 = gr.Button(\"Classify Sentiment\")\n\n    b1.click(speech_to_text, inputs=audio_file, outputs=text)\n    b2.click(text_to_sentiment, inputs=text, outputs=label)\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/blocks_speech_text_sentiment'>\u003C/gradio-app>\n\n## Function Input List vs Dict\n\nThe event listeners you've seen so far have a single input component. If you'd like to have multiple input components pass data to the function, you have two options on how the function can accept input component values:\n\n1. as a list of arguments, or\n2. as a single dictionary of values, keyed by the component\n\nLet's see an example of each:\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    a = gr.Number(label=\"a\")\n    b = gr.Number(label=\"b\")\n    with gr.Row():\n        add_btn = gr.Button(\"Add\")\n        sub_btn = gr.Button(\"Subtract\")\n    c = gr.Number(label=\"sum\")\n\n    def add(num1, num2):\n        return num1 + num2\n    add_btn.click(add, inputs=[a, b], outputs=c)\n\n    def sub(data):\n        return data[a] - data[b]\n    sub_btn.click(sub, inputs={a, b}, outputs=c)\n\n\ndemo.launch()\n```\n\nBoth `add()` and `sub()` take `a` and `b` as inputs. However, the syntax is different between these listeners.\n\n1. To the `add_btn` listener, we pass the inputs as a list. The function `add()` takes each of these inputs as arguments. The value of `a` maps to the argument `num1`, and the value of `b` maps to the argument `num2`.\n2. To the `sub_btn` listener, we pass the inputs as a set (note the curly brackets!). The function `sub()` takes a single dictionary argument `data`, where the keys are the input components, and the values are the values of those components.\n\nIt is a matter of preference which syntax you prefer! For functions with many input components, option 2 may be easier to manage.\n\n\u003Cgradio-app space='gradio/calculator_list_and_dict'>\u003C/gradio-app>\n\n## Function Return List vs Dict\n\nSimilarly, you may return values for multiple output components either as:\n\n1. a list of values, or\n2. a dictionary keyed by the component\n\nLet's first see an example of (1), where we set the values of two output components by returning two values:\n\n```python\nwith gr.Blocks() as demo:\n    food_box = gr.Number(value=10, label=\"Food Count\")\n    status_box = gr.Textbox()\n    def eat(food):\n        if food > 0:\n            return food - 1, \"full\"\n        else:\n            return 0, \"hungry\"\n    gr.Button(\"EAT\").click(\n        fn=eat,\n        inputs=food_box,\n        outputs=[food_box, status_box]\n    )\n```\n\nAbove, each return statement returns two values corresponding to `food_box` and `status_box`, respectively.\n\nInstead of returning a list of values corresponding to each output component in order, you can also return a dictionary, with the key corresponding to the output component and the value as the new value. This also allows you to skip updating some output components.\n\n```python\nwith gr.Blocks() as demo:\n    food_box = gr.Number(value=10, label=\"Food Count\")\n    status_box = gr.Textbox()\n    def eat(food):\n        if food > 0:\n            return {food_box: food - 1, status_box: \"full\"}\n        else:\n            return {status_box: \"hungry\"}\n    gr.Button(\"EAT\").click(\n        fn=eat,\n        inputs=food_box,\n        outputs=[food_box, status_box]\n    )\n```\n\nNotice how when there is no food, we only update the `status_box` element. We skipped updating the `food_box` component.\n\nDictionary returns are helpful when an event listener affects many components on return, or conditionally affects outputs and not others.\n\nKeep in mind that with dictionary returns, we still need to specify the possible outputs in the event listener.\n\n## Updating Component Configurations\n\nThe return value of an event listener function is usually the updated value of the corresponding output Component. Sometimes we want to update the configuration of the Component as well, such as the visibility. In this case, we return a new Component, setting the properties we want to change.\n\n```python\nimport gradio as gr\n\n\ndef change_textbox(choice):\n    if choice == \"short\":\n        return gr.Textbox(lines=2, visible=True)\n    elif choice == \"long\":\n        return gr.Textbox(lines=8, visible=True, value=\"Lorem ipsum dolor sit amet\")\n    else:\n        return gr.Textbox(visible=False)\n\n\nwith gr.Blocks() as demo:\n    radio = gr.Radio(\n        [\"short\", \"long\", \"none\"], label=\"What kind of essay would you like to write?\"\n    )\n    text = gr.Textbox(lines=2, interactive=True, show_copy_button=True)\n    radio.change(fn=change_textbox, inputs=radio, outputs=text)\n\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/blocks_essay_simple'>\u003C/gradio-app>\n\nSee how we can configure the Textbox itself through a new `gr.Textbox()` method. The `value=` argument can still be used to update the value along with Component configuration. Any arguments we do not set will use their previous values.\n\n## Examples\n\nJust like with `gr.Interface`, you can also add examples for your functions when you are working with `gr.Blocks`. In this case, instantiate a `gr.Examples` similar to how you would instantiate any other component. The constructor of `gr.Examples` takes two required arguments:\n\n* `examples`: a nested list of examples, in which the outer list consists of examples and each inner list consists of an input corresponding to each input component\n* `inputs`: the component or list of components that should be populated when the examples are clicked\n\nYou can also set `cache_examples=True` similar to `gr.Interface`, in which case two additional arguments must be provided:\n\n* `outputs`: the component or list of components corresponding to the output of the examples\n* `fn`: the function to run to generate the outputs corresponding to the examples\n\nHere's an example showing how to use `gr.Examples` in a `gr.Blocks` app:\n\n```python\nimport gradio as gr\n\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        return num1 / num2\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            num_1 = gr.Number(value=4)\n            operation = gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"])\n            num_2 = gr.Number(value=0)\n            submit_btn = gr.Button(value=\"Calculate\")\n        with gr.Column():\n            result = gr.Number()\n\n    submit_btn.click(\n        calculator, inputs=[num_1, operation, num_2], outputs=[result], api_name=False\n    )\n    examples = gr.Examples(\n        examples=[\n            [5, \"add\", 3],\n            [4, \"divide\", 2],\n            [-4, \"multiply\", 2.5],\n            [0, \"subtract\", 1.2],\n        ],\n        inputs=[num_1, operation, num_2],\n    )\n\nif __name__ == \"__main__\":\n    demo.launch(show_api=False)\n\n```\n\n**Note**: In Gradio 4.0 or later, when you click on examples, not only does the value of the input component update to the example value, but the component's configuration also reverts to the properties with which you constructed the component. This ensures that the examples are compatible with the component even if its configuration has been changed. \n\n\n\n## Running Events Consecutively\n\nYou can also run events consecutively by using the `then` method of an event listener. This will run an event after the previous event has finished running. This is useful for running events that update components in multiple steps.\n\nFor example, in the chatbot example below, we first update the chatbot with the user message immediately, and then update the chatbot with the computer response after a simulated delay.\n\n```python\nimport gradio as gr\nimport random\nimport time\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    msg = gr.Textbox()\n    clear = gr.Button(\"Clear\")\n\n    def user(user_message, history):\n        return \"\", history + [[user_message, None]]\n\n    def bot(history):\n        bot_message = random.choice([\"How are you?\", \"I love you\", \"I'm very hungry\"])\n        time.sleep(2)\n        history[-1][1] = bot_message\n        return history\n\n    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n        bot, chatbot, chatbot\n    )\n    clear.click(lambda: None, None, chatbot, queue=False)\n    \ndemo.queue()\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/chatbot_consecutive'>\u003C/gradio-app>\n\nThe `.then()` method of an event listener executes the subsequent event regardless of whether the previous event raised any errors. If you'd like to only run subsequent events if the previous event executed successfully, use the `.success()` method, which takes the same arguments as `.then()`.\n\n## Running Events Continuously\n\nYou can run events on a fixed schedule using the `every` parameter of the event listener. This will run the event `every` number of seconds while the client connection is open. If the connection is closed, the event will stop running after the following iteration. Note that this does not take into account the runtime of the event itself. So a function with a 1 second runtime running with `every=5`, would actually run every 6 seconds. Also note that this parameter does not apply to the `js` function, only the Python function associated with the event listener.\n\nHere is an example of a sine curve that updates every second!\n\n```python\nimport math\nimport gradio as gr\nimport plotly.express as px\nimport numpy as np\n\n\nplot_end = 2 * math.pi\n\n\ndef get_plot(period=1):\n    global plot_end\n    x = np.arange(plot_end - 2 * math.pi, plot_end, 0.02)\n    y = np.sin(2*math.pi*period * x)\n    fig = px.line(x=x, y=y)\n    plot_end += 2 * math.pi\n    if plot_end > 1000:\n        plot_end = 2 * math.pi\n    return fig\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            gr.Markdown(\"Change the value of the slider to automatically update the plot\")\n            period = gr.Slider(label=\"Period of plot\", value=1, minimum=0, maximum=10, step=1)\n            plot = gr.Plot(label=\"Plot (updates every half second)\")\n\n    dep = demo.load(get_plot, None, plot, every=1)\n    period.change(get_plot, period, plot, every=1, cancels=[dep])\n\n\nif __name__ == \"__main__\":\n    demo.queue().launch()\n\n```\n\u003Cgradio-app space='gradio/sine_curve'>\u003C/gradio-app>\n\n## Gathering Event Data\n\nYou can gather specific data about an event by adding the associated event data class as a type hint to an argument in the event listener function.\n\nFor example, event data for `.select()` can be type hinted by a `gradio.SelectData` argument. This event is triggered when a user selects some part of the triggering component, and the event data includes information about what the user specifically selected. If a user selected a specific word in a `Textbox`, a specific image in a `Gallery`, or a specific cell in a `DataFrame`, the event data argument would contain information about the specific selection.\n\nIn the 2 player tic-tac-toe demo below, a user can select a cell in the `DataFrame` to make a move. The event data argument contains information about the specific cell that was selected. We can first check to see if the cell is empty, and then update the cell with the user's move.\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    turn = gr.Textbox(\"X\", interactive=False, label=\"Turn\")\n    board = gr.Dataframe(value=[[\"\", \"\", \"\"]] * 3, interactive=False, type=\"array\")\n\n    def place(board, turn, evt: gr.SelectData):\n        if evt.value:\n            return board, turn\n        board[evt.index[0]][evt.index[1]] = turn\n        turn = \"O\" if turn == \"X\" else \"X\"\n        return board, turn\n\n    board.select(place, [board, turn], [board, turn], show_progress=\"hidden\")\n\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/tictactoe'>\u003C/gradio-app>\n\n## Binding Multiple Triggers to a Function\n\nOften times, you may want to bind multiple triggers to the same function. For example, you may want to allow a user to click a submit button, or press enter to submit a form. You can do this using the `gr.on` method and passing a list of triggers to the `trigger`.\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    name = gr.Textbox(label=\"Name\")\n    output = gr.Textbox(label=\"Output Box\")\n    greet_btn = gr.Button(\"Greet\")\n    trigger = gr.Textbox(label=\"Trigger Box\")\n    trigger2 = gr.Textbox(label=\"Trigger Box\")\n\n    def greet(name, evt_data: gr.EventData):\n        return \"Hello \" + name + \"!\", evt_data.target.__class__.__name__\n    \n    def clear_name(evt_data: gr.EventData):\n        return \"\", evt_data.target.__class__.__name__\n    \n    gr.on(\n        triggers=[name.submit, greet_btn.click],\n        fn=greet,\n        inputs=name,\n        outputs=[output, trigger],\n    ).then(clear_name, outputs=[name, trigger2])\n\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/on_listener_basic'>\u003C/gradio-app>\n\nYou can use decorator syntax as well:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    name = gr.Textbox(label=\"Name\")\n    output = gr.Textbox(label=\"Output Box\")\n    greet_btn = gr.Button(\"Greet\")\n\n    @gr.on(triggers=[name.submit, greet_btn.click], inputs=name, outputs=output)\n    def greet(name):\n        return \"Hello \" + name + \"!\"\n\n\ndemo.launch()\n\n```\n\nYou can use `gr.on` to create \"live\" events by binding to the `change` event of components that implement it. If you do not specify any triggers, the function will automatically bind to all `change` event of all input components that include a `change` event (for example `gr.Textbox` has a `change` event whereas `gr.Button` does not).\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        num1 = gr.Slider(1, 10)\n        num2 = gr.Slider(1, 10)\n        num3 = gr.Slider(1, 10)\n    output = gr.Number(label=\"Sum\")\n\n    @gr.on(inputs=[num1, num2, num3], outputs=output)\n    def sum(a, b, c):\n        return a + b + c\n\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/on_listener_live'>\u003C/gradio-app>\n\nYou can follow `gr.on` with `.then`, just like any regular event listener. This handy method should save you from having to write a lot of repetitive code!\n",tags:[],spaces:[],url:"/guides/blocks-and-event-listeners/",contributor:null},{name:"controlling-layout",category:"building-with-blocks",pretty_category:"Building With Blocks",guide_index:2,absolute_index:16,pretty_name:"Controlling Layout",content:"# Controlling Layout\n\nBy default, Components in Blocks are arranged vertically. Let's take a look at how we can rearrange Components. Under the hood, this layout structure uses the [flexbox model of web development](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox).\n\n## Rows\n\nElements within a `with gr.Row` clause will all be displayed horizontally. For example, to display two Buttons side by side:\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        btn1 = gr.Button(\"Button 1\")\n        btn2 = gr.Button(\"Button 2\")\n```\n\nTo make every element in a Row have the same height, use the `equal_height` argument of the `style` method.\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row(equal_height=True):\n        textbox = gr.Textbox()\n        btn2 = gr.Button(\"Button 2\")\n```\n\nThe widths of elements in a Row can be controlled via a combination of `scale` and `min_width` arguments that are present in every Component.\n\n- `scale` is an integer that defines how an element will take up space in a Row. If scale is set to `0`, the element will not expand to take up space. If scale is set to `1` or greater, the element will expand. Multiple elements in a row will expand proportional to their scale. Below, `btn2` will expand twice as much as `btn1`, while `btn0` will not expand at all:\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        btn0 = gr.Button(\"Button 0\", scale=0)\n        btn1 = gr.Button(\"Button 1\", scale=1)\n        btn2 = gr.Button(\"Button 2\", scale=2)\n```\n\n- `min_width` will set the minimum width the element will take. The Row will wrap if there isn't sufficient space to satisfy all `min_width` values.\n\nLearn more about Rows in the [docs](https://gradio.app/docs/row).\n\n## Columns and Nesting\n\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        text1 = gr.Textbox(label=\"t1\")\n        slider2 = gr.Textbox(label=\"s2\")\n        drop3 = gr.Dropdown([\"a\", \"b\", \"c\"], label=\"d3\")\n    with gr.Row():\n        with gr.Column(scale=1, min_width=600):\n            text1 = gr.Textbox(label=\"prompt 1\")\n            text2 = gr.Textbox(label=\"prompt 2\")\n            inbtw = gr.Button(\"Between\")\n            text4 = gr.Textbox(label=\"prompt 1\")\n            text5 = gr.Textbox(label=\"prompt 2\")\n        with gr.Column(scale=2, min_width=600):\n            img1 = gr.Image(\"images/cheetah.jpg\")\n            btn = gr.Button(\"Go\")\n\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/rows_and_columns'>\u003C/gradio-app>\n\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width.\n\nLearn more about Columns in the [docs](https://gradio.app/docs/column).\n\n# Dimensions\n\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specific design requirements. When omitted, Gradio uses default dimensions suited for most use cases.\n\nBelow is an example illustrating the use of viewport width (vw):\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    im = gr.ImageEditor(\n        width=\"50vw\",\n    )\n\ndemo.launch()\n```\n\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\n\n\n```python\nimport gradio as gr\n\ncss = \"\"\"\n.container {\n    height: 100vh;\n}\n\"\"\"\n\nwith gr.Blocks(css=css) as demo:\n    with gr.Column(elem_classes=[\"container\"]):\n        name = gr.Chatbot(value=[[\"1\", \"2\"]], height=\"70%\")\n\ndemo.launch()\n```\n\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height.\n\nYou can apply any valid CSS unit for these parameters. For a comprehensive list of CSS units, refer to [this guide](https://www.w3schools.com/cssref/css_units.php). We recommend you always consider responsiveness and test your interfaces on various screen sizes to ensure a consistent user experience.\n\n\n\n## Tabs and Accordions\n\nYou can also create Tabs using the `with gr.Tab('tab_name'):` clause. Any component created inside of a `with gr.Tab('tab_name'):` context appears in that tab. Consecutive Tab clauses are grouped together so that a single tab can be selected at one time, and only the components within that Tab's context are shown.\n\nFor example:\n\n```python\nimport numpy as np\nimport gradio as gr\n\n\ndef flip_text(x):\n    return x[::-1]\n\n\ndef flip_image(x):\n    return np.fliplr(x)\n\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Flip text or image files using this demo.\")\n    with gr.Tab(\"Flip Text\"):\n        text_input = gr.Textbox()\n        text_output = gr.Textbox()\n        text_button = gr.Button(\"Flip\")\n    with gr.Tab(\"Flip Image\"):\n        with gr.Row():\n            image_input = gr.Image()\n            image_output = gr.Image()\n        image_button = gr.Button(\"Flip\")\n\n    with gr.Accordion(\"Open for More!\", open=False):\n        gr.Markdown(\"Look at me...\")\n        temp_slider = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=0.1,\n            step=0.1,\n            interactive=True,\n            label=\"Slide me\",\n        )\n        temp_slider.change(lambda x: x, [temp_slider])\n\n    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/blocks_flipper'>\u003C/gradio-app>\n\nAlso note the `gr.Accordion('label')` in this example. The Accordion is a layout that can be toggled open or closed. Like `Tabs`, it is a layout element that can selectively hide or show content. Any components that are defined inside of a `with gr.Accordion('label'):` will be hidden or shown when the accordion's toggle icon is clicked.\n\nLearn more about [Tabs](https://gradio.app/docs/tab) and [Accordions](https://gradio.app/docs/accordion) in the docs.\n\n## Visibility\n\nBoth Components and Layout elements have a `visible` argument that can set initially and also updated. Setting `gr.Column(visible=...)` on a Column can be used to show or hide a set of Components.\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    error_box = gr.Textbox(label=\"Error\", visible=False)\n\n    name_box = gr.Textbox(label=\"Name\")\n    age_box = gr.Number(label=\"Age\", minimum=0, maximum=100)\n    symptoms_box = gr.CheckboxGroup([\"Cough\", \"Fever\", \"Runny Nose\"])\n    submit_btn = gr.Button(\"Submit\")\n\n    with gr.Column(visible=False) as output_col:\n        diagnosis_box = gr.Textbox(label=\"Diagnosis\")\n        patient_summary_box = gr.Textbox(label=\"Patient Summary\")\n\n    def submit(name, age, symptoms):\n        if len(name) == 0:\n            return {error_box: gr.Textbox(value=\"Enter name\", visible=True)}\n        return {\n            output_col: gr.Column(visible=True),\n            diagnosis_box: \"covid\" if \"Cough\" in symptoms else \"flu\",\n            patient_summary_box: f\"{name}, {age} y/o\",\n        }\n\n    submit_btn.click(\n        submit,\n        [name_box, age_box, symptoms_box],\n        [error_box, diagnosis_box, patient_summary_box, output_col],\n    )\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/blocks_form'>\u003C/gradio-app>\n\n## Variable Number of Outputs\n\nBy adjusting the visibility of components in a dynamic way, it is possible to create\ndemos with Gradio that support a _variable numbers of outputs_. Here's a very simple example\nwhere the number of output textboxes is controlled by an input slider:\n\n```python\nimport gradio as gr\n\nmax_textboxes = 10\n\ndef variable_outputs(k):\n    k = int(k)\n    return [gr.Textbox(visible=True)]*k + [gr.Textbox(visible=False)]*(max_textboxes-k)\n\nwith gr.Blocks() as demo:\n    s = gr.Slider(1, max_textboxes, value=max_textboxes, step=1, label=\"How many textboxes to show:\")\n    textboxes = []\n    for i in range(max_textboxes):\n        t = gr.Textbox(f\"Textbox {i}\")\n        textboxes.append(t)\n\n    s.change(variable_outputs, s, textboxes)\n\nif __name__ == \"__main__\":\n   demo.launch()\n\n```\n\u003Cgradio-app space='gradio/variable_outputs'>\u003C/gradio-app>\n\n## Defining and Rendering Components Separately\n\nIn some cases, you might want to define components before you actually render them in your UI. For instance, you might want to show an examples section using `gr.Examples` above the corresponding `gr.Textbox` input. Since `gr.Examples` requires as a parameter the input component object, you will need to first define the input component, but then render it later, after you have defined the `gr.Examples` object.\n\nThe solution to this is to define the `gr.Textbox` outside of the `gr.Blocks()` scope and use the component's `.render()` method wherever you'd like it placed in the UI.\n\nHere's a full code example:\n\n```python\ninput_textbox = gr.Textbox()\n\nwith gr.Blocks() as demo:\n    gr.Examples([\"hello\", \"bonjour\", \"merhaba\"], input_textbox)\n    input_textbox.render()\n```\n",tags:[],spaces:[],url:"/guides/controlling-layout/",contributor:null},{name:"state-in-blocks",category:"building-with-blocks",pretty_category:"Building With Blocks",guide_index:3,absolute_index:17,pretty_name:"State In Blocks",content:"# State in Blocks\n\nWe covered [State in Interfaces](https://gradio.app/interface-state), this guide takes a look at state in Blocks, which works mostly the same.\n\n## Global State\n\nGlobal state in Blocks works the same as in Interface. Any variable created outside a function call is a reference shared between all users.\n\n## Session State\n\nGradio supports session **state**, where data persists across multiple submits within a page session, in Blocks apps as well. To reiterate, session data is _not_ shared between different users of your model. To store data in a session state, you need to do three things:\n\n1. Create a `gr.State()` object. If there is a default value to this stateful object, pass that into the constructor.\n2. In the event listener, put the `State` object as an input and output.\n3. In the event listener function, add the variable to the input parameters and the return value.\n\nLet's take a look at a game of hangman.\n\n```python\nimport gradio as gr\n\nsecret_word = \"gradio\"\n\nwith gr.Blocks() as demo:    \n    used_letters_var = gr.State([])\n    with gr.Row() as row:\n        with gr.Column():\n            input_letter = gr.Textbox(label=\"Enter letter\")\n            btn = gr.Button(\"Guess Letter\")\n        with gr.Column():\n            hangman = gr.Textbox(\n                label=\"Hangman\",\n                value=\"_\"*len(secret_word)\n            )\n            used_letters_box = gr.Textbox(label=\"Used Letters\")\n\n    def guess_letter(letter, used_letters):\n        used_letters.append(letter)\n        answer = \"\".join([\n            (letter if letter in used_letters else \"_\")\n            for letter in secret_word\n        ])\n        return {\n            used_letters_var: used_letters,\n            used_letters_box: \", \".join(used_letters),\n            hangman: answer\n        }\n    btn.click(\n        guess_letter, \n        [input_letter, used_letters_var],\n        [used_letters_var, used_letters_box, hangman]\n        )\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/hangman'>\u003C/gradio-app>\n\nLet's see how we do each of the 3 steps listed above in this game:\n\n1. We store the used letters in `used_letters_var`. In the constructor of `State`, we set the initial value of this to `[]`, an empty list.\n2. In `btn.click()`, we have a reference to `used_letters_var` in both the inputs and outputs.\n3. In `guess_letter`, we pass the value of this `State` to `used_letters`, and then return an updated value of this `State` in the return statement.\n\nWith more complex apps, you will likely have many State variables storing session state in a single Blocks app.\n\nLearn more about `State` in the [docs](https://gradio.app/docs/state).\n",tags:[],spaces:[],url:"/guides/state-in-blocks/",contributor:null},{name:"custom-CSS-and-JS",category:"building-with-blocks",pretty_category:"Building With Blocks",guide_index:5,absolute_index:19,pretty_name:"Custom CSS And JS",content:"# Customizing your demo with CSS and Javascript\n\nGradio allows you to customize your demo in several ways. You can customize the layout of your demo, add custom HTML, and add custom theming as well. This tutorial will go beyond that and walk you through how to add custom CSS and JavaScript code to your demo in order to add custom styling, animations, custom UI functionality, analytics, and more.\n\n## Adding custom CSS to your demo\n\nGradio themes are the easiest way to customize the look and feel of your app. You can choose from a variety of themes, or create your own. To do so, pass the `theme=` kwarg to the `Blocks` constructor. For example:\n\n```python\nwith gr.Blocks(theme=gr.themes.Glass()):\n    ...\n```\n\nGradio comes with a set of prebuilt themes which you can load from `gr.themes.*`. You can extend these themes or create your own themes from scratch - see the [Theming guide](/guides/theming-guide) for more details.\n\nFor additional styling ability, you can pass any CSS to your app using the `css=` kwarg. You can either the filepath to a CSS file, or a string of CSS code.\n\n**Warning**: The use of query selectors in custom JS and CSS is _not_ guaranteed to work across Gradio versions as the Gradio HTML DOM may change. We recommend using query selectors sparingly.\n\nThe base class for the Gradio app is `gradio-container`, so here's an example that changes the background color of the Gradio app:\n\n```python\nwith gr.Blocks(css=\".gradio-container {background-color: red}\") as demo:\n    ...\n```\n\nIf you'd like to reference external files in your css, preface the file path (which can be a relative or absolute path) with `\"file=\"`, for example:\n\n```python\nwith gr.Blocks(css=\".gradio-container {background: url('file=clouds.jpg')}\") as demo:\n    ...\n```\n\nNote: By default, files in the host machine are not accessible to users running the Gradio app. As a result, you should make sure that any referenced files (such as `clouds.jpg` here) are either URLs or allowed via the `allow_list` parameter in `launch()`. Read more in our [section on Security and File Access](/guides/sharing-your-app#security-and-file-access).\n\n\n## The `elem_id` and `elem_classes` Arguments\n\nYou can `elem_id` to add an HTML element `id` to any component, and `elem_classes` to add a class or list of classes. This will allow you to select elements more easily with CSS. This approach is also more likely to be stable across Gradio versions as built-in class names or ids may change (however, as mentioned in the warning above, we cannot guarantee complete compatibility between Gradio versions if you use custom CSS as the DOM elements may themselves change).\n\n```python\ncss = \"\"\"\n#warning {background-color: #FFCCCB}\n.feedback textarea {font-size: 24px !important}\n\"\"\"\n\nwith gr.Blocks(css=css) as demo:\n    box1 = gr.Textbox(value=\"Good Job\", elem_classes=\"feedback\")\n    box2 = gr.Textbox(value=\"Failure\", elem_id=\"warning\", elem_classes=\"feedback\")\n```\n\nThe CSS `#warning` ruleset will only target the second Textbox, while the `.feedback` ruleset will target both. Note that when targeting classes, you might need to put the `!important` selector to override the default Gradio styles.\n\n## Adding custom JavaScript to your demo\n\nThere are 3 ways to add javascript code to your Gradio demo:\n\n1. You can add JavaScript code as a string or as a filepath to the `js` parameter of the `Blocks` or `Interface` initializer. This will run the JavaScript code when the demo is first loaded.\n\nBelow is an example of adding custom js to show an animated welcome message when the demo first loads.\n\n```python\nimport gradio as gr\n\ndef welcome(name):\n    return f\"Welcome to Gradio, {name}!\"\n\njs = \"\"\"\nfunction createGradioAnimation() {\n    var container = document.createElement('div');\n    container.id = 'gradio-animation';\n    container.style.fontSize = '2em';\n    container.style.fontWeight = 'bold';\n    container.style.textAlign = 'center';\n    container.style.marginBottom = '20px';\n\n    var text = 'Welcome to Gradio!';\n    for (var i = 0; i \u003C text.length; i++) {\n        (function(i){\n            setTimeout(function(){\n                var letter = document.createElement('span');\n                letter.style.opacity = '0';\n                letter.style.transition = 'opacity 0.5s';\n                letter.innerText = text[i];\n\n                container.appendChild(letter);\n\n                setTimeout(function() {\n                    letter.style.opacity = '1';\n                }, 50);\n            }, i * 250);\n        })(i);\n    }\n\n    var gradioContainer = document.querySelector('.gradio-container');\n    gradioContainer.insertBefore(container, gradioContainer.firstChild);\n\n    return 'Animation created';\n}\n\"\"\"\nwith gr.Blocks(js=js) as demo:\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n    inp.change(welcome, inp, out)\n\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/blocks_js_load'>\u003C/gradio-app>\n\nNote: You can also supply your custom js code as a file path. For example, if you have a file called `custom.js` in the same directory as your Python script, you can add it to your demo like so: `with gr.Blocks(js=\"custom.js\") as demo:`. Same goes for `Interface` (ex: `gr.Interface(..., js=\"custom.js\")`).\n\n2. When using `Blocks` and event listeners, events have a `js` argument that can take a JavaScript function as a string and treat it just like a Python event listener function. You can pass both a JavaScript function and a Python function (in which case the JavaScript function is run first) or only Javascript (and set the Python `fn` to `None`). Take a look at the code below:\n   \n```python\nimport gradio as gr\n\nblocks = gr.Blocks()\n\nwith blocks as demo:\n    subject = gr.Textbox(placeholder=\"subject\")\n    verb = gr.Radio([\"ate\", \"loved\", \"hated\"])\n    object = gr.Textbox(placeholder=\"object\")\n\n    with gr.Row():\n        btn = gr.Button(\"Create sentence.\")\n        reverse_btn = gr.Button(\"Reverse sentence.\")\n        foo_bar_btn = gr.Button(\"Append foo\")\n        reverse_then_to_the_server_btn = gr.Button(\n            \"Reverse sentence and send to server.\"\n        )\n\n    def sentence_maker(w1, w2, w3):\n        return f\"{w1} {w2} {w3}\"\n\n    output1 = gr.Textbox(label=\"output 1\")\n    output2 = gr.Textbox(label=\"verb\")\n    output3 = gr.Textbox(label=\"verb reversed\")\n    output4 = gr.Textbox(label=\"front end process and then send to backend\")\n\n    btn.click(sentence_maker, [subject, verb, object], output1)\n    reverse_btn.click(\n        None, [subject, verb, object], output2, js=\"(s, v, o) => o + ' ' + v + ' ' + s\"\n    )\n    verb.change(lambda x: x, verb, output3, js=\"(x) => [...x].reverse().join('')\")\n    foo_bar_btn.click(None, [], subject, js=\"(x) => x + ' foo'\")\n\n    reverse_then_to_the_server_btn.click(\n        sentence_maker,\n        [subject, verb, object],\n        output4,\n        js=\"(s, v, o) => [s, v, o].map(x => [...x].reverse().join(''))\",\n    )\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/blocks_js_methods'>\u003C/gradio-app>\n\n3. Lastly, you can add JavaScript code to the `head` param of the `Blocks` initializer. This will add the code to the head of the HTML document. For example, you can add Google Analytics to your demo like so:\n\n\n```python\nhead = f\"\"\"\n\u003Cscript async src=\"https://www.googletagmanager.com/gtag/js?id={google_analytics_tracking_id}\">\u003C/script>\n\u003Cscript>\n  window.dataLayer = window.dataLayer || [];\n  function gtag(){{dataLayer.push(arguments);}}\n  gtag('js', new Date());\n  gtag('config', '{google_analytics_tracking_id}');\n\u003C/script>\n\"\"\"\n\nwith gr.Blocks(head=head) as demo:\n    ...demo code...\n```\n\nThe `head` parameter accepts any HTML tags you would normally insert into the `\u003Chead>` of a page. For example, you can also include `\u003Cmeta>` tags to `head`.\n\nNote that injecting custom HTML can affect browser behavior and compatibility (e.g. keyboard shortcuts). You should test your interface across different browsers and be mindful of how scripts may interact with browser defaults.\nHere's an example where pressing `Shift + s` triggers the `click` event of a specific `Button` component if the browser focus is _not_ on an input component (e.g. `Textbox` component):\n\n```python\nimport gradio as gr\n\nshortcut_js = \"\"\"\n\u003Cscript>\nfunction shortcuts(e) {\n    var event = document.all ? window.event : e;\n    switch (e.target.tagName.toLowerCase()) {\n        case \"input\":\n        case \"textarea\":\n        break;\n        default:\n        if (e.key.toLowerCase() == \"s\" && e.shiftKey) {\n            document.getElementById(\"my_btn\").click();\n        }\n    }\n}\ndocument.addEventListener('keypress', shortcuts, false);\n\u003C/script>\n\"\"\"\n\nwith gr.Blocks(head=shortcut_js) as demo:\n    action_button = gr.Button(value=\"Name\", elem_id=\"my_btn\")\n    textbox = gr.Textbox()\n    action_button.click(lambda : \"button pressed\", None, textbox)\n    \ndemo.launch()\n```\n",tags:[],spaces:[],url:"/guides/custom-CSS-and-JS/",contributor:null},{name:"using-blocks-like-functions",category:"building-with-blocks",pretty_category:"Building With Blocks",guide_index:6,absolute_index:20,pretty_name:"Using Blocks Like Functions",content:"# Using Gradio Blocks Like Functions\n\n\n\n**Prerequisite**: This Guide builds on the Blocks Introduction. Make sure to [read that guide first](https://gradio.app/blocks-and-event-listeners).\n\n## Introduction\n\nDid you know that apart from being a full-stack machine learning demo, a Gradio Blocks app is also a regular-old python function!?\n\nThis means that if you have a gradio Blocks (or Interface) app called `demo`, you can use `demo` like you would any python function.\n\nSo doing something like `output = demo(\"Hello\", \"friend\")` will run the first event defined in `demo` on the inputs \"Hello\" and \"friend\" and store it\nin the variable `output`.\n\nIf I put you to sleep ü•±, please bear with me! By using apps like functions, you can seamlessly compose Gradio apps.\nThe following section will show how.\n\n## Treating Blocks like functions\n\nLet's say we have the following demo that translates english text to german text.\n\n```python\nimport gradio as gr\n\nfrom transformers import pipeline\n\npipe = pipeline(\"translation\", model=\"t5-base\")\n\n\ndef translate(text):\n    return pipe(text)[0][\"translation_text\"]\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            english = gr.Textbox(label=\"English text\")\n            translate_btn = gr.Button(value=\"Translate\")\n        with gr.Column():\n            german = gr.Textbox(label=\"German Text\")\n\n    translate_btn.click(translate, inputs=english, outputs=german, api_name=\"translate-to-german\")\n    examples = gr.Examples(examples=[\"I went to the supermarket yesterday.\", \"Helen is a good swimmer.\"],\n                           inputs=[english])\n\ndemo.launch()\n```\n\nI already went ahead and hosted it in Hugging Face spaces at [gradio/english_translator](https://huggingface.co/spaces/gradio/english_translator).\n\nYou can see the demo below as well:\n\n\u003Cgradio-app space='gradio/english_translator'>\u003C/gradio-app>\n\nNow, let's say you have an app that generates english text, but you wanted to additionally generate german text.\n\nYou could either:\n\n1. Copy the source code of my english-to-german translation and paste it in your app.\n\n2. Load my english-to-german translation in your app and treat it like a normal python function.\n\nOption 1 technically always works, but it often introduces unwanted complexity.\n\nOption 2 lets you borrow the functionality you want without tightly coupling our apps.\n\nAll you have to do is call the `Blocks.load` class method in your source file.\nAfter that, you can use my translation app like a regular python function!\n\nThe following code snippet and demo shows how to use `Blocks.load`.\n\nNote that the variable `english_translator` is my english to german app, but its used in `generate_text` like a regular function.\n\n```python\nimport gradio as gr\n\nfrom transformers import pipeline\n\nenglish_translator = gr.load(name=\"spaces/gradio/english_translator\")\nenglish_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n\n\ndef generate_text(text):\n    english_text = english_generator(text)[0][\"generated_text\"]\n    german_text = english_translator(english_text)\n    return english_text, german_text\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            seed = gr.Text(label=\"Input Phrase\")\n        with gr.Column():\n            english = gr.Text(label=\"Generated English Text\")\n            german = gr.Text(label=\"Generated German Text\")\n    btn = gr.Button(\"Generate\")\n    btn.click(generate_text, inputs=[seed], outputs=[english, german])\n    gr.Examples([\"My name is Clara and I am\"], inputs=[seed])\n\ndemo.launch()\n```\n\n\u003Cgradio-app space='gradio/generate_english_german'>\u003C/gradio-app>\n\n## How to control which function in the app to use\n\nIf the app you are loading defines more than one function, you can specify which function to use\nwith the `fn_index` and `api_name` parameters.\n\nIn the code for our english to german demo, you'll see the following line:\n\n```python\ntranslate_btn.click(translate, inputs=english, outputs=german, api_name=\"translate-to-german\")\n```\n\nThe `api_name` gives this function a unique name in our app. You can use this name to tell gradio which\nfunction in the upstream space you want to use:\n\n```python\nenglish_generator(text, api_name=\"translate-to-german\")[0][\"generated_text\"]\n```\n\nYou can also use the `fn_index` parameter.\nImagine my app also defined an english to spanish translation function.\nIn order to use it in our text generation app, we would use the following code:\n\n```python\nenglish_generator(text, fn_index=1)[0][\"generated_text\"]\n```\n\nFunctions in gradio spaces are zero-indexed, so since the spanish translator would be the second function in my space,\nyou would use index 1.\n\n## Parting Remarks\n\nWe showed how treating a Blocks app like a regular python helps you compose functionality across different apps.\nAny Blocks app can be treated like a function, but a powerful pattern is to `load` an app hosted on\n[Hugging Face Spaces](https://huggingface.co/spaces) prior to treating it like a function in your own app.\nYou can also load models hosted on the [Hugging Face Model Hub](https://huggingface.co/models) - see the [Using Hugging Face Integrations](/using_hugging_face_integrations) guide for an example.\n\n### Happy building! ‚öíÔ∏è\n",tags:["TRANSLATION","HUB","SPACES"],spaces:[],url:"/guides/using-blocks-like-functions/",contributor:null}],parent:"gradio"},accordion:{class:null,name:"Accordion",description:"Accordion is a layout element which can be toggled to show/hide the contained content.",tags:{},parameters:[{name:"label",annotation:"str | None",doc:"name of accordion section.",default:"None"},{name:"open",annotation:"bool",doc:"if True, accordion is open by default.",default:"True"},{name:"visible",annotation:"bool",doc:null,default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional string or list of strings that are assigned as the class of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, this layout will not be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"}],returns:{annotation:null},example:"with gr.Accordion(\"See Details\"):\n    gr.Markdown(\"lorem ipsum\")",fns:[],parent:"gradio"},column:{class:null,name:"Column",description:"Column is a layout element within Blocks that renders all children vertically. The widths of columns can be set through the `scale` and `min_width` parameters. If a certain scale results in a column narrower than min_width, the min_width parameter will win.",tags:{guides:"controlling-layout"},parameters:[{name:"scale",annotation:"int",doc:"relative width compared to adjacent Columns. For example, if Column A has scale=2, and Column B has scale=1, A will be twice as wide as B.",default:"1"},{name:"min_width",annotation:"int",doc:"minimum pixel width of Column, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in a column narrower than min_width, the min_width parameter will be respected first.",default:"320"},{name:"variant",annotation:"Literal[('default', 'panel', 'compact')]",doc:"column type, &#x27;default&#x27; (no background), &#x27;panel&#x27; (gray background color and rounded corners), or &#x27;compact&#x27; (rounded corners and no internal gap).",default:"\"default\""},{name:"visible",annotation:"bool",doc:"If False, column will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional string or list of strings that are assigned as the class of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"show_progress",annotation:"bool",doc:"If True, shows progress animation when being updated.",default:"False"}],returns:{annotation:null},example:"with gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column(scale=1):\n            text1 = gr.Textbox()\n            text2 = gr.Textbox()\n        with gr.Column(scale=4):\n            btn1 = gr.Button(\"Button 1\")\n            btn2 = gr.Button(\"Button 2\")",fns:[],guides:[{name:"controlling-layout",category:"building-with-blocks",pretty_category:"Building With Blocks",guide_index:2,absolute_index:16,pretty_name:"Controlling Layout",content:"# Controlling Layout\n\nBy default, Components in Blocks are arranged vertically. Let's take a look at how we can rearrange Components. Under the hood, this layout structure uses the [flexbox model of web development](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox).\n\n## Rows\n\nElements within a `with gr.Row` clause will all be displayed horizontally. For example, to display two Buttons side by side:\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        btn1 = gr.Button(\"Button 1\")\n        btn2 = gr.Button(\"Button 2\")\n```\n\nTo make every element in a Row have the same height, use the `equal_height` argument of the `style` method.\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row(equal_height=True):\n        textbox = gr.Textbox()\n        btn2 = gr.Button(\"Button 2\")\n```\n\nThe widths of elements in a Row can be controlled via a combination of `scale` and `min_width` arguments that are present in every Component.\n\n- `scale` is an integer that defines how an element will take up space in a Row. If scale is set to `0`, the element will not expand to take up space. If scale is set to `1` or greater, the element will expand. Multiple elements in a row will expand proportional to their scale. Below, `btn2` will expand twice as much as `btn1`, while `btn0` will not expand at all:\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        btn0 = gr.Button(\"Button 0\", scale=0)\n        btn1 = gr.Button(\"Button 1\", scale=1)\n        btn2 = gr.Button(\"Button 2\", scale=2)\n```\n\n- `min_width` will set the minimum width the element will take. The Row will wrap if there isn't sufficient space to satisfy all `min_width` values.\n\nLearn more about Rows in the [docs](https://gradio.app/docs/row).\n\n## Columns and Nesting\n\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        text1 = gr.Textbox(label=\"t1\")\n        slider2 = gr.Textbox(label=\"s2\")\n        drop3 = gr.Dropdown([\"a\", \"b\", \"c\"], label=\"d3\")\n    with gr.Row():\n        with gr.Column(scale=1, min_width=600):\n            text1 = gr.Textbox(label=\"prompt 1\")\n            text2 = gr.Textbox(label=\"prompt 2\")\n            inbtw = gr.Button(\"Between\")\n            text4 = gr.Textbox(label=\"prompt 1\")\n            text5 = gr.Textbox(label=\"prompt 2\")\n        with gr.Column(scale=2, min_width=600):\n            img1 = gr.Image(\"images/cheetah.jpg\")\n            btn = gr.Button(\"Go\")\n\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/rows_and_columns'>\u003C/gradio-app>\n\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width.\n\nLearn more about Columns in the [docs](https://gradio.app/docs/column).\n\n# Dimensions\n\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specific design requirements. When omitted, Gradio uses default dimensions suited for most use cases.\n\nBelow is an example illustrating the use of viewport width (vw):\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    im = gr.ImageEditor(\n        width=\"50vw\",\n    )\n\ndemo.launch()\n```\n\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\n\n\n```python\nimport gradio as gr\n\ncss = \"\"\"\n.container {\n    height: 100vh;\n}\n\"\"\"\n\nwith gr.Blocks(css=css) as demo:\n    with gr.Column(elem_classes=[\"container\"]):\n        name = gr.Chatbot(value=[[\"1\", \"2\"]], height=\"70%\")\n\ndemo.launch()\n```\n\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height.\n\nYou can apply any valid CSS unit for these parameters. For a comprehensive list of CSS units, refer to [this guide](https://www.w3schools.com/cssref/css_units.php). We recommend you always consider responsiveness and test your interfaces on various screen sizes to ensure a consistent user experience.\n\n\n\n## Tabs and Accordions\n\nYou can also create Tabs using the `with gr.Tab('tab_name'):` clause. Any component created inside of a `with gr.Tab('tab_name'):` context appears in that tab. Consecutive Tab clauses are grouped together so that a single tab can be selected at one time, and only the components within that Tab's context are shown.\n\nFor example:\n\n```python\nimport numpy as np\nimport gradio as gr\n\n\ndef flip_text(x):\n    return x[::-1]\n\n\ndef flip_image(x):\n    return np.fliplr(x)\n\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Flip text or image files using this demo.\")\n    with gr.Tab(\"Flip Text\"):\n        text_input = gr.Textbox()\n        text_output = gr.Textbox()\n        text_button = gr.Button(\"Flip\")\n    with gr.Tab(\"Flip Image\"):\n        with gr.Row():\n            image_input = gr.Image()\n            image_output = gr.Image()\n        image_button = gr.Button(\"Flip\")\n\n    with gr.Accordion(\"Open for More!\", open=False):\n        gr.Markdown(\"Look at me...\")\n        temp_slider = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=0.1,\n            step=0.1,\n            interactive=True,\n            label=\"Slide me\",\n        )\n        temp_slider.change(lambda x: x, [temp_slider])\n\n    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/blocks_flipper'>\u003C/gradio-app>\n\nAlso note the `gr.Accordion('label')` in this example. The Accordion is a layout that can be toggled open or closed. Like `Tabs`, it is a layout element that can selectively hide or show content. Any components that are defined inside of a `with gr.Accordion('label'):` will be hidden or shown when the accordion's toggle icon is clicked.\n\nLearn more about [Tabs](https://gradio.app/docs/tab) and [Accordions](https://gradio.app/docs/accordion) in the docs.\n\n## Visibility\n\nBoth Components and Layout elements have a `visible` argument that can set initially and also updated. Setting `gr.Column(visible=...)` on a Column can be used to show or hide a set of Components.\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    error_box = gr.Textbox(label=\"Error\", visible=False)\n\n    name_box = gr.Textbox(label=\"Name\")\n    age_box = gr.Number(label=\"Age\", minimum=0, maximum=100)\n    symptoms_box = gr.CheckboxGroup([\"Cough\", \"Fever\", \"Runny Nose\"])\n    submit_btn = gr.Button(\"Submit\")\n\n    with gr.Column(visible=False) as output_col:\n        diagnosis_box = gr.Textbox(label=\"Diagnosis\")\n        patient_summary_box = gr.Textbox(label=\"Patient Summary\")\n\n    def submit(name, age, symptoms):\n        if len(name) == 0:\n            return {error_box: gr.Textbox(value=\"Enter name\", visible=True)}\n        return {\n            output_col: gr.Column(visible=True),\n            diagnosis_box: \"covid\" if \"Cough\" in symptoms else \"flu\",\n            patient_summary_box: f\"{name}, {age} y/o\",\n        }\n\n    submit_btn.click(\n        submit,\n        [name_box, age_box, symptoms_box],\n        [error_box, diagnosis_box, patient_summary_box, output_col],\n    )\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/blocks_form'>\u003C/gradio-app>\n\n## Variable Number of Outputs\n\nBy adjusting the visibility of components in a dynamic way, it is possible to create\ndemos with Gradio that support a _variable numbers of outputs_. Here's a very simple example\nwhere the number of output textboxes is controlled by an input slider:\n\n```python\nimport gradio as gr\n\nmax_textboxes = 10\n\ndef variable_outputs(k):\n    k = int(k)\n    return [gr.Textbox(visible=True)]*k + [gr.Textbox(visible=False)]*(max_textboxes-k)\n\nwith gr.Blocks() as demo:\n    s = gr.Slider(1, max_textboxes, value=max_textboxes, step=1, label=\"How many textboxes to show:\")\n    textboxes = []\n    for i in range(max_textboxes):\n        t = gr.Textbox(f\"Textbox {i}\")\n        textboxes.append(t)\n\n    s.change(variable_outputs, s, textboxes)\n\nif __name__ == \"__main__\":\n   demo.launch()\n\n```\n\u003Cgradio-app space='gradio/variable_outputs'>\u003C/gradio-app>\n\n## Defining and Rendering Components Separately\n\nIn some cases, you might want to define components before you actually render them in your UI. For instance, you might want to show an examples section using `gr.Examples` above the corresponding `gr.Textbox` input. Since `gr.Examples` requires as a parameter the input component object, you will need to first define the input component, but then render it later, after you have defined the `gr.Examples` object.\n\nThe solution to this is to define the `gr.Textbox` outside of the `gr.Blocks()` scope and use the component's `.render()` method wherever you'd like it placed in the UI.\n\nHere's a full code example:\n\n```python\ninput_textbox = gr.Textbox()\n\nwith gr.Blocks() as demo:\n    gr.Examples([\"hello\", \"bonjour\", \"merhaba\"], input_textbox)\n    input_textbox.render()\n```\n",tags:[],spaces:[],url:"/guides/controlling-layout/",contributor:null}],parent:"gradio"},row:{class:null,name:"Row",description:"Row is a layout element within Blocks that renders all children horizontally.",tags:{guides:"controlling-layout"},parameters:[{name:"variant",annotation:"Literal[('default', 'panel', 'compact')]",doc:"row type, &#x27;default&#x27; (no background), &#x27;panel&#x27; (gray background color and rounded corners), or &#x27;compact&#x27; (rounded corners and no internal gap).",default:"\"default\""},{name:"visible",annotation:"bool",doc:"If False, row will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional string or list of strings that are assigned as the class of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, this layout will not be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"equal_height",annotation:"bool",doc:"If True, makes every child element have equal height",default:"True"},{name:"show_progress",annotation:"bool",doc:"If True, shows progress animation when being updated.",default:"False"}],returns:{annotation:null},example:"with gr.Blocks() as demo:\n    with gr.Row():\n        gr.Image(\"lion.jpg\", scale=2)\n        gr.Image(\"tiger.jpg\", scale=1)\ndemo.launch()",fns:[],guides:[{name:"controlling-layout",category:"building-with-blocks",pretty_category:"Building With Blocks",guide_index:2,absolute_index:16,pretty_name:"Controlling Layout",content:"# Controlling Layout\n\nBy default, Components in Blocks are arranged vertically. Let's take a look at how we can rearrange Components. Under the hood, this layout structure uses the [flexbox model of web development](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox).\n\n## Rows\n\nElements within a `with gr.Row` clause will all be displayed horizontally. For example, to display two Buttons side by side:\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        btn1 = gr.Button(\"Button 1\")\n        btn2 = gr.Button(\"Button 2\")\n```\n\nTo make every element in a Row have the same height, use the `equal_height` argument of the `style` method.\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row(equal_height=True):\n        textbox = gr.Textbox()\n        btn2 = gr.Button(\"Button 2\")\n```\n\nThe widths of elements in a Row can be controlled via a combination of `scale` and `min_width` arguments that are present in every Component.\n\n- `scale` is an integer that defines how an element will take up space in a Row. If scale is set to `0`, the element will not expand to take up space. If scale is set to `1` or greater, the element will expand. Multiple elements in a row will expand proportional to their scale. Below, `btn2` will expand twice as much as `btn1`, while `btn0` will not expand at all:\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        btn0 = gr.Button(\"Button 0\", scale=0)\n        btn1 = gr.Button(\"Button 1\", scale=1)\n        btn2 = gr.Button(\"Button 2\", scale=2)\n```\n\n- `min_width` will set the minimum width the element will take. The Row will wrap if there isn't sufficient space to satisfy all `min_width` values.\n\nLearn more about Rows in the [docs](https://gradio.app/docs/row).\n\n## Columns and Nesting\n\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        text1 = gr.Textbox(label=\"t1\")\n        slider2 = gr.Textbox(label=\"s2\")\n        drop3 = gr.Dropdown([\"a\", \"b\", \"c\"], label=\"d3\")\n    with gr.Row():\n        with gr.Column(scale=1, min_width=600):\n            text1 = gr.Textbox(label=\"prompt 1\")\n            text2 = gr.Textbox(label=\"prompt 2\")\n            inbtw = gr.Button(\"Between\")\n            text4 = gr.Textbox(label=\"prompt 1\")\n            text5 = gr.Textbox(label=\"prompt 2\")\n        with gr.Column(scale=2, min_width=600):\n            img1 = gr.Image(\"images/cheetah.jpg\")\n            btn = gr.Button(\"Go\")\n\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/rows_and_columns'>\u003C/gradio-app>\n\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width.\n\nLearn more about Columns in the [docs](https://gradio.app/docs/column).\n\n# Dimensions\n\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specific design requirements. When omitted, Gradio uses default dimensions suited for most use cases.\n\nBelow is an example illustrating the use of viewport width (vw):\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    im = gr.ImageEditor(\n        width=\"50vw\",\n    )\n\ndemo.launch()\n```\n\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\n\n\n```python\nimport gradio as gr\n\ncss = \"\"\"\n.container {\n    height: 100vh;\n}\n\"\"\"\n\nwith gr.Blocks(css=css) as demo:\n    with gr.Column(elem_classes=[\"container\"]):\n        name = gr.Chatbot(value=[[\"1\", \"2\"]], height=\"70%\")\n\ndemo.launch()\n```\n\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height.\n\nYou can apply any valid CSS unit for these parameters. For a comprehensive list of CSS units, refer to [this guide](https://www.w3schools.com/cssref/css_units.php). We recommend you always consider responsiveness and test your interfaces on various screen sizes to ensure a consistent user experience.\n\n\n\n## Tabs and Accordions\n\nYou can also create Tabs using the `with gr.Tab('tab_name'):` clause. Any component created inside of a `with gr.Tab('tab_name'):` context appears in that tab. Consecutive Tab clauses are grouped together so that a single tab can be selected at one time, and only the components within that Tab's context are shown.\n\nFor example:\n\n```python\nimport numpy as np\nimport gradio as gr\n\n\ndef flip_text(x):\n    return x[::-1]\n\n\ndef flip_image(x):\n    return np.fliplr(x)\n\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Flip text or image files using this demo.\")\n    with gr.Tab(\"Flip Text\"):\n        text_input = gr.Textbox()\n        text_output = gr.Textbox()\n        text_button = gr.Button(\"Flip\")\n    with gr.Tab(\"Flip Image\"):\n        with gr.Row():\n            image_input = gr.Image()\n            image_output = gr.Image()\n        image_button = gr.Button(\"Flip\")\n\n    with gr.Accordion(\"Open for More!\", open=False):\n        gr.Markdown(\"Look at me...\")\n        temp_slider = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=0.1,\n            step=0.1,\n            interactive=True,\n            label=\"Slide me\",\n        )\n        temp_slider.change(lambda x: x, [temp_slider])\n\n    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/blocks_flipper'>\u003C/gradio-app>\n\nAlso note the `gr.Accordion('label')` in this example. The Accordion is a layout that can be toggled open or closed. Like `Tabs`, it is a layout element that can selectively hide or show content. Any components that are defined inside of a `with gr.Accordion('label'):` will be hidden or shown when the accordion's toggle icon is clicked.\n\nLearn more about [Tabs](https://gradio.app/docs/tab) and [Accordions](https://gradio.app/docs/accordion) in the docs.\n\n## Visibility\n\nBoth Components and Layout elements have a `visible` argument that can set initially and also updated. Setting `gr.Column(visible=...)` on a Column can be used to show or hide a set of Components.\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    error_box = gr.Textbox(label=\"Error\", visible=False)\n\n    name_box = gr.Textbox(label=\"Name\")\n    age_box = gr.Number(label=\"Age\", minimum=0, maximum=100)\n    symptoms_box = gr.CheckboxGroup([\"Cough\", \"Fever\", \"Runny Nose\"])\n    submit_btn = gr.Button(\"Submit\")\n\n    with gr.Column(visible=False) as output_col:\n        diagnosis_box = gr.Textbox(label=\"Diagnosis\")\n        patient_summary_box = gr.Textbox(label=\"Patient Summary\")\n\n    def submit(name, age, symptoms):\n        if len(name) == 0:\n            return {error_box: gr.Textbox(value=\"Enter name\", visible=True)}\n        return {\n            output_col: gr.Column(visible=True),\n            diagnosis_box: \"covid\" if \"Cough\" in symptoms else \"flu\",\n            patient_summary_box: f\"{name}, {age} y/o\",\n        }\n\n    submit_btn.click(\n        submit,\n        [name_box, age_box, symptoms_box],\n        [error_box, diagnosis_box, patient_summary_box, output_col],\n    )\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/blocks_form'>\u003C/gradio-app>\n\n## Variable Number of Outputs\n\nBy adjusting the visibility of components in a dynamic way, it is possible to create\ndemos with Gradio that support a _variable numbers of outputs_. Here's a very simple example\nwhere the number of output textboxes is controlled by an input slider:\n\n```python\nimport gradio as gr\n\nmax_textboxes = 10\n\ndef variable_outputs(k):\n    k = int(k)\n    return [gr.Textbox(visible=True)]*k + [gr.Textbox(visible=False)]*(max_textboxes-k)\n\nwith gr.Blocks() as demo:\n    s = gr.Slider(1, max_textboxes, value=max_textboxes, step=1, label=\"How many textboxes to show:\")\n    textboxes = []\n    for i in range(max_textboxes):\n        t = gr.Textbox(f\"Textbox {i}\")\n        textboxes.append(t)\n\n    s.change(variable_outputs, s, textboxes)\n\nif __name__ == \"__main__\":\n   demo.launch()\n\n```\n\u003Cgradio-app space='gradio/variable_outputs'>\u003C/gradio-app>\n\n## Defining and Rendering Components Separately\n\nIn some cases, you might want to define components before you actually render them in your UI. For instance, you might want to show an examples section using `gr.Examples` above the corresponding `gr.Textbox` input. Since `gr.Examples` requires as a parameter the input component object, you will need to first define the input component, but then render it later, after you have defined the `gr.Examples` object.\n\nThe solution to this is to define the `gr.Textbox` outside of the `gr.Blocks()` scope and use the component's `.render()` method wherever you'd like it placed in the UI.\n\nHere's a full code example:\n\n```python\ninput_textbox = gr.Textbox()\n\nwith gr.Blocks() as demo:\n    gr.Examples([\"hello\", \"bonjour\", \"merhaba\"], input_textbox)\n    input_textbox.render()\n```\n",tags:[],spaces:[],url:"/guides/controlling-layout/",contributor:null}],parent:"gradio"},group:{class:null,name:"Group",description:"Group is a layout element within Blocks which groups together children so that they do not have any padding or margin between them.",tags:{},parameters:[{name:"visible",annotation:"bool",doc:"If False, group will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional string or list of strings that are assigned as the class of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, this layout will not be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"}],returns:{annotation:null},example:"with gr.Group():\n    gr.Textbox(label=\"First\")\n    gr.Textbox(label=\"Last\")",fns:[],parent:"gradio"},tab:{class:null,name:"Tab",description:"Tab (or its alias TabItem) is a layout element. Components defined within the Tab will be visible when this tab is selected tab.",tags:{guides:"controlling-layout"},parameters:[{name:"label",annotation:"str | None",doc:"The visual label for the tab",default:"None"},{name:"visible",annotation:"bool",doc:"If False, Tab will be hidden.",default:"True"},{name:"interactive",annotation:"bool",doc:"If False, Tab will not be clickable.",default:"True"},{name:"id",annotation:"int | str | None",doc:"An optional identifier for the tab, required if you wish to control the selected tab from a predict function.",default:"None"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of the &lt;div&gt; containing the contents of the Tab layout. The same string followed by &quot;-button&quot; is attached to the Tab button. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional string or list of strings that are assigned as the class of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, this layout will not be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"}],returns:{annotation:null},example:"with gr.Blocks() as demo:\n    with gr.Tab(\"Lion\"):\n        gr.Image(\"lion.jpg\")\n        gr.Button(\"New Lion\")\n    with gr.Tab(\"Tiger\"):\n        gr.Image(\"tiger.jpg\")\n        gr.Button(\"New Tiger\")",fns:[{fn:null,name:"select",description:"Event listener for when the user selects or deselects the Tab. Uses event data gradio.SelectData to carry `value` referring to the label of the Tab, and `selected` to refer to state of the Tab. See EventData documentation on how to use this event data",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Tab"}],guides:[{name:"controlling-layout",category:"building-with-blocks",pretty_category:"Building With Blocks",guide_index:2,absolute_index:16,pretty_name:"Controlling Layout",content:"# Controlling Layout\n\nBy default, Components in Blocks are arranged vertically. Let's take a look at how we can rearrange Components. Under the hood, this layout structure uses the [flexbox model of web development](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox).\n\n## Rows\n\nElements within a `with gr.Row` clause will all be displayed horizontally. For example, to display two Buttons side by side:\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        btn1 = gr.Button(\"Button 1\")\n        btn2 = gr.Button(\"Button 2\")\n```\n\nTo make every element in a Row have the same height, use the `equal_height` argument of the `style` method.\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row(equal_height=True):\n        textbox = gr.Textbox()\n        btn2 = gr.Button(\"Button 2\")\n```\n\nThe widths of elements in a Row can be controlled via a combination of `scale` and `min_width` arguments that are present in every Component.\n\n- `scale` is an integer that defines how an element will take up space in a Row. If scale is set to `0`, the element will not expand to take up space. If scale is set to `1` or greater, the element will expand. Multiple elements in a row will expand proportional to their scale. Below, `btn2` will expand twice as much as `btn1`, while `btn0` will not expand at all:\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        btn0 = gr.Button(\"Button 0\", scale=0)\n        btn1 = gr.Button(\"Button 1\", scale=1)\n        btn2 = gr.Button(\"Button 2\", scale=2)\n```\n\n- `min_width` will set the minimum width the element will take. The Row will wrap if there isn't sufficient space to satisfy all `min_width` values.\n\nLearn more about Rows in the [docs](https://gradio.app/docs/row).\n\n## Columns and Nesting\n\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        text1 = gr.Textbox(label=\"t1\")\n        slider2 = gr.Textbox(label=\"s2\")\n        drop3 = gr.Dropdown([\"a\", \"b\", \"c\"], label=\"d3\")\n    with gr.Row():\n        with gr.Column(scale=1, min_width=600):\n            text1 = gr.Textbox(label=\"prompt 1\")\n            text2 = gr.Textbox(label=\"prompt 2\")\n            inbtw = gr.Button(\"Between\")\n            text4 = gr.Textbox(label=\"prompt 1\")\n            text5 = gr.Textbox(label=\"prompt 2\")\n        with gr.Column(scale=2, min_width=600):\n            img1 = gr.Image(\"images/cheetah.jpg\")\n            btn = gr.Button(\"Go\")\n\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/rows_and_columns'>\u003C/gradio-app>\n\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width.\n\nLearn more about Columns in the [docs](https://gradio.app/docs/column).\n\n# Dimensions\n\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specific design requirements. When omitted, Gradio uses default dimensions suited for most use cases.\n\nBelow is an example illustrating the use of viewport width (vw):\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    im = gr.ImageEditor(\n        width=\"50vw\",\n    )\n\ndemo.launch()\n```\n\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\n\n\n```python\nimport gradio as gr\n\ncss = \"\"\"\n.container {\n    height: 100vh;\n}\n\"\"\"\n\nwith gr.Blocks(css=css) as demo:\n    with gr.Column(elem_classes=[\"container\"]):\n        name = gr.Chatbot(value=[[\"1\", \"2\"]], height=\"70%\")\n\ndemo.launch()\n```\n\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height.\n\nYou can apply any valid CSS unit for these parameters. For a comprehensive list of CSS units, refer to [this guide](https://www.w3schools.com/cssref/css_units.php). We recommend you always consider responsiveness and test your interfaces on various screen sizes to ensure a consistent user experience.\n\n\n\n## Tabs and Accordions\n\nYou can also create Tabs using the `with gr.Tab('tab_name'):` clause. Any component created inside of a `with gr.Tab('tab_name'):` context appears in that tab. Consecutive Tab clauses are grouped together so that a single tab can be selected at one time, and only the components within that Tab's context are shown.\n\nFor example:\n\n```python\nimport numpy as np\nimport gradio as gr\n\n\ndef flip_text(x):\n    return x[::-1]\n\n\ndef flip_image(x):\n    return np.fliplr(x)\n\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Flip text or image files using this demo.\")\n    with gr.Tab(\"Flip Text\"):\n        text_input = gr.Textbox()\n        text_output = gr.Textbox()\n        text_button = gr.Button(\"Flip\")\n    with gr.Tab(\"Flip Image\"):\n        with gr.Row():\n            image_input = gr.Image()\n            image_output = gr.Image()\n        image_button = gr.Button(\"Flip\")\n\n    with gr.Accordion(\"Open for More!\", open=False):\n        gr.Markdown(\"Look at me...\")\n        temp_slider = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=0.1,\n            step=0.1,\n            interactive=True,\n            label=\"Slide me\",\n        )\n        temp_slider.change(lambda x: x, [temp_slider])\n\n    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/blocks_flipper'>\u003C/gradio-app>\n\nAlso note the `gr.Accordion('label')` in this example. The Accordion is a layout that can be toggled open or closed. Like `Tabs`, it is a layout element that can selectively hide or show content. Any components that are defined inside of a `with gr.Accordion('label'):` will be hidden or shown when the accordion's toggle icon is clicked.\n\nLearn more about [Tabs](https://gradio.app/docs/tab) and [Accordions](https://gradio.app/docs/accordion) in the docs.\n\n## Visibility\n\nBoth Components and Layout elements have a `visible` argument that can set initially and also updated. Setting `gr.Column(visible=...)` on a Column can be used to show or hide a set of Components.\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    error_box = gr.Textbox(label=\"Error\", visible=False)\n\n    name_box = gr.Textbox(label=\"Name\")\n    age_box = gr.Number(label=\"Age\", minimum=0, maximum=100)\n    symptoms_box = gr.CheckboxGroup([\"Cough\", \"Fever\", \"Runny Nose\"])\n    submit_btn = gr.Button(\"Submit\")\n\n    with gr.Column(visible=False) as output_col:\n        diagnosis_box = gr.Textbox(label=\"Diagnosis\")\n        patient_summary_box = gr.Textbox(label=\"Patient Summary\")\n\n    def submit(name, age, symptoms):\n        if len(name) == 0:\n            return {error_box: gr.Textbox(value=\"Enter name\", visible=True)}\n        return {\n            output_col: gr.Column(visible=True),\n            diagnosis_box: \"covid\" if \"Cough\" in symptoms else \"flu\",\n            patient_summary_box: f\"{name}, {age} y/o\",\n        }\n\n    submit_btn.click(\n        submit,\n        [name_box, age_box, symptoms_box],\n        [error_box, diagnosis_box, patient_summary_box, output_col],\n    )\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/blocks_form'>\u003C/gradio-app>\n\n## Variable Number of Outputs\n\nBy adjusting the visibility of components in a dynamic way, it is possible to create\ndemos with Gradio that support a _variable numbers of outputs_. Here's a very simple example\nwhere the number of output textboxes is controlled by an input slider:\n\n```python\nimport gradio as gr\n\nmax_textboxes = 10\n\ndef variable_outputs(k):\n    k = int(k)\n    return [gr.Textbox(visible=True)]*k + [gr.Textbox(visible=False)]*(max_textboxes-k)\n\nwith gr.Blocks() as demo:\n    s = gr.Slider(1, max_textboxes, value=max_textboxes, step=1, label=\"How many textboxes to show:\")\n    textboxes = []\n    for i in range(max_textboxes):\n        t = gr.Textbox(f\"Textbox {i}\")\n        textboxes.append(t)\n\n    s.change(variable_outputs, s, textboxes)\n\nif __name__ == \"__main__\":\n   demo.launch()\n\n```\n\u003Cgradio-app space='gradio/variable_outputs'>\u003C/gradio-app>\n\n## Defining and Rendering Components Separately\n\nIn some cases, you might want to define components before you actually render them in your UI. For instance, you might want to show an examples section using `gr.Examples` above the corresponding `gr.Textbox` input. Since `gr.Examples` requires as a parameter the input component object, you will need to first define the input component, but then render it later, after you have defined the `gr.Examples` object.\n\nThe solution to this is to define the `gr.Textbox` outside of the `gr.Blocks()` scope and use the component's `.render()` method wherever you'd like it placed in the UI.\n\nHere's a full code example:\n\n```python\ninput_textbox = gr.Textbox()\n\nwith gr.Blocks() as demo:\n    gr.Examples([\"hello\", \"bonjour\", \"merhaba\"], input_textbox)\n    input_textbox.render()\n```\n",tags:[],spaces:[],url:"/guides/controlling-layout/",contributor:null}],parent:"gradio"},interface:{class:null,name:"Interface",description:"Interface is Gradio's main high-level class, and allows you to create a web-based GUI / demo around a machine learning model (or any Python function) in a few lines of code. You must specify three parameters: (1) the function to create a GUI for (2) the desired input components and (3) the desired output components. Additional parameters can be used to control the appearance and behavior of the demo. \u003Cbr>",tags:{demos:"hello_world, hello_world_2, hello_world_3",guides:"quickstart, key-features, sharing-your-app, interface-state, reactive-interfaces, advanced-interface-features, setting-up-a-gradio-demo-for-maximum-performance"},parameters:[{name:"fn",annotation:"Callable",doc:"The function to wrap an interface around. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component."},{name:"inputs",annotation:"str | Component | list[str | Component] | None",doc:"A single Gradio component, or list of Gradio components. Components can either be passed as instantiated objects, or referred to by their string shortcuts. The number of input components should match the number of parameters in fn. If set to None, then only the output components will be displayed."},{name:"outputs",annotation:"str | Component | list[str | Component] | None",doc:"A single Gradio component, or list of Gradio components. Components can either be passed as instantiated objects, or referred to by their string shortcuts. The number of output components should match the number of values returned by fn. If set to None, then only the input components will be displayed."},{name:"examples",annotation:"list[Any] | list[list[Any]] | str | None",doc:"Sample inputs for the function; if provided, appear below the UI components and can be clicked to populate the interface. Should be nested list, in which the outer list consists of samples and each inner list consists of an input corresponding to each input component. A string path to a directory of examples can also be provided, but it should be within the directory with the python file running the gradio app. If there are multiple input components and a directory is provided, a log.csv file must be present in the directory to link corresponding inputs.",default:"None"},{name:"cache_examples",annotation:"bool | Literal['lazy'] | None",doc:"If True, caches examples in the server for fast runtime in examples. If &quot;lazy&quot;, then examples are cached after their first use. If `fn` is a generator function, then the last yielded value will be used as the output. Can also be set by the GRADIO_CACHE_EXAMPLES environment variable, which takes a case-insensitive value, one of: {&quot;true&quot;, &quot;false&quot;, &quot;lazy&quot;}. The default option in HuggingFace Spaces is True. The default option elsewhere is False.",default:"None"},{name:"examples_per_page",annotation:"int",doc:"If examples are provided, how many to display per page.",default:"10"},{name:"live",annotation:"bool",doc:"Whether the interface should automatically rerun if any of the inputs change.",default:"False"},{name:"title",annotation:"str | None",doc:"A title for the interface; if provided, appears above the input and output components in large font. Also used as the tab title when opened in a browser window.",default:"None"},{name:"description",annotation:"str | None",doc:"A description for the interface; if provided, appears above the input and output components and beneath the title in regular font. Accepts Markdown and HTML content.",default:"None"},{name:"article",annotation:"str | None",doc:"An expanded article explaining the interface; if provided, appears below the input and output components in regular font. Accepts Markdown and HTML content. If it is an HTTP(S) link to a downloadable remote file, the content of this file is displayed.",default:"None"},{name:"thumbnail",annotation:"str | None",doc:"This parameter has been deprecated and has no effect.",default:"None"},{name:"theme",annotation:"Theme | str | None",doc:"A Theme object or a string representing a theme. If a string, will look for a built-in theme with that name (e.g. &quot;soft&quot; or &quot;default&quot;), or will attempt to load a theme from the Hugging Face Hub (e.g. &quot;gradio/monochrome&quot;). If None, will use the Default theme.",default:"None"},{name:"css",annotation:"str | None",doc:"Custom css as a string or path to a css file. This css will be included in the demo webpage.",default:"None"},{name:"allow_flagging",annotation:"Literal['never'] | Literal['auto'] | Literal['manual'] | None",doc:"One of &quot;never&quot;, &quot;auto&quot;, or &quot;manual&quot;. If &quot;never&quot; or &quot;auto&quot;, users will not see a button to flag an input and output. If &quot;manual&quot;, users will see a button to flag. If &quot;auto&quot;, every input the user submits will be automatically flagged, along with the generated output. If &quot;manual&quot;, both the input and outputs are flagged when the user clicks flag button. This parameter can be set with environmental variable GRADIO_ALLOW_FLAGGING; otherwise defaults to &quot;manual&quot;.",default:"None"},{name:"flagging_options",annotation:"list[str] | list[tuple[str, str]] | None",doc:"If provided, allows user to select from the list of options when flagging. Only applies if allow_flagging is &quot;manual&quot;. Can either be a list of tuples of the form (label, value), where label is the string that will be displayed on the button and value is the string that will be stored in the flagging CSV; or it can be a list of strings [&quot;X&quot;, &quot;Y&quot;], in which case the values will be the list of strings and the labels will [&quot;Flag as X&quot;, &quot;Flag as Y&quot;], etc.",default:"None"},{name:"flagging_dir",annotation:"str",doc:"What to name the directory where flagged data is stored.",default:"\"flagged\""},{name:"flagging_callback",annotation:"FlaggingCallback | None",doc:"None or an instance of a subclass of FlaggingCallback which will be called when a sample is flagged. If set to None, an instance of gradio.flagging.CSVLogger will be created and logs will be saved to a local CSV file in flagging_dir. Default to None.",default:"None"},{name:"analytics_enabled",annotation:"bool | None",doc:"Whether to allow basic telemetry. If None, will use GRADIO_ANALYTICS_ENABLED environment variable if defined, or default to True.",default:"None"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"api_name",annotation:"str | Literal[False] | None",doc:"Defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None, the name of the prediction function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"\"predict\""},{name:"allow_duplication",annotation:"bool",doc:"If True, then will show a &#x27;Duplicate Spaces&#x27; button on Hugging Face Spaces.",default:"False"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"js",annotation:"str | None",doc:"Custom js as a string or path to a js file. The custom js should be in the form of a single js function. This function will automatically be executed when the page loads. For more flexibility, use the head parameter to insert js inside &lt;script&gt; tags.",default:"None"},{name:"head",annotation:"str | None",doc:"Custom html to insert into the head of the demo webpage. This can be used to add custom meta tags, scripts, stylesheets, etc. to the page.",default:"None"},{name:"additional_inputs",annotation:"str | Component | list[str | Component] | None",doc:"A single Gradio component, or list of Gradio components. Components can either be passed as instantiated objects, or referred to by their string shortcuts. These components will be rendered in an accordion below the main input components. By default, no additional input components will be displayed.",default:"None"},{name:"additional_inputs_accordion",annotation:"str | Accordion | None",doc:"If a string is provided, this is the label of the `gr.Accordion` to use to contain additional inputs. A `gr.Accordion` object can be provided as well to configure other properties of the container holding the additional inputs. Defaults to a `gr.Accordion(label=&quot;Additional Inputs&quot;, open=False)`. This parameter is only used if `additional_inputs` is provided.",default:"None"},{name:"submit_btn",annotation:"str | Button",doc:"The button to use for submitting inputs. Defaults to a `gr.Button(&quot;Submit&quot;, variant=&quot;primary&quot;)`. This parameter does not apply if the Interface is output-only, in which case the submit button always displays &quot;Generate&quot;. Can be set to a string (which becomes the button label) or a `gr.Button` object (which allows for more customization).",default:"\"Submit\""},{name:"stop_btn",annotation:"str | Button",doc:"The button to use for stopping the interface. Defaults to a `gr.Button(&quot;Stop&quot;, variant=&quot;stop&quot;, visible=False)`. Can be set to a string (which becomes the button label) or a `gr.Button` object (which allows for more customization).",default:"\"Stop\""},{name:"clear_btn",annotation:"str | Button | None",doc:"The button to use for clearing the inputs. Defaults to a `gr.Button(&quot;Clear&quot;, variant=&quot;secondary&quot;)`. Can be set to a string (which becomes the button label) or a `gr.Button` object (which allows for more customization). Can be set to None, which hides the button.",default:"\"Clear\""},{name:"delete_cache",annotation:"tuple[int, int] | None",doc:"A tuple corresponding [frequency, age] both expressed in number of seconds. Every `frequency` seconds, the temporary files created by this Blocks instance will be deleted if more than `age` seconds have passed since the file was created. For example, setting this to (86400, 86400) will delete temporary files every day. The cache will be deleted entirely when the server restarts. If None, no cache deletion will occur.",default:"None"}],returns:{annotation:null},example:"import gradio as gr\n\ndef image_classifier(inp):\n    return {'cat': 0.3, 'dog': 0.7}\n\ndemo = gr.Interface(fn=image_classifier, inputs=\"image\", outputs=\"label\")\ndemo.launch()",fns:[{fn:null,name:"launch",description:"Launches a simple web server that serves the demo. Can also be used to create a public link used by anyone to access the demo from their browser by setting share=True. &lt;br&gt;",tags:{},parameters:[{name:"inline",annotation:"bool | None",doc:"whether to display in the gradio app inline in an iframe. Defaults to True in python notebooks; False otherwise.",default:"None"},{name:"inbrowser",annotation:"bool",doc:"whether to automatically launch the gradio app in a new tab on the default browser.",default:"False"},{name:"share",annotation:"bool | None",doc:"whether to create a publicly shareable link for the gradio app. Creates an SSH tunnel to make your UI accessible from anywhere. If not provided, it is set to False by default every time, except when running in Google Colab. When localhost is not accessible (e.g. Google Colab), setting share=False is not supported. Can be set by environment variable GRADIO_SHARE=True.",default:"None"},{name:"debug",annotation:"bool",doc:"if True, blocks the main thread from running. If running in Google Colab, this is needed to print the errors in the cell output.",default:"False"},{name:"max_threads",annotation:"int",doc:"the maximum number of total threads that the Gradio app can generate in parallel. The default is inherited from the starlette library (currently 40).",default:"40"},{name:"auth",annotation:"Callable | tuple[str, str] | list[tuple[str, str]] | None",doc:"If provided, username and password (or list of username-password tuples) required to access app. Can also provide function that takes username and password and returns True if valid login.",default:"None"},{name:"auth_message",annotation:"str | None",doc:"If provided, HTML message provided on login page.",default:"None"},{name:"prevent_thread_lock",annotation:"bool",doc:"By default, the gradio app blocks the main thread while the server is running. If set to True, the gradio app will not block and the gradio server will terminate as soon as the script finishes.",default:"False"},{name:"show_error",annotation:"bool",doc:"If True, any errors in the gradio app will be displayed in an alert modal and printed in the browser console log",default:"False"},{name:"server_name",annotation:"str | None",doc:"to make app accessible on local network, set this to &quot;0.0.0.0&quot;. Can be set by environment variable GRADIO_SERVER_NAME. If None, will use &quot;127.0.0.1&quot;.",default:"None"},{name:"server_port",annotation:"int | None",doc:"will start gradio app on this port (if available). Can be set by environment variable GRADIO_SERVER_PORT. If None, will search for an available port starting at 7860.",default:"None"},{name:"height",annotation:"int",doc:"The height in pixels of the iframe element containing the gradio app (used if inline=True)",default:"500"},{name:"width",annotation:"int | str",doc:"The width in pixels of the iframe element containing the gradio app (used if inline=True)",default:"\"100%\""},{name:"favicon_path",annotation:"str | None",doc:"If a path to a file (.png, .gif, or .ico) is provided, it will be used as the favicon for the web page.",default:"None"},{name:"ssl_keyfile",annotation:"str | None",doc:"If a path to a file is provided, will use this as the private key file to create a local server running on https.",default:"None"},{name:"ssl_certfile",annotation:"str | None",doc:"If a path to a file is provided, will use this as the signed certificate for https. Needs to be provided if ssl_keyfile is provided.",default:"None"},{name:"ssl_keyfile_password",annotation:"str | None",doc:"If a password is provided, will use this with the ssl certificate for https.",default:"None"},{name:"ssl_verify",annotation:"bool",doc:"If False, skips certificate validation which allows self-signed certificates to be used.",default:"True"},{name:"quiet",annotation:"bool",doc:"If True, suppresses most print statements.",default:"False"},{name:"show_api",annotation:"bool",doc:"If True, shows the api docs in the footer of the app. Default True.",default:"True"},{name:"allowed_paths",annotation:"list[str] | None",doc:"List of complete filepaths or parent directories that gradio is allowed to serve. Must be absolute paths. Warning: if you provide directories, any files in these directories or their subdirectories are accessible to all users of your app.",default:"None"},{name:"blocked_paths",annotation:"list[str] | None",doc:"List of complete filepaths or parent directories that gradio is not allowed to serve (i.e. users of your app are not allowed to access). Must be absolute paths. Warning: takes precedence over `allowed_paths` and all other directories exposed by Gradio by default.",default:"None"},{name:"root_path",annotation:"str | None",doc:"The root path (or &quot;mount point&quot;) of the application, if it&#x27;s not served from the root (&quot;/&quot;) of the domain. Often used when the application is behind a reverse proxy that forwards requests to the application. For example, if the application is served at &quot;https://example.com/myapp&quot;, the `root_path` should be set to &quot;/myapp&quot;. A full URL beginning with http:// or https:// can be provided, which will be used as the root path in its entirety. Can be set by environment variable GRADIO_ROOT_PATH. Defaults to &quot;&quot;.",default:"None"},{name:"app_kwargs",annotation:"dict[str, Any] | None",doc:"Additional keyword arguments to pass to the underlying FastAPI app as a dictionary of parameter keys and argument values. For example, `{&quot;docs_url&quot;: &quot;/docs&quot;}`",default:"None"},{name:"state_session_capacity",annotation:"int",doc:"The maximum number of sessions whose information to store in memory. If the number of sessions exceeds this number, the oldest sessions will be removed. Reduce capacity to reduce memory usage when using gradio.State or returning updated components from functions. Defaults to 10000.",default:"10000"},{name:"share_server_address",annotation:"str | None",doc:"Use this to specify a custom FRP server and port for sharing Gradio apps (only applies if share=True). If not provided, will use the default FRP server at https://gradio.live. See https://github.com/huggingface/frp for more information.",default:"None"},{name:"share_server_protocol",annotation:"Literal[('http', 'https')] | None",doc:"Use this to specify the protocol to use for the share links. Defaults to &quot;https&quot;, unless a custom share_server_address is provided, in which case it defaults to &quot;http&quot;. If you are using a custom share_server_address and want to use https, you must set this to &quot;https&quot;.",default:"None"},{name:"auth_dependency",annotation:"Callable[[fastapi.Request], str | None] | None",doc:"A function that takes a FastAPI request and returns a string user ID or None. If the function returns None for a specific request, that user is not authorized to access the app (they will see a 401 Unauthorized response). To be used with external authentication systems like OAuth. Cannot be used with `auth`.",default:"None"},{name:"max_file_size",annotation:"str | int | None",doc:"The maximum file size in bytes that can be uploaded. Can be a string of the form &quot;&lt;value&gt;&lt;unit&gt;&quot;, where value is any positive integer and unit is one of &quot;b&quot;, &quot;kb&quot;, &quot;mb&quot;, &quot;gb&quot;, &quot;tb&quot;. If None, no limit is set.",default:"None"},{name:"enable_monitoring",annotation:"bool",doc:null,default:"False"}],returns:{},example:"import gradio as gr\ndef reverse(text):\n    return text[::-1]\ndemo = gr.Interface(reverse, \"text\", \"text\")\ndemo.launch(share=True, auth=(\"username\", \"password\"))",override_signature:null,parent:"gradio.Interface"},{fn:null,name:"load",description:"This listener is triggered when the Interface initially loads in the browser.",tags:{},parameters:[{name:"block",annotation:"Block | None",doc:null},{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Interface"},{fn:null,name:"from_pipeline",description:"Class method that constructs an Interface from a Hugging Face transformers.Pipeline or diffusers.DiffusionPipeline object. The input and output components are automatically determined from the pipeline.",tags:{},parameters:[{name:"pipeline",annotation:"Pipeline | DiffusionPipeline",doc:"the pipeline object to use."}],returns:{annotation:"Interface",doc:"a Gradio Interface object from the given Pipeline"},example:"import gradio as gr\nfrom transformers import pipeline\npipe = pipeline(\"image-classification\")\ngr.Interface.from_pipeline(pipe).launch()",override_signature:null,parent:"gradio.Interface"},{fn:null,name:"integrate",description:"A catch-all method for integrating with other libraries. This method should be run after launch()",tags:{},parameters:[{name:"comet_ml",annotation:"\u003Cclass 'inspect._empty'>",doc:"If a comet_ml Experiment object is provided, will integrate with the experiment and appear on Comet dashboard",default:"None"},{name:"wandb",annotation:"ModuleType | None",doc:"If the wandb module is provided, will integrate with it and appear on WandB dashboard",default:"None"},{name:"mlflow",annotation:"ModuleType | None",doc:"If the mlflow module  is provided, will integrate with the experiment and appear on ML Flow dashboard",default:"None"}],returns:{},example:null,override_signature:null,parent:"gradio.Interface"},{fn:null,name:"queue",description:"By enabling the queue you can control when users know their position in the queue, and set a limit on maximum number of events allowed.",tags:{},parameters:[{name:"status_update_rate",annotation:"float | Literal['auto']",doc:"If &quot;auto&quot;, Queue will send status estimations to all clients whenever a job is finished. Otherwise Queue will send status at regular intervals set by this parameter as the number of seconds.",default:"\"auto\""},{name:"api_open",annotation:"bool | None",doc:"If True, the REST routes of the backend will be open, allowing requests made directly to those endpoints to skip the queue.",default:"None"},{name:"max_size",annotation:"int | None",doc:"The maximum number of events the queue will store at any given moment. If the queue is full, new events will not be added and a user will receive a message saying that the queue is full. If None, the queue size will be unlimited.",default:"None"},{name:"concurrency_count",annotation:"int | None",doc:"Deprecated. Set the concurrency_limit directly on event listeners e.g. btn.click(fn, ..., concurrency_limit=10) or gr.Interface(concurrency_limit=10). If necessary, the total number of workers can be configured via `max_threads` in launch().",default:"None"},{name:"default_concurrency_limit",annotation:"int | None | Literal['not_set']",doc:"The default value of `concurrency_limit` to use for event listeners that don&#x27;t specify a value. Can be set by environment variable GRADIO_DEFAULT_CONCURRENCY_LIMIT. Defaults to 1 if not set otherwise.",default:"\"not_set\""}],returns:{},example:"demo = gr.Interface(image_generator, gr.Textbox(), gr.Image())\ndemo.queue(max_size=20)\ndemo.launch()",override_signature:null,parent:"gradio.Interface"}],demos:[["hello_world","import gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(fn=greet, inputs=\"textbox\", outputs=\"textbox\")\n    \nif __name__ == \"__main__\":\n    demo.launch()   "],["hello_world_2","import gradio as gr\n\ndef greet(name, intensity):\n    return \"Hello, \" + name + \"!\" * intensity\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=[\"text\", gr.Slider(value=2, minimum=1, maximum=10, step=1)],\n    outputs=[gr.Textbox(label=\"greeting\", lines=3)],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"],["hello_world_3","import gradio as gr\n\ndef greet(name, is_morning, temperature):\n    salutation = \"Good morning\" if is_morning else \"Good evening\"\n    greeting = f\"{salutation} {name}. It is {temperature} degrees today\"\n    celsius = (temperature - 32) * 5 / 9\n    return greeting, round(celsius, 2)\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=[\"text\", \"checkbox\", gr.Slider(0, 100)],\n    outputs=[\"text\", \"number\"],\n)\nif __name__ == \"__main__\":\n    demo.launch()\n"]],guides:[{name:"quickstart",category:"getting-started",pretty_category:"Getting Started",guide_index:1,absolute_index:0,pretty_name:"Quickstart",content:"# Quickstart\n\nGradio is an open-source Python package that allows you to quickly **build** a demo or web application for your machine learning model, API, or any arbitary Python function. You can then **share** a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. *No JavaScript, CSS, or web hosting experience needed!*\n\n\u003Cimg src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/lcm-screenshot-3.gif\" style=\"padding-bottom: 10px\">\n\nIt just takes a few lines of Python to create a demo like the one above, so let's get started üí´\n\n## Installation\n\n**Prerequisite**: Gradio requires [Python 3.8 or higher](https://www.python.org/downloads/)\n\n\nWe recommend installing Gradio using `pip`, which is included by default in Python. Run this in your terminal or command prompt:\n\n```bash\npip install gradio\n```\n\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> it is best to install Gradio in a virtual environment. Detailed installation instructions for all common operating systems \u003Ca href=\"https://www.gradio.app/main/guides/installing-gradio-in-a-virtual-environment\">are provided here\u003C/a>. \u003C/p>\n\n## Building Your First Demo\n\nYou can run Gradio in your favorite code editor, Jupyter notebook, Google Colab, or anywhere else you write Python. Let's write your first Gradio app:\n\n\n```python\nimport gradio as gr\n\ndef greet(name, intensity):\n    return \"Hello, \" + name + \"!\" * int(intensity)\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=[\"text\", \"slider\"],\n    outputs=[\"text\"],\n)\n\ndemo.launch()\n\n```\n\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> We shorten the imported name from \u003Ccode>gradio\u003C/code> to \u003Ccode>gr\u003C/code> for better readability of code. This is a widely adopted convention that you should follow so that anyone working with your code can easily understand it. \u003C/p>\n\nNow, run your code. If you've written the Python code in a file named, for example, `app.py`, then you would run `python app.py` from the terminal.\n\nThe demo below will open in a browser on [http://localhost:7860](http://localhost:7860) if running from a file. If you are running within a notebook, the demo will appear embedded within the notebook.\n\n\u003Cgradio-app space='gradio/hello_world_4'>\u003C/gradio-app>\n\nType your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> When developing locally, you can run your Gradio app in \u003Cstrong>hot reload mode\u003C/strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in \u003Ccode>gradio\u003C/code> before the name of the file instead of \u003Ccode>python\u003C/code>. In the example above, you would type: `gradio app.py` in your terminal. Learn more about hot reloading in the \u003Ca href=\"https://www.gradio.app/guides/developing-faster-with-reload-mode\">Hot Reloading Guide\u003C/a>.\u003C/p>\n\n\n**Understanding the `Interface` Class**\n\nYou'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs. \n\nThe `Interface` class has three core arguments:\n\n- `fn`: the function to wrap a user interface (UI) around\n- `inputs`: the Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.\n- `outputs`: the Gradio component(s) to use for the output. The number of components should match the number of return values from your function.\n\nThe `fn` argument is very flexible -- you can pass *any* Python function that you want to wrap with a UI. In the example above, we saw a relatively simple function, but the function could be anything from a music generator to a tax calculator to the prediction function of a pretrained machine learning model.\n\nThe `inputs` and `outputs` arguments take one or more Gradio components. As we'll see, Gradio includes more than [30 built-in components](https://www.gradio.app/docs/gradio/components) (such as the `gr.Textbox()`, `gr.Image()`, and `gr.HTML()` components) that are designed for machine learning applications. \u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> For the `inputs` and `outputs` arguments, you can pass in the name of these components as a string (`\"textbox\"`) or an instance of the class (`gr.Textbox()`).\u003C/p>\n\nIf your function accepts more than one argument, as is the case above, pass a list of input components to `inputs`, with each input component corresponding to one of the arguments of the function, in order. The same holds true if your function returns more than one value: simply pass in a list of components to `outputs`. This flexibility makes the `Interface` class a very powerful way to create demos.\n\nWe'll dive deeper into the `gr.Interface` on our series on [building Interfaces](https://www.gradio.app/main/guides/the-interface-class).\n\n## Sharing Your Demo\n\nWhat good is a beautiful demo if you can't share it? Gradio lets you easily share a machine learning demo without having to worry about the hassle of hosting on a web server. Simply set `share=True` in `launch()`, and a publicly accessible URL will be created for your demo. Let's revisit our example demo,  but change the last line as follows:\n\n```python\nimport gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(fn=greet, inputs=\"textbox\", outputs=\"textbox\")\n    \ndemo.launch(share=True)  # Share your demo with just 1 extra parameter üöÄ\n```\n\nWhen you run this code, a public URL will be generated for your demo in a matter of seconds, something like:\n\nüëâ &nbsp; `https://a23dsf231adb.gradio.live`\n\nNow, anyone around the world can try your Gradio demo from their browser, while the machine learning model and all computation continues to run locally on your computer.\n\nTo learn more about sharing your demo, read our dedicated guide on [sharing your Gradio application](https://www.gradio.app/guides/sharing-your-app).\n\n\n## Core Gradio Classes\n\nSo far, we've been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio include?aaa\n\n### Chatbots with `gr.ChatInterface`\n\nGradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight to [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast).\n\n### Custom Demos with `gr.Blocks`\n\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction ‚Äî still all in Python. \n\nYou can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\n\n### The Gradio Python & JavaScript Ecosystem\n\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\n* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio_client`): query any Gradio app programmatically in Python.\n* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript.\n* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \n* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications ‚Äî for free!\n\n## What's Next?\n\nKeep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [let's dive deeper into the Interface class](https://www.gradio.app/guides/the-interface-class).\n\nOr, if you already know the basics and are looking for something specific, you can search the more [technical API documentation](https://www.gradio.app/docs/).\n\n\n",tags:[],spaces:[],url:"/guides/quickstart/",contributor:null},{name:"key-features",category:"getting-started",pretty_category:"Getting Started",guide_index:2,absolute_index:1,pretty_name:"Key Features",content:"# Key Features\n\nLet's go through some of the key features of Gradio. This guide is intended to be a high-level overview of various things that you should be aware of as you build your demo. Where appropriate, we link to more detailed guides on specific topics.\n\n1. [Components](#components)\n2. [Queuing](#queuing)\n3. [Streaming outputs](#streaming-outputs)\n4. [Streaming inputs](#streaming-inputs)\n5. [Alert modals](#alert-modals)\n6. [Styling](#styling)\n7. [Progress bars](#progress-bars)\n8. [Batch functions](#batch-functions)\n\n## Components\n\nGradio includes more than 30 pre-built components (as well as many user-built _custom components_) that can be used as inputs or outputs in your demo with a single line of code. These components correspond to common data types in machine learning and data science, e.g. the `gr.Image` component is designed to handle input or output images, the `gr.Label` component displays classification labels and probabilities, the `gr.Plot` component displays various kinds of plots, and so on.\n\nEach component includes various constructor attributes that control the properties of the component. For example, you can control the number of lines in a `gr.Textbox` using the `lines` argument (which takes a positive integer) in its constructor. Or you can control the way that a user can provide an image in the `gr.Image` component using the `sources` parameter (which takes a list like `[\"webcam\", \"upload\"]`).\n\n**Static and Interactive Components**\n\nEvery component has a _static_ version that is designed to *display* data, and most components also have an _interactive_ version designed to let users input or modify the data. Typically, you don't need to think about this distinction, because when you build a Gradio demo, Gradio automatically figures out whether the component should be static or interactive based on whether it is being used as an input or output. However, you can set this manually using the `interactive` argument that every component supports.\n\n**Preprocessing and Postprocessing**\n\nWhen a component is used as an input, Gradio automatically handles the _preprocessing_ needed to convert the data from a type sent by the user's browser (such as an uploaded image) to a form that can be accepted by your function (such as a `numpy` array).\n\n\nSimilarly, when a component is used as an output, Gradio automatically handles the _postprocessing_ needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user's browser (a gallery of images).\n\nConsider an example demo with three input components (`gr.Textbox`, `gr.Number`, and `gr.Image`) and two outputs (`gr.Number` and `gr.Gallery`) that serve as a UI for your image-to-image generation model. Below is a diagram of what our preprocessing will send to the model and what our postprocessing will require from it.\n\n![](https://github.com/gradio-app/gradio/blob/main/guides/assets/dataflow.svg?raw=true)\n\nIn this image, the following preprocessing steps happen to send the data from the browser to your function:\n\n* The text in the textbox is converted to a Python `str` (essentially no preprocessing)\n* The number in the number input in converted to a Python `float` (essentially no preprocessing)\n* Most importantly, ihe image supplied by the user is converted to a `numpy.array` representation of the RGB values in the image\n\nImages are converted to NumPy arrays because they are a common format for machine learning workflows. You can control the _preprocessing_ using the component's parameters when constructing the component. For example, if you instantiate the `Image` component with the following parameters, it will preprocess the image to the `PIL` format instead:\n\n```py\nimg = gr.Image(type=\"pil\")\n```\n\nPostprocessing is even simpler! Gradio automatically recognizes the format of the returned data (e.g. does the user's function return a `numpy` array or a `str` filepath for the `gr.Image` component?) and postprocesses it appropriately into a format that can be displayed by the browser.\n\nSo in the image above, the following postprocessing steps happen to send the data returned from a user's function to the browser:\n\n* The `float` is displayed as a number and displayed directly to the user\n* The list of string filepaths (`list[str]`) is interpreted as a list of image filepaths and displayed as a gallery in the browser\n\nTake a look at the [Docs](https://gradio.app/docs) to see all the parameters for each Gradio component.\n\n## Queuing\n\nEvery Gradio app comes with a built-in queuing system that can scale to thousands of concurrent users. You can configure the queue by using `queue()` method which is supported by the `gr.Interface`, `gr.Blocks`, and `gr.ChatInterface` classes. \n\nFor example, you can control the number of requests processed at a single time by setting the `default_concurrency_limit` parameter of `queue()`, e.g.\n\n```python\ndemo = gr.Interface(...).queue(default_concurrency_limit=5)\ndemo.launch()\n```\n\nThis limits the number of requests processed for this event listener at a single time to 5. By default, the `default_concurrency_limit` is actually set to `1`, which means that when many users are using your app, only a single user's request will be processed at a time. This is because many machine learning functions consume a significant amount of memory and so it is only suitable to have a single user using the demo at a time. However, you can change this parameter in your demo easily.\n\nSee the [docs on queueing](https://gradio.app/docs/gradio/interface#interface-queue) for more details on configuring the queuing parameters.\n\n## Streaming outputs\n\nIn some cases, you may want to stream a sequence of outputs rather than show a single output at once. For example, you might have an image generation model and you want to show the image that is generated at each step, leading up to the final image. Or you might have a chatbot which streams its response one token at a time instead of returning it all at once.\n\nIn such cases, you can supply a **generator** function into Gradio instead of a regular function. Creating generators in Python is very simple: instead of a single `return` value, a function should `yield` a series of values instead. Usually the `yield` statement is put in some kind of loop. Here's an example of an generator that simply counts up to a given number:\n\n```python\ndef my_generator(x):\n    for i in range(x):\n        yield i\n```\n\nYou supply a generator into Gradio the same way as you would a regular function. For example, here's a a (fake) image generation model that generates noise for several steps before outputting an image using the `gr.Interface` class:\n\n```python\nimport gradio as gr\nimport numpy as np\nimport time\n\ndef fake_diffusion(steps):\n    rng = np.random.default_rng()\n    for i in range(steps):\n        time.sleep(1)\n        image = rng.random(size=(600, 600, 3))\n        yield image\n    image = np.ones((1000,1000,3), np.uint8)\n    image[:] = [255, 124, 0]\n    yield image\n\n\ndemo = gr.Interface(fake_diffusion,\n                    inputs=gr.Slider(1, 10, 3, step=1),\n                    outputs=\"image\")\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/fake_diffusion'>\u003C/gradio-app>\n\nNote that we've added a `time.sleep(1)` in the iterator to create an artificial pause between steps so that you are able to observe the steps of the iterator (in a real image generation model, this probably wouldn't be necessary).\n\n## Streaming inputs\n\nSimilarly, Gradio can handle streaming inputs, e.g. a live audio stream that can gets transcribed to text in real time, or an image generation model that reruns every time a user types a letter in a textbox. This is covered in more details in our guide on building [reactive Interfaces](/guides/reactive-interfaces). \n\n## Alert modals\n\nYou may wish to raise alerts to the user. To do so, raise a `gr.Error(\"custom message\")` to display an error message. You can also issue `gr.Warning(\"message\")` and `gr.Info(\"message\")` by having them as standalone lines in your function, which will immediately display modals while continuing the execution of your function. Queueing needs to be enabled for this to work.\n\nNote below how the `gr.Error` has to be raised, while the `gr.Warning` and `gr.Info` are single lines.\n\n```python\ndef start_process(name):\n    gr.Info(\"Starting process\")\n    if name is None:\n        gr.Warning(\"Name is empty\")\n    ...\n    if success == False:\n        raise gr.Error(\"Process failed\")\n```\n\n\n\n## Styling\n\nGradio themes are the easiest way to customize the look and feel of your app. You can choose from a variety of themes, or create your own. To do so, pass the `theme=` kwarg to the `Interface` constructor. For example:\n\n```python\ndemo = gr.Interface(..., theme=gr.themes.Monochrome())\n```\n\nGradio comes with a set of prebuilt themes which you can load from `gr.themes.*`. You can extend these themes or create your own themes from scratch - see the [theming guide](https://gradio.app/guides/theming-guide) for more details.\n\nFor additional styling ability, you can pass any CSS (as well as custom JavaScript) to your Gradio application. This is discussed in more detail in our [custom JS and CSS guide](/guides/custom-CSS-and-JS).\n\n\n## Progress bars\n\nGradio supports the ability to create custom Progress Bars so that you have customizability and control over the progress update that you show to the user. In order to enable this, simply add an argument to your method that has a default value of a `gr.Progress` instance. Then you can update the progress levels by calling this instance directly with a float between 0 and 1, or using the `tqdm()` method of the `Progress` instance to track progress over an iterable, as shown below.\n\n```python\nimport gradio as gr\nimport time\n\ndef slowly_reverse(word, progress=gr.Progress()):\n    progress(0, desc=\"Starting\")\n    time.sleep(1)\n    progress(0.05)\n    new_string = \"\"\n    for letter in progress.tqdm(word, desc=\"Reversing\"):\n        time.sleep(0.25)\n        new_string = letter + new_string\n    return new_string\n\ndemo = gr.Interface(slowly_reverse, gr.Text(), gr.Text())\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/progress_simple'>\u003C/gradio-app>\n\nIf you use the `tqdm` library, you can even report progress updates automatically from any `tqdm.tqdm` that already exists within your function by setting the default argument as `gr.Progress(track_tqdm=True)`!\n\n## Batch functions\n\nGradio supports the ability to pass _batch_ functions. Batch functions are just\nfunctions which take in a list of inputs and return a list of predictions.\n\nFor example, here is a batched function that takes in two lists of inputs (a list of\nwords and a list of ints), and returns a list of trimmed words as output:\n\n```py\nimport time\n\ndef trim_words(words, lens):\n    trimmed_words = []\n    time.sleep(5)\n    for w, l in zip(words, lens):\n        trimmed_words.append(w[:int(l)])\n    return [trimmed_words]\n```\n\nThe advantage of using batched functions is that if you enable queuing, the Gradio server can automatically _batch_ incoming requests and process them in parallel,\npotentially speeding up your demo. Here's what the Gradio code looks like (notice the `batch=True` and `max_batch_size=16`)\n\nWith the `gr.Interface` class:\n\n```python\ndemo = gr.Interface(\n    fn=trim_words, \n    inputs=[\"textbox\", \"number\"], \n    outputs=[\"output\"],\n    batch=True, \n    max_batch_size=16\n)\n\ndemo.launch()\n```\n\nWith the `gr.Blocks` class:\n\n```py\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        word = gr.Textbox(label=\"word\")\n        leng = gr.Number(label=\"leng\")\n        output = gr.Textbox(label=\"Output\")\n    with gr.Row():\n        run = gr.Button()\n\n    event = run.click(trim_words, [word, leng], output, batch=True, max_batch_size=16)\n\ndemo.launch()\n```\n\nIn the example above, 16 requests could be processed in parallel (for a total inference time of 5 seconds), instead of each request being processed separately (for a total\ninference time of 80 seconds). Many Hugging Face `transformers` and `diffusers` models work very naturally with Gradio's batch mode: here's [an example demo using diffusers to\ngenerate images in batches](https://github.com/gradio-app/gradio/blob/main/demo/diffusers_with_batching/run.py)\n\n\n\n",tags:[],spaces:[],url:"/guides/key-features/",contributor:null},{name:"sharing-your-app",category:"additional-features",pretty_category:"Additional Features",guide_index:7,absolute_index:14,pretty_name:"Sharing Your App",content:"# Sharing Your App\n\nIn this Guide, we dive more deeply into the various aspects of sharing a Gradio app with others. We will cover:\n\n1. [Sharing demos with the share parameter](#sharing-demos)\n2. [Hosting on HF Spaces](#hosting-on-hf-spaces)\n3. [Embedding hosted spaces](#embedding-hosted-spaces)\n4. [Using the API page](#api-page)\n5. [Accessing network requests](#accessing-the-network-request-directly)\n6. [Mounting within FastAPI](#mounting-within-another-fast-api-app)\n7. [Authentication](#authentication)\n8. [Security and file access](#security-and-file-access)\n9. [Analytics](#analytics)\n\n## Sharing Demos\n\nGradio demos can be easily shared publicly by setting `share=True` in the `launch()` method. Like this:\n\n```python\nimport gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(fn=greet, inputs=\"textbox\", outputs=\"textbox\")\n    \ndemo.launch(share=True)  # Share your demo with just 1 extra parameter üöÄ\n```\n\nThis generates a public, shareable link that you can send to anybody! When you send this link, the user on the other side can try out the model in their browser. Because the processing happens on your device (as long as your device stays on), you don't have to worry about any packaging any dependencies. \n\n![sharing](https://github.com/gradio-app/gradio/blob/main/guides/assets/sharing.svg?raw=true)\n\n\nA share link usually looks something like this: **https://07ff8706ab.gradio.live**. Although the link is served through the Gradio Share Servers, these servers are only a proxy for your local server, and do not store any data sent through your app. Share links expire after 72 hours. (it is [also possible to set up your own Share Server](https://github.com/huggingface/frp/) on your own cloud server to overcome this restriction.)\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> Keep in mind that share links are publicly accessible, meaning that anyone can use your model for prediction! Therefore, make sure not to expose any sensitive information through the functions you write, or allow any critical changes to occur on your device. Or you can [add authentication to your Gradio app](#authentication) as discussed below.\u003C/p>\n\nNote that by default, `share=False`, which means that your server is only running locally. (This is the default, except in Google Colab notebooks, where share links are automatically created). As an alternative to using share links, you can use use [SSH port-forwarding](https://www.ssh.com/ssh/tunneling/example) to share your local server with specific users.\n\n\n## Hosting on HF Spaces\n\nIf you'd like to have a permanent link to your Gradio demo on the internet, use Hugging Face Spaces. [Hugging Face Spaces](http://huggingface.co/spaces/) provides the infrastructure to permanently host your machine learning model for free!\n\nAfter you have [created a free Hugging Face account](https://huggingface.co/join), you have two methods to deploy your Gradio app to Hugging Face Spaces:\n\n1. From terminal: run `gradio deploy` in your app directory. The CLI will gather some basic metadata and then launch your app. To update your space, you can re-run this command or enable the Github Actions option to automatically update the Spaces on `git push`.\n\n2. From your browser: Drag and drop a folder containing your Gradio model and all related files [here](https://huggingface.co/new-space). See [this guide how to host on Hugging Face Spaces](https://huggingface.co/blog/gradio-spaces) for more information, or watch the embedded video:\n\n\u003Cvideo autoplay muted loop>\n  \u003Csource src=\"https://github.com/gradio-app/gradio/blob/main/guides/assets/hf_demo.mp4?raw=true\" type=\"video/mp4\" />\n\u003C/video>\n\n\n## Embedding Hosted Spaces\n\nOnce you have hosted your app on Hugging Face Spaces (or on your own server), you may want to embed the demo on a different website, such as your blog or your portfolio. Embedding an interactive demo allows people to try out the machine learning model that you have built, without needing to download or install anything ‚Äî right in their browser! The best part is that you can embed interactive demos even in static websites, such as GitHub pages.\n\nThere are two ways to embed your Gradio demos. You can find quick links to both options directly on the Hugging Face Space page, in the \"Embed this Space\" dropdown option:\n\n![Embed this Space dropdown option](https://github.com/gradio-app/gradio/blob/main/guides/assets/embed_this_space.png?raw=true)\n\n### Embedding with Web Components\n\nWeb components typically offer a better experience to users than IFrames. Web components load lazily, meaning that they won't slow down the loading time of your website, and they automatically adjust their height based on the size of the Gradio app.\n\nTo embed with Web Components:\n\n1. Import the gradio JS library into into your site by adding the script below in your site (replace {GRADIO_VERSION} in the URL with the library version of Gradio you are using).\n\n```html\n\u003Cscript\n\ttype=\"module\"\n\tsrc=\"https://gradio.s3-us-west-2.amazonaws.com/{GRADIO_VERSION}/gradio.js\"\n>\u003C/script>\n```\n\n2. Add\n\n```html\n\u003Cgradio-app src=\"https://$your_space_host.hf.space\">\u003C/gradio-app>\n```\n\nelement where you want to place the app. Set the `src=` attribute to your Space's embed URL, which you can find in the \"Embed this Space\" button. For example:\n\n```html\n\u003Cgradio-app\n\tsrc=\"https://abidlabs-pytorch-image-classifier.hf.space\"\n>\u003C/gradio-app>\n```\n\n\u003Cscript>\nfetch(\"https://pypi.org/pypi/gradio/json\"\n).then(r => r.json()\n).then(obj => {\n    let v = obj.info.version;\n    content = document.querySelector('.prose');\n    content.innerHTML = content.innerHTML.replaceAll(\"{GRADIO_VERSION}\", v);\n});\n\u003C/script>\n\nYou can see examples of how web components look \u003Ca href=\"https://www.gradio.app\">on the Gradio landing page\u003C/a>.\n\nYou can also customize the appearance and behavior of your web component with attributes that you pass into the `\u003Cgradio-app>` tag:\n\n- `src`: as we've seen, the `src` attributes links to the URL of the hosted Gradio demo that you would like to embed\n- `space`: an optional shorthand if your Gradio demo is hosted on Hugging Face Space. Accepts a `username/space_name` instead of a full URL. Example: `gradio/Echocardiogram-Segmentation`. If this attribute attribute is provided, then `src` does not need to be provided.\n- `control_page_title`: a boolean designating whether the html title of the page should be set to the title of the Gradio app (by default `\"false\"`)\n- `initial_height`: the initial height of the web component while it is loading the Gradio app, (by default `\"300px\"`). Note that the final height is set based on the size of the Gradio app.\n- `container`: whether to show the border frame and information about where the Space is hosted (by default `\"true\"`)\n- `info`: whether to show just the information about where the Space is hosted underneath the embedded app (by default `\"true\"`)\n- `autoscroll`: whether to autoscroll to the output when prediction has finished (by default `\"false\"`)\n- `eager`: whether to load the Gradio app as soon as the page loads (by default `\"false\"`)\n- `theme_mode`: whether to use the `dark`, `light`, or default `system` theme mode (by default `\"system\"`)\n- `render`: an event that is triggered once the embedded space has finished rendering.\n\nHere's an example of how to use these attributes to create a Gradio app that does not lazy load and has an initial height of 0px.\n\n```html\n\u003Cgradio-app\n\tspace=\"gradio/Echocardiogram-Segmentation\"\n\teager=\"true\"\n\tinitial_height=\"0px\"\n>\u003C/gradio-app>\n```\n\nHere's another example of how to use the `render` event. An event listener is used to capture the `render` event and will call the `handleLoadComplete()` function once rendering is complete. \n\n```html\n\u003Cscript>\n\tfunction handleLoadComplete() {\n\t\tconsole.log(\"Embedded space has finished rendering\");\n\t}\n\n\tconst gradioApp = document.querySelector(\"gradio-app\");\n\tgradioApp.addEventListener(\"render\", handleLoadComplete);\n\u003C/script>\n```\n\n_Note: While Gradio's CSS will never impact the embedding page, the embedding page can affect the style of the embedded Gradio app. Make sure that any CSS in the parent page isn't so general that it could also apply to the embedded Gradio app and cause the styling to break. Element selectors such as `header { ... }` and `footer { ... }` will be the most likely to cause issues._\n\n### Embedding with IFrames\n\nTo embed with IFrames instead (if you cannot add javascript to your website, for example), add this element:\n\n```html\n\u003Ciframe src=\"https://$your_space_host.hf.space\">\u003C/iframe>\n```\n\nAgain, you can find the `src=` attribute to your Space's embed URL, which you can find in the \"Embed this Space\" button.\n\nNote: if you use IFrames, you'll probably want to add a fixed `height` attribute and set `style=\"border:0;\"` to remove the boreder. In addition, if your app requires permissions such as access to the webcam or the microphone, you'll need to provide that as well using the `allow` attribute.\n\n## API Page\n\nYou can use almost any Gradio app as an API! In the footer of a Gradio app [like this one](https://huggingface.co/spaces/gradio/hello_world), you'll see a \"Use via API\" link.\n\n![Use via API](https://github.com/gradio-app/gradio/blob/main/guides/assets/use_via_api.png?raw=true)\n\nThis is a page that lists the endpoints that can be used to query the Gradio app, via our supported clients: either [the Python client](https://gradio.app/guides/getting-started-with-the-python-client/), or [the JavaScript client](https://gradio.app/guides/getting-started-with-the-js-client/). For each endpoint, Gradio automatically generates the parameters and their types, as well as example inputs, like this.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/view-api.png)\n\nThe endpoints are automatically created when you launch a Gradio `Interface`. If you are using Gradio `Blocks`, you can also set up a Gradio API page, though we recommend that you explicitly name each event listener, such as\n\n```python\nbtn.click(add, [num1, num2], output, api_name=\"addition\")\n```\n\nThis will add and document the endpoint `/api/addition/` to the automatically generated API page. Otherwise, your API endpoints will appear as \"unnamed\" endpoints.\n\n## Accessing the Network Request Directly\n\nWhen a user makes a prediction to your app, you may need the underlying network request, in order to get the request headers (e.g. for advanced authentication), log the client's IP address, getting the query parameters, or for other reasons. Gradio supports this in a similar manner to FastAPI: simply add a function parameter whose type hint is `gr.Request` and Gradio will pass in the network request as that parameter. Here is an example:\n\n```python\nimport gradio as gr\n\ndef echo(text, request: gr.Request):\n    if request:\n        print(\"Request headers dictionary:\", request.headers)\n        print(\"IP address:\", request.client.host)\n        print(\"Query parameters:\", dict(request.query_params))\n    return text\n\nio = gr.Interface(echo, \"textbox\", \"textbox\").launch()\n```\n\nNote: if your function is called directly instead of through the UI (this happens, for\nexample, when examples are cached, or when the Gradio app is called via API), then `request` will be `None`. \nYou should handle this case explicitly to ensure that your app does not throw any errors. That is why\nwe have the explicit check `if request`.\n\n## Mounting Within Another FastAPI App\n\nIn some cases, you might have an existing FastAPI app, and you'd like to add a path for a Gradio demo.\nYou can easily do this with `gradio.mount_gradio_app()`.\n\nHere's a complete example:\n\n```python\nfrom fastapi import FastAPI\nimport gradio as gr\n\nCUSTOM_PATH = \"/gradio\"\n\napp = FastAPI()\n\n\n@app.get(\"/\")\ndef read_main():\n    return {\"message\": \"This is your main app\"}\n\n\nio = gr.Interface(lambda x: \"Hello, \" + x + \"!\", \"textbox\", \"textbox\")\napp = gr.mount_gradio_app(app, io, path=CUSTOM_PATH)\n\n\n# Run this from the terminal as you would normally start a FastAPI app: `uvicorn run:app`\n# and navigate to http://localhost:8000/gradio in your browser.\n\n```\n\nNote that this approach also allows you run your Gradio apps on custom paths (`http://localhost:8000/gradio` in the example above).\n\n\n## Authentication\n\n### Password-protected app\n\nYou may wish to put an authentication page in front of your app to limit who can open your app. With the `auth=` keyword argument in the `launch()` method, you can provide a tuple with a username and password, or a list of acceptable username/password tuples; Here's an example that provides password-based authentication for a single user named \"admin\":\n\n```python\ndemo.launch(auth=(\"admin\", \"pass1234\"))\n```\n\nFor more complex authentication handling, you can even pass a function that takes a username and password as arguments, and returns `True` to allow access, `False` otherwise.\n\nHere's an example of a function that accepts any login where the username and password are the same:\n\n```python\ndef same_auth(username, password):\n    return username == password\ndemo.launch(auth=same_auth)\n```\n\nIf you have multiple users, you may wish to customize the content that is shown depending on the user that is logged in. You can retrieve the logged in user by [accessing the network request directly](#accessing-the-network-request-directly) as discussed above, and then reading the `.username` attribute of the request. Here's an example:\n\n\n```python\nimport gradio as gr\n\ndef update_message(request: gr.Request):\n    return f\"Welcome, {request.username}\"\n\nwith gr.Blocks() as demo:\n    m = gr.Markdown()\n    demo.load(update_message, None, m)\n    \ndemo.launch(auth=[(\"Abubakar\", \"Abubakar\"), (\"Ali\", \"Ali\")])\n```\n\nNote: For authentication to work properly, third party cookies must be enabled in your browser. This is not the case by default for Safari or for Chrome Incognito Mode. \n\nIf users visit the `/logout` page of your Gradio app, they will automatically be logged out and session cookies deleted. This allows you to add logout functionality to your Gradio app as well. Let's update the previous example to include a log out button:\n\n```python\nimport gradio as gr\n\ndef update_message(request: gr.Request):\n    return f\"Welcome, {request.username}\"\n\nwith gr.Blocks() as demo:\n    m = gr.Markdown()\n    logout_button = gr.Button(\"Logout\", link=\"/logout\")\n    demo.load(update_message, None, m)\n    \ndemo.launch(auth=[(\"Pete\", \"Pete\"), (\"Dawood\", \"Dawood\")])\n```\n\nNote: Gradio's built-in authentication provides a straightforward and basic layer of access control but does not offer robust security features for applications that require stringent access controls (e.g.  multi-factor authentication, rate limiting, or automatic lockout policies).\n\n### OAuth (Login via Hugging Face)\n\nGradio natively supports OAuth login via Hugging Face. In other words, you can easily add a _\"Sign in with Hugging Face\"_ button to your demo, which allows you to get a user's HF username as well as other information from their HF profile. Check out [this Space](https://huggingface.co/spaces/Wauplin/gradio-oauth-demo) for a live demo.\n\nTo enable OAuth, you must set `hf_oauth: true` as a Space metadata in your README.md file. This will register your Space\nas an OAuth application on Hugging Face. Next, you can use `gr.LoginButton` to add a login button to\nyour Gradio app. Once a user is logged in with their HF account, you can retrieve their profile by adding a parameter of type\n`gr.OAuthProfile` to any Gradio function. The user profile will be automatically injected as a parameter value. If you want\nto perform actions on behalf of the user (e.g. list user's private repos, create repo, etc.), you can retrieve the user\ntoken by adding a parameter of type `gr.OAuthToken`. You must define which scopes you will use in your Space metadata\n(see [documentation](https://huggingface.co/docs/hub/spaces-oauth#scopes) for more details).\n\nHere is a short example:\n\n```py\nimport gradio as gr\nfrom huggingface_hub import whoami\n\ndef hello(profile: gr.OAuthProfile | None) -> str:\n    if profile is None:\n        return \"I don't know you.\"\n    return f\"Hello {profile.name}\"\n\ndef list_organizations(oauth_token: gr.OAuthToken | None) -> str:\n    if oauth_token is None:\n        return \"Please log in to list organizations.\"\n    org_names = [org[\"name\"] for org in whoami(oauth_token.token)[\"orgs\"]]\n    return f\"You belong to {', '.join(org_names)}.\"\n\nwith gr.Blocks() as demo:\n    gr.LoginButton()\n    m1 = gr.Markdown()\n    m2 = gr.Markdown()\n    demo.load(hello, inputs=None, outputs=m1)\n    demo.load(list_organizations, inputs=None, outputs=m2)\n\ndemo.launch()\n```\n\nWhen the user clicks on the login button, they get redirected in a new page to authorize your Space.\n\n\u003Ccenter>\n\u003Cimg src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/oauth_sign_in.png\" style=\"width:300px; max-width:80%\">\n\u003C/center>\n\nUsers can revoke access to their profile at any time in their [settings](https://huggingface.co/settings/connected-applications).\n\nAs seen above, OAuth features are available only when your app runs in a Space. However, you often need to test your app\nlocally before deploying it. To test OAuth features locally, your machine must be logged in to Hugging Face. Please run `huggingface-cli login` or set `HF_TOKEN` as environment variable with one of your access token. You can generate a new token in your settings page (https://huggingface.co/settings/tokens). Then, clicking on the `gr.LoginButton` will login your local Hugging Face profile, allowing you to debug your app with your Hugging Face account before deploying it to a Space.\n\n\n### OAuth (with external providers)\n\nIt is also possible to authenticate with external OAuth providers (e.g. Google OAuth) in your Gradio apps. To do this, first mount your Gradio app within a FastAPI app ([as discussed above](#mounting-within-another-fast-api-app)). Then, you must write an *authentication function*, which gets the user's username from the OAuth provider and returns it. This function should be passed to the `auth_dependency` parameter in `gr.mount_gradio_app`. \n\nSimilar to [FastAPI dependency functions](https://fastapi.tiangolo.com/tutorial/dependencies/), the function specified by `auth_dependency` will run before any Gradio-related route in your FastAPI app. The function should accept a single parameter: the FastAPI `Request` and return either a string (representing a user's username) or `None`. If a string is returned, the user will be able to access the Gradio-related routes in your FastAPI app. \n\nFirst, let's show a simplistic example to illustrate the `auth_dependency` parameter:\n\n```python\nfrom fastapi import FastAPI, Request\nimport gradio as gr\n\napp = FastAPI()\n\ndef get_user(request: Request):\n    return request.headers.get(\"user\")\n\ndemo = gr.Interface(lambda s: f\"Hello {s}!\", \"textbox\", \"textbox\")\n\napp = gr.mount_gradio_app(app, demo, path=\"/demo\", auth_dependency=get_user)\n\nif __name__ == '__main__':\n    uvicorn.run(app)\n```\n\nIn this example, only requests that include a \"user\" header will be allowed to access the Gradio app. Of course, this does not add much security, since any user can add this header in their request.\n\nHere's a more complete example showing how to add Google OAuth to a Gradio app (assuming you've already created OAuth Credentials on the [Google Developer Console](https://console.cloud.google.com/project)):\n\n```python\nimport os\nfrom authlib.integrations.starlette_client import OAuth, OAuthError\nfrom fastapi import FastAPI, Depends, Request\nfrom starlette.config import Config\nfrom starlette.responses import RedirectResponse\nfrom starlette.middleware.sessions import SessionMiddleware\nimport uvicorn\nimport gradio as gr\n\napp = FastAPI()\n\n# Replace these with your own OAuth settings\nGOOGLE_CLIENT_ID = \"...\"\nGOOGLE_CLIENT_SECRET = \"...\"\nSECRET_KEY = \"...\"\n\nconfig_data = {'GOOGLE_CLIENT_ID': GOOGLE_CLIENT_ID, 'GOOGLE_CLIENT_SECRET': GOOGLE_CLIENT_SECRET}\nstarlette_config = Config(environ=config_data)\noauth = OAuth(starlette_config)\noauth.register(\n    name='google',\n    server_metadata_url='https://accounts.google.com/.well-known/openid-configuration',\n    client_kwargs={'scope': 'openid email profile'},\n)\n\nSECRET_KEY = os.environ.get('SECRET_KEY') or \"a_very_secret_key\"\napp.add_middleware(SessionMiddleware, secret_key=SECRET_KEY)\n\n# Dependency to get the current user\ndef get_user(request: Request):\n    user = request.session.get('user')\n    if user:\n        return user['name']\n    return None\n\n@app.get('/')\ndef public(user: dict = Depends(get_user)):\n    if user:\n        return RedirectResponse(url='/gradio')\n    else:\n        return RedirectResponse(url='/login-demo')\n\n@app.route('/logout')\nasync def logout(request: Request):\n    request.session.pop('user', None)\n    return RedirectResponse(url='/')\n\n@app.route('/login')\nasync def login(request: Request):\n    redirect_uri = request.url_for('auth')\n    # If your app is running on https, you should ensure that the\n    # `redirect_uri` is https, e.g. uncomment the following lines:\n    # \n    # from urllib.parse import urlparse, urlunparse\n    # redirect_uri = urlunparse(urlparse(str(redirect_uri))._replace(scheme='https'))\n    return await oauth.google.authorize_redirect(request, redirect_uri)\n\n@app.route('/auth')\nasync def auth(request: Request):\n    try:\n        access_token = await oauth.google.authorize_access_token(request)\n    except OAuthError:\n        return RedirectResponse(url='/')\n    request.session['user'] = dict(access_token)[\"userinfo\"]\n    return RedirectResponse(url='/')\n\nwith gr.Blocks() as login_demo:\n    gr.Button(\"Login\", link=\"/login\")\n\napp = gr.mount_gradio_app(app, login_demo, path=\"/login-demo\")\n\ndef greet(request: gr.Request):\n    return f\"Welcome to Gradio, {request.username}\"\n\nwith gr.Blocks() as main_demo:\n    m = gr.Markdown(\"Welcome to Gradio!\")\n    gr.Button(\"Logout\", link=\"/logout\")\n    main_demo.load(greet, None, m)\n\napp = gr.mount_gradio_app(app, main_demo, path=\"/gradio\", auth_dependency=get_user)\n\nif __name__ == '__main__':\n    uvicorn.run(app)\n```\n\nThere are actually two separate Gradio apps in this example! One that simply displays a log in button (this demo is accessible to any user), while the other main demo is only accessible to users that are logged in. You can try this example out on [this Space](https://huggingface.co/spaces/gradio/oauth-example).\n\n\n\n## Security and File Access\n\nSharing your Gradio app with others (by hosting it on Spaces, on your own server, or through temporary share links) **exposes** certain files on the host machine to users of your Gradio app.\n\nIn particular, Gradio apps ALLOW users to access to four kinds of files:\n\n- **Temporary files created by Gradio.** These are files that are created by Gradio as part of running your prediction function. For example, if your prediction function returns a video file, then Gradio will save that video to a temporary cache on your device and then send the path to the file to the front end. You can customize the location of temporary cache files created by Gradio by setting the environment variable `GRADIO_TEMP_DIR` to an absolute path, such as `/home/usr/scripts/project/temp/`. You can delete the files created by your app when it shuts down with the `delete_cache` parameter of `gradio.Blocks`, `gradio.Interface`, and `gradio.ChatInterface`. This parameter is a tuple of integers of the form `[frequency, age]` where `frequency` is how often to delete files and `age` is the time in seconds since the file was last modified.\n\n\n- **Cached examples created by Gradio.** These are files that are created by Gradio as part of caching examples for faster runtimes, if you set `cache_examples=True` or `cache_examples=\"lazy\"` in `gr.Interface()`, `gr.ChatInterface()` or in `gr.Examples()`. By default, these files are saved in the `gradio_cached_examples/` subdirectory within your app's working directory. You can customize the location of cached example files created by Gradio by setting the environment variable `GRADIO_EXAMPLES_CACHE` to an absolute path or a path relative to your working directory.\n\n- **Files that you explicitly allow via the `allowed_paths` parameter in `launch()`**. This parameter allows you to pass in a list of additional directories or exact filepaths you'd like to allow users to have access to. (By default, this parameter is an empty list).\n\n- **Static files that you explicitly set via the `gr.set_static_paths` function**. This parameter allows you to pass in a list of directories or filenames that will be considered static. This means that they will not be copied to the cache and will be served directly from your computer. This can help save disk space and reduce the time your app takes to launch but be mindful of possible security implications.\n\nGradio DOES NOT ALLOW access to:\n\n- **Files that you explicitly block via the `blocked_paths` parameter in `launch()`**. You can pass in a list of additional directories or exact filepaths to the `blocked_paths` parameter in `launch()`. This parameter takes precedence over the files that Gradio exposes by default or by the `allowed_paths`.\n\n- **Any other paths on the host machine**. Users should NOT be able to access other arbitrary paths on the host.\n\nSharing your Gradio application will also allow users to upload files to your computer or server. You can set a maximum file size for uploads to prevent abuse and to preserve disk space. You can do this with the `max_file_size` parameter of `.launch`. For example, the following two code snippets limit file uploads to 5 megabytes per file.\n\n```python\nimport gradio as gr\n\ndemo = gr.Interface(lambda x: x, \"image\", \"image\")\n\ndemo.launch(max_file_size=\"5mb\")\n# or\ndemo.launch(max_file_size=5 * gr.FileSize.MB)\n```\n\nPlease make sure you are running the latest version of `gradio` for these security settings to apply.\n\n## Analytics\n\nBy default, Gradio collects certain analytics to help us better understand the usage of the `gradio` library. This includes the following information:\n\n* What environment the Gradio app is running on (e.g. Colab Notebook, Hugging Face Spaces)\n* What input/output components are being used in the Gradio app\n* Whether the Gradio app is utilizing certain advanced features, such as `auth` or `show_error` \n* The IP address which is used solely to measure the number of unique developers using Gradio \n* The version of Gradio that is running \n\nNo information is collected from _users_ of your Gradio app. If you'd like to diable analytics altogether, you can do so by setting the `analytics_enabled` parameter to `False` in `gr.Blocks`, `gr.Interface`, or `gr.ChatInterface`. Or, you can set the GRADIO_ANALYTICS_ENABLED environment variable to `\"False\"` to apply this to all Gradio apps created across your system.\n\n*Note*: this reflects the analytics policy as of `gradio>=4.32.0`. \n",tags:[],spaces:[],url:"/guides/sharing-your-app/",contributor:null},{name:"interface-state",category:"building-interfaces",pretty_category:"Building Interfaces",guide_index:3,absolute_index:5,pretty_name:"Interface State",content:"# Interface State\n\nSo far, we've assumed that your demos are *stateless*: that they do not persist information beyond a single function call. What if you want to modify the behavior of your demo based on previous interactions with the demo? There are two approaches in Gradio: *global state* and *session state*.\n\n## Global State\n\nIf the state is something that should be accessible to all function calls and all users, you can create a variable outside the function call and access it inside the function. For example, you may load a large model outside the function and use it inside the function so that every function call does not need to reload the model.\n\n```python\nimport gradio as gr\n\nscores = []\n\ndef track_score(score):\n    scores.append(score)\n    top_scores = sorted(scores, reverse=True)[:3]\n    return top_scores\n\ndemo = gr.Interface(\n    track_score, \n    gr.Number(label=\"Score\"), \n    gr.JSON(label=\"Top Scores\")\n)\ndemo.launch()\n```\n\nIn the code above, the `scores` array is shared between all users. If multiple users are accessing this demo, their scores will all be added to the same list, and the returned top 3 scores will be collected from this shared reference.\n\n## Session State\n\nAnother type of data persistence Gradio supports is session state, where data persists across multiple submits within a page session. However, data is _not_ shared between different users of your model. To store data in a session state, you need to do three things:\n\n1. Pass in an extra parameter into your function, which represents the state of the interface.\n2. At the end of the function, return the updated value of the state as an extra return value.\n3. Add the `'state'` input and `'state'` output components when creating your `Interface`\n\nHere's a simple app to illustrate session state - this app simply stores users previous submissions and displays them back to the user:\n\n\n```python\nimport gradio as gr\n\ndef store_message(message: str, history: list[str]):\n    output = {\n        \"Current messages\": message,\n        \"Previous messages\": history[::-1]\n    }\n    history.append(message)\n    return output, history\n\ndemo = gr.Interface(fn=store_message, \n                    inputs=[\"textbox\", gr.State(value=[])], \n                    outputs=[\"json\", gr.State()])\n\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/interface_state'>\u003C/gradio-app>\n\n\nNotice how the state persists across submits within each page, but if you load this demo in another tab (or refresh the page), the demos will not share chat history. Here, we could not store the submission history in a global variable, otherwise the submission history would then get jumbled between different users.\n\nThe initial value of the `State` is `None` by default. If you pass a parameter to the `value` argument of `gr.State()`, it is used as the default value of the state instead. \n\nNote: the `Interface` class only supports a single session state variable (though it can be a list with multiple elements). For more complex use cases, you can use Blocks, [which supports multiple `State` variables](/guides/state-in-blocks/). Alternatively, if you are building a chatbot that maintains user state, consider using the `ChatInterface` abstraction, [which manages state automatically](/guides/creating-a-chatbot-fast).\n",tags:[],spaces:[],url:"/guides/interface-state/",contributor:null},{name:"reactive-interfaces",category:"building-interfaces",pretty_category:"Building Interfaces",guide_index:4,absolute_index:6,pretty_name:"Reactive Interfaces",content:"# Reactive Interfaces\n\nFinally, we cover how to get Gradio demos to refresh automatically or continuously stream data.\n\n## Live Interfaces\n\nYou can make interfaces automatically refresh by setting `live=True` in the interface. Now the interface will recalculate as soon as the user input changes.\n\n```python\nimport gradio as gr\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        return num1 / num2\n\ndemo = gr.Interface(\n    calculator,\n    [\n        \"number\",\n        gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]),\n        \"number\"\n    ],\n    \"number\",\n    live=True,\n)\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/calculator_live'>\u003C/gradio-app>\n\nNote there is no submit button, because the interface resubmits automatically on change.\n\n## Streaming Components\n\nSome components have a \"streaming\" mode, such as `Audio` component in microphone mode, or the `Image` component in webcam mode. Streaming means data is sent continuously to the backend and the `Interface` function is continuously being rerun.\n\nThe difference between `gr.Audio(source='microphone')` and `gr.Audio(source='microphone', streaming=True)`, when both are used in `gr.Interface(live=True)`, is that the first `Component` will automatically submit data and run the `Interface` function when the user stops recording, whereas the second `Component` will continuously send data and run the `Interface` function _during_ recording.\n\nHere is example code of streaming images from the webcam.\n\n```python\nimport gradio as gr\nimport numpy as np\n\ndef flip(im):\n    return np.flipud(im)\n\ndemo = gr.Interface(\n    flip, \n    gr.Image(sources=[\"webcam\"], streaming=True), \n    \"image\",\n    live=True\n)\ndemo.launch()\n    \n```\n\nStreaming can also be done in an output component. A `gr.Audio(streaming=True)` output component can take a stream of audio data yielded piece-wise by a generator function and combines them into a single audio file.\n\n```python\nimport gradio as gr\nfrom pydub import AudioSegment\nfrom time import sleep\n\nwith gr.Blocks() as demo:\n    input_audio = gr.Audio(label=\"Input Audio\", type=\"filepath\", format=\"mp3\")\n    with gr.Row():\n        with gr.Column():\n            stream_as_file_btn = gr.Button(\"Stream as File\")\n            format = gr.Radio([\"wav\", \"mp3\"], value=\"wav\", label=\"Format\")\n            stream_as_file_output = gr.Audio(streaming=True)\n\n            def stream_file(audio_file, format):\n                audio = AudioSegment.from_file(audio_file)\n                i = 0\n                chunk_size = 1000\n                while chunk_size * i \u003C len(audio):\n                    chunk = audio[chunk_size * i : chunk_size * (i + 1)]\n                    i += 1\n                    if chunk:\n                        file = f\"/tmp/{i}.{format}\"\n                        chunk.export(file, format=format)\n                        yield file\n                        sleep(0.5)\n\n            stream_as_file_btn.click(\n                stream_file, [input_audio, format], stream_as_file_output\n            )\n\n            gr.Examples(\n                [[\"audio/cantina.wav\", \"wav\"], [\"audio/cantina.wav\", \"mp3\"]],\n                [input_audio, format],\n                fn=stream_file,\n                outputs=stream_as_file_output,\n            )\n\n        with gr.Column():\n            stream_as_bytes_btn = gr.Button(\"Stream as Bytes\")\n            stream_as_bytes_output = gr.Audio(format=\"bytes\", streaming=True)\n\n            def stream_bytes(audio_file):\n                chunk_size = 20_000\n                with open(audio_file, \"rb\") as f:\n                    while True:\n                        chunk = f.read(chunk_size)\n                        if chunk:\n                            yield chunk\n                            sleep(1)\n                        else:\n                            break\n            stream_as_bytes_btn.click(stream_bytes, input_audio, stream_as_bytes_output)\n\nif __name__ == \"__main__\":\n    demo.queue().launch()\n\n```\n\nFor a more detailed example, see our guide on performing [automatic speech recognition](/guides/real-time-speech-recognition) with Gradio.\n",tags:[],spaces:[],url:"/guides/reactive-interfaces/",contributor:null}],parent:"gradio"},tabbedinterface:{class:null,name:"TabbedInterface",description:"A TabbedInterface is created by providing a list of Interfaces or Blocks, each of which gets rendered in a separate tab. Only the components from the Interface/Blocks will be rendered in the tab. Certain high-level attributes of the Blocks (e.g. custom `css`, `js`, and `head` attributes) will not be loaded. \u003Cbr>",tags:{demos:"tabbed_interface_lite"},parameters:[{name:"interface_list",annotation:"list[Interface]",doc:"A list of Interfaces (or Blocks) to be rendered in the tabs."},{name:"tab_names",annotation:"list[str] | None",doc:"A list of tab names. If None, the tab names will be &quot;Tab 1&quot;, &quot;Tab 2&quot;, etc.",default:"None"},{name:"title",annotation:"str | None",doc:"The tab title to display when this demo is opened in a browser window.",default:"None"},{name:"theme",annotation:"Theme | str | None",doc:"A Theme object or a string representing a theme. If a string, will look for a built-in theme with that name (e.g. &quot;soft&quot; or &quot;default&quot;), or will attempt to load a theme from the Hugging Face Hub (e.g. &quot;gradio/monochrome&quot;). If None, will use the Default theme.",default:"None"},{name:"analytics_enabled",annotation:"bool | None",doc:"Whether to allow basic telemetry. If None, will use GRADIO_ANALYTICS_ENABLED environment variable or default to True.",default:"None"},{name:"css",annotation:"str | None",doc:"Custom css as a string or path to a css file. This css will be included in the demo webpage.",default:"None"},{name:"js",annotation:"str | None",doc:"Custom js as a string or path to a js file. The custom js should in the form of a single js function. This function will automatically be executed when the page loads. For more flexibility, use the head parameter to insert js inside &lt;script&gt; tags.",default:"None"},{name:"head",annotation:"str | None",doc:"Custom html to insert into the head of the demo webpage. This can be used to add custom meta tags, multiple scripts, stylesheets, etc. to the page.",default:"None"}],returns:{annotation:null,doc:"a Gradio Tabbed Interface for the given interfaces"},example:null,fns:[],demos:[["tabbed_interface_lite","import gradio as gr\n\nhello_world = gr.Interface(lambda name: \"Hello \" + name, \"text\", \"text\")\nbye_world = gr.Interface(lambda name: \"Bye \" + name, \"text\", \"text\")\n\ndemo = gr.TabbedInterface([hello_world, bye_world], [\"Hello World\", \"Bye World\"])\n\nif __name__ == \"__main__\":\n    demo.launch()"]],parent:"gradio"},render:{class:null,name:"render",description:"The render decorator allows Gradio Blocks apps to have dynamic layouts, so that the components and event listeners in your app can change depending on custom logic. Attaching a @gr.render decorator to a function will cause the function to be re-run whenever the inputs are changed (or specified triggers are activated). The function contains the components and event listeners that will update based on the inputs. \u003Cbr> The basic usage of @gr.render is as follows: \u003Cbr> 1. Create a function and attach the @gr.render decorator to it. \u003Cbr> 2. Add the input components to the `inputs=` argument of @gr.render, and create a corresponding argument in your function for each component. \u003Cbr> 3. Add all components inside the function that you want to update based on the inputs. Any event listeners that use these components should also be inside this function. \u003Cbr>",tags:{parameters:"inputs: List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.\u003Cbr>triggers: List of triggers to listen to, e.g. [btn.click, number.change]. If None, will listen to changes to any inputs.\u003Cbr>queue: If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.\u003Cbr>trigger_mode: If \"once\" (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to \"multiple\", unlimited submissions are allowed while pending, and \"always_last\" (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.\u003Cbr>concurrency_limit: If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to \"default\" to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).\u003Cbr>concurrency_id: If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit."},parameters:[{name:"inputs",annotation:"list[Component] | Component | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"triggers",annotation:"Sequence[EventListenerCallable] | EventListenerCallable | None",doc:"List of triggers to listen to, e.g. [btn.click, number.change]. If None, will listen to changes to any inputs.",default:"None"},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"\"always_last\""},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"None"},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"}],returns:{annotation:null},example:"import gradio as gr\n\nwith gr.Blocks() as demo:\n    input_text = gr.Textbox()\n\n    @gr.render(inputs=input_text)\n    def show_split(text):\n        if len(text) == 0:\n            gr.Markdown(\"## No Input Provided\")\n        else:\n            for letter in text:\n                with gr.Row():\n                    text = gr.Textbox(letter)\n                    btn = gr.Button(\"Clear\")\n                    btn.click(lambda: gr.Textbox(value=\"\"), None, text)",fns:[],parent:"gradio"}},components:{annotatedimage:{class:null,name:"AnnotatedImage",description:"Creates a component to displays a base image and colored annotations on top of that image. Annotations can take the from of rectangles (e.g. object detection) or masks (e.g. image segmentation). As this component does not accept user input, it is rarely used as an input component. \u003Cbr>",tags:{demos:"image_segmentation"},parameters:[{name:"value",annotation:"tuple[np.ndarray | PIL.Image.Image | str, list[tuple[np.ndarray | tuple[int, int, int, int], str]]] | None",doc:"Tuple of base image and list of (annotation, label) pairs.",default:"None"},{name:"format",annotation:"str",doc:"Format used to save images before it is returned to the front end, such as &#x27;jpeg&#x27; or &#x27;png&#x27;. This parameter only takes effect when the base image is returned from the prediction function as a numpy array or a PIL Image. The format should be supported by the PIL library.",default:"\"webp\""},{name:"show_legend",annotation:"bool",doc:"If True, will show a legend of the annotations.",default:"True"},{name:"height",annotation:"int | str | None",doc:"The height of the image, specified in pixels if a number is passed, or in CSS units if a string is passed.",default:"None"},{name:"width",annotation:"int | str | None",doc:"The width of the image, specified in pixels if a number is passed, or in CSS units if a string is passed.",default:"None"},{name:"color_map",annotation:"dict[str, str] | None",doc:"A dictionary mapping labels to colors. The colors must be specified as hex codes.",default:"None"},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"Relative width compared to adjacent Components in a Row. For example, if Component A has scale=2, and Component B has scale=1, A will be twice as wide as B. Should be an integer.",default:"None"},{name:"min_width",annotation:"int",doc:"Minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"select",description:"Event listener for when the user selects or deselects the AnnotatedImage. Uses event data gradio.SelectData to carry `value` referring to the label of the AnnotatedImage, and `selected` to refer to state of the AnnotatedImage. See EventData documentation on how to use this event data",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.AnnotatedImage"}],preprocess:{parameter_doc:[{name:"payload",annotation:"AnnotatedImageData | None",doc:"Dict of base image and list of annotations."}],return_doc:{annotation:"tuple[str, list[tuple[str, str]]] | None",doc:"Passes its value as a `tuple` consisting of a `str` filepath to a base image and `list` of annotations. Each annotation itself is `tuple` of a mask (as a `str` filepath to image) and a `str` label."}},postprocess:{parameter_doc:[{name:"value",annotation:"tuple[np.ndarray | PIL.Image.Image | str, list[tuple[np.ndarray | tuple[int, int, int, int], str]]] | None",doc:"Expects a a tuple of a base image and list of annotations: a `tuple[Image, list[Annotation]]`. The `Image` itself can be `str` filepath, `numpy.ndarray`, or `PIL.Image`. Each `Annotation` is a `tuple[Mask, str]`. The `Mask` can be either a `tuple` of 4 `int`'s representing the bounding box coordinates (x1, y1, x2, y2), or 0-1 confidence mask in the form of a `numpy.ndarray` of the same shape as the image, while the second element of the `Annotation` tuple is a `str` label."}],return_doc:{annotation:"AnnotatedImageData | None",doc:"Tuple of base image file and list of annotations, with each annotation a two-part tuple where the first element image path of the mask, and the second element is the label."}},string_shortcuts:[["AnnotatedImage","annotatedimage","Uses default values"]],demos:[["image_segmentation","import gradio as gr\nimport numpy as np\nimport random\n\nwith gr.Blocks() as demo:\n    section_labels = [\n        \"apple\",\n        \"banana\",\n        \"carrot\",\n        \"donut\",\n        \"eggplant\",\n        \"fish\",\n        \"grapes\",\n        \"hamburger\",\n        \"ice cream\",\n        \"juice\",\n    ]\n\n    with gr.Row():\n        num_boxes = gr.Slider(0, 5, 2, step=1, label=\"Number of boxes\")\n        num_segments = gr.Slider(0, 5, 1, step=1, label=\"Number of segments\")\n\n    with gr.Row():\n        img_input = gr.Image()\n        img_output = gr.AnnotatedImage(\n            color_map={\"banana\": \"#a89a00\", \"carrot\": \"#ffae00\"}\n        )\n\n    section_btn = gr.Button(\"Identify Sections\")\n    selected_section = gr.Textbox(label=\"Selected Section\")\n\n    def section(img, num_boxes, num_segments):\n        sections = []\n        for a in range(num_boxes):\n            x = random.randint(0, img.shape[1])\n            y = random.randint(0, img.shape[0])\n            w = random.randint(0, img.shape[1] - x)\n            h = random.randint(0, img.shape[0] - y)\n            sections.append(((x, y, x + w, y + h), section_labels[a]))\n        for b in range(num_segments):\n            x = random.randint(0, img.shape[1])\n            y = random.randint(0, img.shape[0])\n            r = random.randint(0, min(x, y, img.shape[1] - x, img.shape[0] - y))\n            mask = np.zeros(img.shape[:2])\n            for i in range(img.shape[0]):\n                for j in range(img.shape[1]):\n                    dist_square = (i - y) ** 2 + (j - x) ** 2\n                    if dist_square \u003C r**2:\n                        mask[i, j] = round((r**2 - dist_square) / r**2 * 4) / 4\n            sections.append((mask, section_labels[b + num_boxes]))\n        return (img, sections)\n\n    section_btn.click(section, [img_input, num_boxes, num_segments], img_output)\n\n    def select_section(evt: gr.SelectData):\n        return section_labels[evt.index]\n\n    img_output.select(select_section, None, selected_section)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]],parent:"gradio"},audio:{class:null,name:"Audio",description:"Creates an audio component that can be used to upload/record audio (as an input) or display audio (as an output).",tags:{demos:"generate_tone, reverse_audio",guides:"real-time-speech-recognition"},parameters:[{name:"value",annotation:"str | Path | tuple[int, np.ndarray] | Callable | None",doc:"A path, URL, or [sample_rate, numpy array] tuple (sample rate in Hz, audio data as a float or int numpy array) for the default value that Audio component is going to take. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"sources",annotation:"list[Literal[('upload', 'microphone')]] | None",doc:"A list of sources permitted for audio. &quot;upload&quot; creates a box where user can drop an audio file, &quot;microphone&quot; creates a microphone input. The first element in the list will be used as the default source. If None, defaults to [&quot;upload&quot;, &quot;microphone&quot;], or [&quot;microphone&quot;] if `streaming` is True.",default:"None"},{name:"type",annotation:"Literal[('numpy', 'filepath')]",doc:"The format the audio file is converted to before being passed into the prediction function. &quot;numpy&quot; converts the audio to a tuple consisting of: (int sample rate, numpy.array for the data), &quot;filepath&quot; passes a str path to a temporary file containing the audio.",default:"\"numpy\""},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"Relative width compared to adjacent Components in a Row. For example, if Component A has scale=2, and Component B has scale=1, A will be twice as wide as B. Should be an integer.",default:"None"},{name:"min_width",annotation:"int",doc:"Minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"interactive",annotation:"bool | None",doc:"If True, will allow users to upload and edit an audio file. If False, can only be used to play audio. If not provided, this is inferred based on whether the component is used as an input or output.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"streaming",annotation:"bool",doc:"If set to True when used in a `live` interface as an input, will automatically stream webcam feed. When used set as an output, takes audio chunks yield from the backend and combines them into one streaming audio output.",default:"False"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"format",annotation:"Literal[('wav', 'mp3')]",doc:"The file format to save audio files. Either &#x27;wav&#x27; or &#x27;mp3&#x27;. wav files are lossless but will tend to be larger files. mp3 files tend to be smaller. Default is wav. Applies both when this component is used as an input (when `type` is &quot;format&quot;) and when this component is used as an output.",default:"\"wav\""},{name:"autoplay",annotation:"bool",doc:"Whether to automatically play the audio when the component is used as an output. Note: browsers will not autoplay audio files if the user has not interacted with the page yet.",default:"False"},{name:"show_download_button",annotation:"bool | None",doc:"If True, will show a download button in the corner of the component for saving audio. If False, icon does not appear. By default, it will be True for output components and False for input components.",default:"None"},{name:"show_share_button",annotation:"bool | None",doc:"If True, will show a share icon in the corner of the component that allows user to share outputs to Hugging Face Spaces Discussions. If False, icon does not appear. If set to None (default behavior), then the icon appears if this Gradio app is launched on Spaces, but not otherwise.",default:"None"},{name:"editable",annotation:"bool",doc:"If True, allows users to manipulate the audio file if the component is interactive. Defaults to True.",default:"True"},{name:"min_length",annotation:"int | None",doc:"The minimum length of audio (in seconds) that the user can pass into the prediction function. If None, there is no minimum length.",default:"None"},{name:"max_length",annotation:"int | None",doc:"The maximum length of audio (in seconds) that the user can pass into the prediction function. If None, there is no maximum length.",default:"None"},{name:"waveform_options",annotation:"WaveformOptions | dict | None",doc:"A dictionary of options for the waveform display. Options include: waveform_color (str), waveform_progress_color (str), show_controls (bool), skip_length (int), trim_region_color (str). Default is None, which uses the default values for these options.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"stream",description:"This listener is triggered when the user streams the Audio.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"hidden\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Audio"},{fn:null,name:"change",description:"Triggered when the value of the Audio changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Audio"},{fn:null,name:"clear",description:"This listener is triggered when the user clears the Audio using the X button for the component.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Audio"},{fn:null,name:"play",description:"This listener is triggered when the user plays the media in the Audio.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Audio"},{fn:null,name:"pause",description:"This listener is triggered when the media in the Audio stops for any reason.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Audio"},{fn:null,name:"stop",description:"This listener is triggered when the user reaches the end of the media playing in the Audio.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Audio"},{fn:null,name:"pause",description:"This listener is triggered when the media in the Audio stops for any reason.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Audio"},{fn:null,name:"start_recording",description:"This listener is triggered when the user starts recording with the Audio.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Audio"},{fn:null,name:"pause_recording",description:"This listener is triggered when the user pauses recording with the Audio.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Audio"},{fn:null,name:"stop_recording",description:"This listener is triggered when the user stops recording with the Audio.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Audio"},{fn:null,name:"upload",description:"This listener is triggered when the user uploads a file into the Audio.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Audio"}],preprocess:{parameter_doc:[{name:"payload",annotation:"FileData | None",doc:"audio data as a FileData object, or None."}],return_doc:{annotation:"str | tuple[int, np.ndarray] | None",doc:"passes audio as one of these formats (depending on `type`): a `str` filepath, or `tuple` of (sample rate in Hz, audio data as numpy array). If the latter, the audio data is a 16-bit `int` array whose values range from -32768 to 32767 and shape of the audio data array is (samples,) for mono audio or (samples, channels) for multi-channel audio."}},postprocess:{parameter_doc:[{name:"value",annotation:"str | Path | bytes | tuple[int, np.ndarray] | None",doc:"expects audio data in any of these formats: a `str` or `pathlib.Path` filepath or URL to an audio file, or a `bytes` object (recommended for streaming), or a `tuple` of (sample rate in Hz, audio data as numpy array). Note: if audio is supplied as a numpy array, the audio will be normalized by its peak value to avoid distortion or clipping in the resulting audio."}],return_doc:{annotation:"FileData | bytes | None",doc:"FileData object, bytes, or None."}},string_shortcuts:[["Audio","audio","Uses default values"],["Microphone","microphone","Uses sources=[\"microphone\"]"]],demos:[["generate_tone","import numpy as np\nimport gradio as gr\n\nnotes = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n\ndef generate_tone(note, octave, duration):\n    sr = 48000\n    a4_freq, tones_from_a4 = 440, 12 * (octave - 4) + (note - 9)\n    frequency = a4_freq * 2 ** (tones_from_a4 / 12)\n    duration = int(duration)\n    audio = np.linspace(0, duration, duration * sr)\n    audio = (20000 * np.sin(audio * (2 * np.pi * frequency))).astype(np.int16)\n    return sr, audio\n\ndemo = gr.Interface(\n    generate_tone,\n    [\n        gr.Dropdown(notes, type=\"index\"),\n        gr.Slider(4, 6, step=1),\n        gr.Textbox(value=1, label=\"Duration in seconds\"),\n    ],\n    \"audio\",\n)\nif __name__ == \"__main__\":\n    demo.launch()\n"],["reverse_audio","import os\n\nimport numpy as np\n\nimport gradio as gr\n\n\ndef reverse_audio(audio):\n    sr, data = audio\n    return (sr, np.flipud(data))\n\n\ninput_audio = gr.Audio(\n    sources=[\"microphone\"],\n    waveform_options=gr.WaveformOptions(\n        waveform_color=\"#01C6FF\",\n        waveform_progress_color=\"#0066B4\",\n        skip_length=2,\n        show_controls=False,\n    ),\n)\ndemo = gr.Interface(\n    fn=reverse_audio,\n    inputs=input_audio,\n    outputs=\"audio\"\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]],guides:[{name:"real-time-speech-recognition",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:60,pretty_name:"Real Time Speech Recognition",content:"# Real Time Speech Recognition\n\n\n\n## Introduction\n\nAutomatic speech recognition (ASR), the conversion of spoken speech to text, is a very important and thriving area of machine learning. ASR algorithms run on practically every smartphone, and are becoming increasingly embedded in professional workflows, such as digital assistants for nurses and doctors. Because ASR algorithms are designed to be used directly by customers and end users, it is important to validate that they are behaving as expected when confronted with a wide variety of speech patterns (different accents, pitches, and background audio conditions).\n\nUsing `gradio`, you can easily build a demo of your ASR model and share that with a testing team, or test it yourself by speaking through the microphone on your device.\n\nThis tutorial will show how to take a pretrained speech-to-text model and deploy it with a Gradio interface. We will start with a **_full-context_** model, in which the user speaks the entire audio before the prediction runs. Then we will adapt the demo to make it **_streaming_**, meaning that the audio model will convert speech as you speak. \n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). You will also need a pretrained speech recognition model. In this tutorial, we will build demos from 2 ASR libraries:\n\n- Transformers (for this, `pip install transformers` and `pip install torch`)\n\nMake sure you have at least one of these installed so that you can follow along the tutorial. You will also need `ffmpeg` [installed on your system](https://www.ffmpeg.org/download.html), if you do not already have it, to process files from the microphone.\n\nHere's how to build a real time speech recognition (ASR) app:\n\n1. [Set up the Transformers ASR Model](#1-set-up-the-transformers-asr-model)\n2. [Create a Full-Context ASR Demo with Transformers](#2-create-a-full-context-asr-demo-with-transformers)\n3. [Create a Streaming ASR Demo with Transformers](#3-create-a-streaming-asr-demo-with-transformers)\n\n## 1. Set up the Transformers ASR Model\n\nFirst, you will need to have an ASR model that you have either trained yourself or you will need to download a pretrained model. In this tutorial, we will start by using a pretrained ASR model from the model, `whisper`.\n\nHere is the code to load `whisper` from Hugging Face `transformers`.\n\n```python\nfrom transformers import pipeline\n\np = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n```\n\nThat's it!\n\n## 2. Create a Full-Context ASR Demo with Transformers\n\nWe will start by creating a _full-context_ ASR demo, in which the user speaks the full audio before using the ASR model to run inference. This is very easy with Gradio -- we simply create a function around the `pipeline` object above.\n\nWe will use `gradio`'s built in `Audio` component, configured to take input from the user's microphone and return a filepath for the recorded audio. The output component will be a plain `Textbox`.\n\n```python\nimport gradio as gr\nfrom transformers import pipeline\nimport numpy as np\n\ntranscriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n\ndef transcribe(audio):\n    sr, y = audio\n    y = y.astype(np.float32)\n    y /= np.max(np.abs(y))\n\n    return transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]\n\n\ndemo = gr.Interface(\n    transcribe,\n    gr.Audio(sources=[\"microphone\"]),\n    \"text\",\n)\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/asr'>\u003C/gradio-app>\n\nThe `transcribe` function takes a single parameter, `audio`, which is a numpy array of the audio the user recorded. The `pipeline` object expects this in float32 format, so we convert it first to float32, and then extract the transcribed text.\n\n## 3. Create a Streaming ASR Demo with Transformers\n\nTo make this a *streaming* demo, we need to make these changes:\n\n1. Set `streaming=True` in the `Audio` component\n2. Set `live=True` in the `Interface`\n3. Add a `state` to the interface to store the recorded audio of a user\n\nTake a look below.\n\n```python\nimport gradio as gr\nfrom transformers import pipeline\nimport numpy as np\n\ntranscriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n\ndef transcribe(stream, new_chunk):\n    sr, y = new_chunk\n    y = y.astype(np.float32)\n    y /= np.max(np.abs(y))\n\n    if stream is not None:\n        stream = np.concatenate([stream, y])\n    else:\n        stream = y\n    return stream, transcriber({\"sampling_rate\": sr, \"raw\": stream})[\"text\"]\n\n\ndemo = gr.Interface(\n    transcribe,\n    [\"state\", gr.Audio(sources=[\"microphone\"], streaming=True)],\n    [\"state\", \"text\"],\n    live=True,\n)\n\ndemo.launch()\n\n```\n\nNotice now we have a state variable now, because we need to track all the audio history. `transcribe` gets called whenever there is a new small chunk of audio, but we also need to keep track of all the audio that has been spoken so far in state. \nAs the interface runs, the `transcribe` function gets called, with a record of all the previously spoken audio in `stream`, as well as the new chunk of audio as `new_chunk`. We return the new full audio so that can be stored back in state, and we also return the transcription.\nHere we naively append the audio together and simply call the `transcriber` object on the entire audio. You can imagine more efficient ways of handling this, such as re-processing only the last 5 seconds of audio whenever a new chunk of audio received. \n\n\u003Cgradio-app space='gradio/stream_asr'>\u003C/gradio-app>\n\nNow the ASR model will run inference as you speak! ",tags:["ASR","SPEECH","STREAMING"],spaces:[],url:"/guides/real-time-speech-recognition/",contributor:null}],parent:"gradio"},plot:{class:null,name:"Plot",description:"Creates a plot component to display various kinds of plots (matplotlib, plotly, altair, or bokeh plots are supported). As this component does not accept user input, it is rarely used as an input component. \u003Cbr>",tags:{demos:"blocks_kinematics, stock_forecast",guides:"plot-component-for-maps"},parameters:[{name:"value",annotation:"Any | None",doc:"Optionally, supply a default plot object to display, must be a matplotlib, plotly, altair, or bokeh figure, or a callable. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"format",annotation:"str",doc:"File format in which to send matplotlib plots to the front end, such as &#x27;jpg&#x27; or &#x27;png&#x27;.",default:"\"webp\""},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the Plot changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Plot"},{fn:null,name:"clear",description:"This listener is triggered when the user clears the Plot using the X button for the component.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Plot"}],preprocess:{parameter_doc:[{name:"payload",annotation:"PlotData | None",doc:"The data to display in the plot."}],return_doc:{annotation:"PlotData | None",doc:"(Rarely used) passes the data displayed in the plot as an PlotData dataclass, which includes the plot information as a JSON string, as well as the type of chart and the plotting library."}},postprocess:{parameter_doc:[{name:"value",annotation:"Any",doc:"Expects plot data in one of these formats: a matplotlib.Figure, bokeh.Model, plotly.Figure, or altair.Chart object."}],return_doc:{annotation:"PlotData | None",doc:"PlotData: A dataclass containing the plot data as a JSON string, as well as the type of chart and the plotting library."}},string_shortcuts:[["Plot","plot","Uses default values"]],demos:[["blocks_kinematics","import pandas as pd\nimport numpy as np\n\nimport gradio as gr\n\n\ndef plot(v, a):\n    g = 9.81\n    theta = a / 180 * 3.14\n    tmax = ((2 * v) * np.sin(theta)) / g\n    timemat = tmax * np.linspace(0, 1, 40)\n\n    x = (v * timemat) * np.cos(theta)\n    y = ((v * timemat) * np.sin(theta)) - ((0.5 * g) * (timemat**2))\n    df = pd.DataFrame({\"x\": x, \"y\": y})\n    return df\n\n\ndemo = gr.Blocks()\n\nwith demo:\n    gr.Markdown(\n        r\"Let's do some kinematics! Choose the speed and angle to see the trajectory. Remember that the range $R = v_0^2 \\cdot \\frac{\\sin(2\\theta)}{g}$\"\n    )\n\n    with gr.Row():\n        speed = gr.Slider(1, 30, 25, label=\"Speed\")\n        angle = gr.Slider(0, 90, 45, label=\"Angle\")\n    output = gr.LinePlot(\n        x=\"x\",\n        y=\"y\",\n        overlay_point=True,\n        tooltip=[\"x\", \"y\"],\n        x_lim=[0, 100],\n        y_lim=[0, 60],\n        width=350,\n        height=300,\n    )\n    btn = gr.Button(value=\"Run\")\n    btn.click(plot, [speed, angle], output)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"],["stock_forecast","import matplotlib.pyplot as plt\nimport numpy as np\n\nimport gradio as gr\n\n\ndef plot_forecast(final_year, companies, noise, show_legend, point_style):\n    start_year = 2020\n    x = np.arange(start_year, final_year + 1)\n    year_count = x.shape[0]\n    plt_format = ({\"cross\": \"X\", \"line\": \"-\", \"circle\": \"o--\"})[point_style]\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    for i, company in enumerate(companies):\n        series = np.arange(0, year_count, dtype=float)\n        series = series**2 * (i + 1)\n        series += np.random.rand(year_count) * noise\n        ax.plot(x, series, plt_format)\n    if show_legend:\n        plt.legend(companies)\n    return fig\n\n\ndemo = gr.Interface(\n    plot_forecast,\n    [\n        gr.Radio([2025, 2030, 2035, 2040], label=\"Project to:\"),\n        gr.CheckboxGroup([\"Google\", \"Microsoft\", \"Gradio\"], label=\"Company Selection\"),\n        gr.Slider(1, 100, label=\"Noise Level\"),\n        gr.Checkbox(label=\"Show Legend\"),\n        gr.Dropdown([\"cross\", \"line\", \"circle\"], label=\"Style\"),\n    ],\n    gr.Plot(label=\"forecast\"),\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]],guides:[{name:"plot-component-for-maps",category:"tabular-data-science-and-plots",pretty_category:"Tabular Data Science And Plots",guide_index:null,absolute_index:37,pretty_name:"Plot Component For Maps",content:"# How to Use the Plot Component for Maps\n\n\n\n## Introduction\n\nThis guide explains how you can use Gradio to plot geographical data on a map using the `gradio.Plot` component. The Gradio `Plot` component works with Matplotlib, Bokeh and Plotly. Plotly is what we will be working with in this guide. Plotly allows developers to easily create all sorts of maps with their geographical data. Take a look [here](https://plotly.com/python/maps/) for some examples.\n\n## Overview\n\nWe will be using the New York City Airbnb dataset, which is hosted on kaggle [here](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data). I've uploaded it to the Hugging Face Hub as a dataset [here](https://huggingface.co/datasets/gradio/NYC-Airbnb-Open-Data) for easier use and download. Using this data we will plot Airbnb locations on a map output and allow filtering based on price and location. Below is the demo that we will be building. ‚ö°Ô∏è\n\n\u003Cgradio-app space='gradio/map_airbnb'>\u003C/gradio-app>\n\n## Step 1 - Loading CSV data üíæ\n\nLet's start by loading the Airbnb NYC data from the Hugging Face Hub.\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"gradio/NYC-Airbnb-Open-Data\", split=\"train\")\ndf = dataset.to_pandas()\n\ndef filter_map(min_price, max_price, boroughs):\n    new_df = df[(df['neighbourhood_group'].isin(boroughs)) &\n            (df['price'] > min_price) & (df['price'] \u003C max_price)]\n    names = new_df[\"name\"].tolist()\n    prices = new_df[\"price\"].tolist()\n    text_list = [(names[i], prices[i]) for i in range(0, len(names))]\n```\n\nIn the code above, we first load the csv data into a pandas dataframe. Let's begin by defining a function that we will use as the prediction function for the gradio app. This function will accept the minimum price and maximum price range as well as the list of boroughs to filter the resulting map. We can use the passed in values (`min_price`, `max_price`, and list of `boroughs`) to filter the dataframe and create `new_df`. Next we will create `text_list` of the names and prices of each Airbnb to use as labels on the map.\n\n## Step 2 - Map Figure üåê\n\nPlotly makes it easy to work with maps. Let's take a look below how we can create a map figure.\n\n```python\nimport plotly.graph_objects as go\n\nfig = go.Figure(go.Scattermapbox(\n            customdata=text_list,\n            lat=new_df['latitude'].tolist(),\n            lon=new_df['longitude'].tolist(),\n            mode='markers',\n            marker=go.scattermapbox.Marker(\n                size=6\n            ),\n            hoverinfo=\"text\",\n            hovertemplate='\u003Cb>Name\u003C/b>: %{customdata[0]}\u003Cbr>\u003Cb>Price\u003C/b>: $%{customdata[1]}'\n        ))\n\nfig.update_layout(\n    mapbox_style=\"open-street-map\",\n    hovermode='closest',\n    mapbox=dict(\n        bearing=0,\n        center=go.layout.mapbox.Center(\n            lat=40.67,\n            lon=-73.90\n        ),\n        pitch=0,\n        zoom=9\n    ),\n)\n```\n\nAbove, we create a scatter plot on mapbox by passing it our list of latitudes and longitudes to plot markers. We also pass in our custom data of names and prices for additional info to appear on every marker we hover over. Next we use `update_layout` to specify other map settings such as zoom, and centering.\n\nMore info [here](https://plotly.com/python/scattermapbox/) on scatter plots using Mapbox and Plotly.\n\n## Step 3 - Gradio App ‚ö°Ô∏è\n\nWe will use two `gr.Number` components and a `gr.CheckboxGroup` to allow users of our app to specify price ranges and borough locations. We will then use the `gr.Plot` component as an output for our Plotly + Mapbox map we created earlier.\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Column():\n        with gr.Row():\n            min_price = gr.Number(value=250, label=\"Minimum Price\")\n            max_price = gr.Number(value=1000, label=\"Maximum Price\")\n        boroughs = gr.CheckboxGroup(choices=[\"Queens\", \"Brooklyn\", \"Manhattan\", \"Bronx\", \"Staten Island\"], value=[\"Queens\", \"Brooklyn\"], label=\"Select Boroughs:\")\n        btn = gr.Button(value=\"Update Filter\")\n        map = gr.Plot()\n    demo.load(filter_map, [min_price, max_price, boroughs], map)\n    btn.click(filter_map, [min_price, max_price, boroughs], map)\n```\n\nWe layout these components using the `gr.Column` and `gr.Row` and we'll also add event triggers for when the demo first loads and when our \"Update Filter\" button is clicked in order to trigger the map to update with our new filters.\n\nThis is what the full demo code looks like:\n\n```python\nimport gradio as gr\nimport plotly.graph_objects as go\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"gradio/NYC-Airbnb-Open-Data\", split=\"train\")\ndf = dataset.to_pandas()\n\ndef filter_map(min_price, max_price, boroughs):\n\n    filtered_df = df[(df['neighbourhood_group'].isin(boroughs)) & \n          (df['price'] > min_price) & (df['price'] \u003C max_price)]\n    names = filtered_df[\"name\"].tolist()\n    prices = filtered_df[\"price\"].tolist()\n    text_list = [(names[i], prices[i]) for i in range(0, len(names))]\n    fig = go.Figure(go.Scattermapbox(\n            customdata=text_list,\n            lat=filtered_df['latitude'].tolist(),\n            lon=filtered_df['longitude'].tolist(),\n            mode='markers',\n            marker=go.scattermapbox.Marker(\n                size=6\n            ),\n            hoverinfo=\"text\",\n            hovertemplate='\u003Cb>Name\u003C/b>: %{customdata[0]}\u003Cbr>\u003Cb>Price\u003C/b>: $%{customdata[1]}'\n        ))\n\n    fig.update_layout(\n        mapbox_style=\"open-street-map\",\n        hovermode='closest',\n        mapbox=dict(\n            bearing=0,\n            center=go.layout.mapbox.Center(\n                lat=40.67,\n                lon=-73.90\n            ),\n            pitch=0,\n            zoom=9\n        ),\n    )\n\n    return fig\n\nwith gr.Blocks() as demo:\n    with gr.Column():\n        with gr.Row():\n            min_price = gr.Number(value=250, label=\"Minimum Price\")\n            max_price = gr.Number(value=1000, label=\"Maximum Price\")\n        boroughs = gr.CheckboxGroup(choices=[\"Queens\", \"Brooklyn\", \"Manhattan\", \"Bronx\", \"Staten Island\"], value=[\"Queens\", \"Brooklyn\"], label=\"Select Boroughs:\")\n        btn = gr.Button(value=\"Update Filter\")\n        map = gr.Plot()\n    demo.load(filter_map, [min_price, max_price, boroughs], map)\n    btn.click(filter_map, [min_price, max_price, boroughs], map)\n\ndemo.launch()\n```\n\n## Step 4 - Deployment ü§ó\n\nIf you run the code above, your app will start running locally.\nYou can even get a temporary shareable link by passing the `share=True` parameter to `launch`.\n\nBut what if you want to a permanent deployment solution?\nLet's deploy our Gradio app to the free HuggingFace Spaces platform.\n\nIf you haven't used Spaces before, follow the previous guide [here](/using_hugging_face_integrations).\n\n## Conclusion üéâ\n\nAnd you're all done! That's all the code you need to build a map demo.\n\nHere's a link to the demo [Map demo](https://huggingface.co/spaces/gradio/map_airbnb) and [complete code](https://huggingface.co/spaces/gradio/map_airbnb/blob/main/run.py) (on Hugging Face Spaces)\n",tags:["PLOTS","MAPS"],spaces:[],url:"/guides/plot-component-for-maps/",contributor:null}],parent:"gradio"},barplot:{class:null,name:"BarPlot",description:"Creates a bar plot component to display data from a pandas DataFrame (as output). As this component does not accept user input, it is rarely used as an input component. \u003Cbr>",tags:{demos:"bar_plot"},parameters:[{name:"value",annotation:"pd.DataFrame | Callable | None",doc:"The pandas dataframe containing the data to display in a scatter plot. If a callable is provided, the function will be called whenever the app loads to set the initial value of the plot.",default:"None"},{name:"x",annotation:"str | None",doc:"Column corresponding to the x axis.",default:"None"},{name:"y",annotation:"str | None",doc:"Column corresponding to the y axis.",default:"None"},{name:"color",annotation:"str | None",doc:"The column to determine the bar color. Must be categorical (discrete values).",default:"None"},{name:"vertical",annotation:"bool",doc:"If True, the bars will be displayed vertically. If False, the x and y axis will be switched, displaying the bars horizontally. Default is True.",default:"True"},{name:"group",annotation:"str | None",doc:"The column with which to split the overall plot into smaller subplots.",default:"None"},{name:"title",annotation:"str | None",doc:"The title to display on top of the chart.",default:"None"},{name:"tooltip",annotation:"list[str] | str | None",doc:"The column (or list of columns) to display on the tooltip when a user hovers over a bar.",default:"None"},{name:"x_title",annotation:"str | None",doc:"The title given to the x axis. By default, uses the value of the x parameter.",default:"None"},{name:"y_title",annotation:"str | None",doc:"The title given to the y axis. By default, uses the value of the y parameter.",default:"None"},{name:"x_label_angle",annotation:"float | None",doc:"The angle (in degrees) of the x axis labels. Positive values are clockwise, and negative values are counter-clockwise.",default:"None"},{name:"y_label_angle",annotation:"float | None",doc:"The angle (in degrees) of the y axis labels. Positive values are clockwise, and negative values are counter-clockwise.",default:"None"},{name:"color_legend_title",annotation:"str | None",doc:"The title given to the color legend. By default, uses the value of color parameter.",default:"None"},{name:"group_title",annotation:"str | None",doc:"The label displayed on top of the subplot columns (or rows if vertical=True). Use an empty string to omit.",default:"None"},{name:"color_legend_position",annotation:"Literal[('left', 'right', 'top', 'bottom', 'top-left', 'top-right', 'bottom-left', 'bottom-right', 'none')] | None",doc:"The position of the color legend. If the string value &#x27;none&#x27; is passed, this legend is omitted. For other valid position values see: https://vega.github.io/vega/docs/legends/#orientation.",default:"None"},{name:"height",annotation:"int | str | None",doc:"The height of the plot, specified in pixels if a number is passed, or in CSS units if a string is passed.",default:"None"},{name:"width",annotation:"int | str | None",doc:"The width of the plot, specified in pixels if a number is passed, or in CSS units if a string is passed.",default:"None"},{name:"y_lim",annotation:"list[int] | None",doc:"A tuple of list containing the limits for the y-axis, specified as [y_min, y_max].",default:"None"},{name:"caption",annotation:"str | None",doc:"The (optional) caption to display below the plot.",default:"None"},{name:"interactive",annotation:"bool | None",doc:"Whether users should be able to interact with the plot by panning or zooming with their mouse or trackpad.",default:"True"},{name:"label",annotation:"str | None",doc:"The (optional) label to display on the top left corner of the plot.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"Whether the label should be displayed.",default:"None"},{name:"container",annotation:"bool",doc:null,default:"True"},{name:"scale",annotation:"int | None",doc:null,default:"None"},{name:"min_width",annotation:"int",doc:null,default:"160"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"visible",annotation:"bool",doc:"Whether the plot should be visible.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"sort",annotation:"Literal[('x', 'y', '-x', '-y')] | None",doc:"Specifies the sorting axis as either &quot;x&quot;, &quot;y&quot;, &quot;-x&quot; or &quot;-y&quot;. If None, no sorting is applied.",default:"None"},{name:"show_actions_button",annotation:"bool",doc:"Whether to show the actions button on the top right corner of the plot.",default:"False"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the Plot changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.BarPlot"},{fn:null,name:"clear",description:"This listener is triggered when the user clears the Plot using the X button for the component.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.BarPlot"}],preprocess:{parameter_doc:[{name:"payload",annotation:"AltairPlotData",doc:"The data to display in a bar plot."}],return_doc:{annotation:"AltairPlotData",doc:"(Rarely used) passes the data displayed in the bar plot as an AltairPlotData dataclass, which includes the plot information as a JSON string, as well as the type of plot (in this case, \"bar\")."}},postprocess:{parameter_doc:[{name:"value",annotation:"pd.DataFrame | None",doc:"Expects a pandas DataFrame containing the data to display in the bar plot. The DataFrame should contain at least two columns, one for the x-axis (corresponding to this component's `x` argument) and one for the y-axis (corresponding to `y`)."}],return_doc:{annotation:"AltairPlotData | None",doc:"The data to display in a bar plot, in the form of an AltairPlotData dataclass, which includes the plot information as a JSON string, as well as the type of plot (in this case, \"bar\")."}},string_shortcuts:[["BarPlot","barplot","Uses default values"]],demos:[["bar_plot","import gradio as gr\nimport pandas as pd\nimport random\n\nsimple = pd.DataFrame(\n    {\n        \"a\": [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\"],\n        \"b\": [28, 55, 43, 91, 81, 53, 19, 87, 52],\n    }\n)\n\nfake_barley = pd.DataFrame(\n    {\n        \"site\": [\n            random.choice(\n                [\n                    \"University Farm\",\n                    \"Waseca\",\n                    \"Morris\",\n                    \"Crookston\",\n                    \"Grand Rapids\",\n                    \"Duluth\",\n                ]\n            )\n            for _ in range(120)\n        ],\n        \"yield\": [random.randint(25, 75) for _ in range(120)],\n        \"variety\": [\n            random.choice(\n                [\n                    \"Manchuria\",\n                    \"Wisconsin No. 38\",\n                    \"Glabron\",\n                    \"No. 457\",\n                    \"No. 462\",\n                    \"No. 475\",\n                ]\n            )\n            for _ in range(120)\n        ],\n        \"year\": [\n            random.choice(\n                [\n                    \"1931\",\n                    \"1932\",\n                ]\n            )\n            for _ in range(120)\n        ],\n    }\n)\n\n\ndef bar_plot_fn(display):\n    if display == \"simple\":\n        return gr.BarPlot(\n            simple,\n            x=\"a\",\n            y=\"b\",\n            title=\"Simple Bar Plot with made up data\",\n            tooltip=[\"a\", \"b\"],\n            y_lim=[20, 100],\n        )\n    elif display == \"stacked\":\n        return gr.BarPlot(\n            fake_barley,\n            x=\"variety\",\n            y=\"yield\",\n            color=\"site\",\n            title=\"Barley Yield Data\",\n            tooltip=[\"variety\", \"site\"],\n        )\n    elif display == \"grouped\":\n        return gr.BarPlot(\n            fake_barley.astype({\"year\": str}),\n            x=\"year\",\n            y=\"yield\",\n            color=\"year\",\n            group=\"site\",\n            title=\"Barley Yield by Year and Site\",\n            group_title=\"\",\n            tooltip=[\"yield\", \"site\", \"year\"],\n        )\n    elif display == \"simple-horizontal\":\n        return gr.BarPlot(\n            simple,\n            x=\"a\",\n            y=\"b\",\n            x_title=\"Variable A\",\n            y_title=\"Variable B\",\n            title=\"Simple Bar Plot with made up data\",\n            tooltip=[\"a\", \"b\"],\n            vertical=False,\n            y_lim=[20, 100],\n        )\n    elif display == \"stacked-horizontal\":\n        return gr.BarPlot(\n            fake_barley,\n            x=\"variety\",\n            y=\"yield\",\n            color=\"site\",\n            title=\"Barley Yield Data\",\n            vertical=False,\n            tooltip=[\"variety\", \"site\"],\n        )\n    elif display == \"grouped-horizontal\":\n        return gr.BarPlot(\n            fake_barley.astype({\"year\": str}),\n            x=\"year\",\n            y=\"yield\",\n            color=\"year\",\n            group=\"site\",\n            title=\"Barley Yield by Year and Site\",\n            group_title=\"\",\n            tooltip=[\"yield\", \"site\", \"year\"],\n            vertical=False,\n        )\n\n\nwith gr.Blocks() as bar_plot:\n    with gr.Row():\n        with gr.Column():\n            display = gr.Dropdown(\n                choices=[\n                    \"simple\",\n                    \"stacked\",\n                    \"grouped\",\n                    \"simple-horizontal\",\n                    \"stacked-horizontal\",\n                    \"grouped-horizontal\",\n                ],\n                value=\"simple\",\n                label=\"Type of Bar Plot\",\n            )\n        with gr.Column():\n            plot = gr.BarPlot()\n    display.change(bar_plot_fn, inputs=display, outputs=plot)\n    bar_plot.load(fn=bar_plot_fn, inputs=display, outputs=plot)\n\nbar_plot.launch()\n"]],parent:"gradio"},button:{class:null,name:"Button",description:"Creates a button that can be assigned arbitrary .click() events. The value (label) of the button can be used as an input to the function (rarely used) or set via the output of a function.",tags:{},parameters:[{name:"value",annotation:"str | Callable",doc:"Default text for the button to display. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"\"Run\""},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"variant",annotation:"Literal[('primary', 'secondary', 'stop')]",doc:"&#x27;primary&#x27; for main call-to-action, &#x27;secondary&#x27; for a more subdued style, &#x27;stop&#x27; for a stop button.",default:"\"secondary\""},{name:"size",annotation:"Literal[('sm', 'lg')] | None",doc:"Size of the button. Can be &quot;sm&quot; or &quot;lg&quot;.",default:"None"},{name:"icon",annotation:"str | None",doc:"URL or path to the icon file to display within the button. If None, no icon will be displayed.",default:"None"},{name:"link",annotation:"str | None",doc:"URL to open when the button is clicked. If None, no link will be used.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"interactive",annotation:"bool",doc:"If False, the Button will be in a disabled state.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int | None",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"click",description:"Triggered when the Button is clicked.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Button"}],preprocess:{parameter_doc:[{name:"payload",annotation:"str | None",doc:"string corresponding to the button label"}],return_doc:{annotation:"str | None",doc:"(Rarely used) the `str` corresponding to the button label when the button is clicked"}},postprocess:{parameter_doc:[{name:"value",annotation:"str | None",doc:"string corresponding to the button label"}],return_doc:{annotation:"str | None",doc:"Expects a `str` value that is set as the button label"}},string_shortcuts:[["Button","button","Uses default values"],["ClearButton","clearbutton","Uses default values"],["DuplicateButton","duplicatebutton","Uses default values"],["LoginButton","loginbutton","Uses default values"],["LogoutButton","logoutbutton","Uses default values"]],parent:"gradio"},chatbot:{class:null,name:"Chatbot",description:"Creates a chatbot that displays user-submitted messages and responses. Supports a subset of Markdown including bold, italics, code, tables. Also supports audio/video/image files, which are displayed in the Chatbot, and other kinds of files which are displayed as links. This component is usually used as an output component. \u003Cbr>",tags:{demos:"chatbot_simple, chatbot_multimodal",guides:"creating-a-chatbot"},parameters:[{name:"value",annotation:"list[list[str | tuple[str] | tuple[str | Path, str] | None]] | Callable | None",doc:"Default value to show in chatbot. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"height",annotation:"int | str | None",doc:"The height of the component, specified in pixels if a number is passed, or in CSS units if a string is passed.",default:"None"},{name:"latex_delimiters",annotation:"list[dict[str, str | bool]] | None",doc:"A list of dicts of the form {&quot;left&quot;: open delimiter (str), &quot;right&quot;: close delimiter (str), &quot;display&quot;: whether to display in newline (bool)} that will be used to render LaTeX expressions. If not provided, `latex_delimiters` is set to `[{ &quot;left&quot;: &quot;$$&quot;, &quot;right&quot;: &quot;$$&quot;, &quot;display&quot;: True }]`, so only expressions enclosed in $$ delimiters will be rendered as LaTeX, and in a new line. Pass in an empty list to disable LaTeX rendering. For more information, see the [KaTeX documentation](https://katex.org/docs/autorender.html).",default:"None"},{name:"rtl",annotation:"bool",doc:"If True, sets the direction of the rendered text to right-to-left. Default is False, which renders text left-to-right.",default:"False"},{name:"show_share_button",annotation:"bool | None",doc:"If True, will show a share icon in the corner of the component that allows user to share outputs to Hugging Face Spaces Discussions. If False, icon does not appear. If set to None (default behavior), then the icon appears if this Gradio app is launched on Spaces, but not otherwise.",default:"None"},{name:"show_copy_button",annotation:"bool",doc:"If True, will show a copy button for each chatbot message.",default:"False"},{name:"avatar_images",annotation:"tuple[str | Path | None, str | Path | None] | None",doc:"Tuple of two avatar image paths or URLs for user and bot (in that order). Pass None for either the user or bot image to skip. Must be within the working directory of the Gradio app or an external URL.",default:"None"},{name:"sanitize_html",annotation:"bool",doc:"If False, will disable HTML sanitization for chatbot messages. This is not recommended, as it can lead to security vulnerabilities.",default:"True"},{name:"render_markdown",annotation:"bool",doc:"If False, will disable Markdown rendering for chatbot messages.",default:"True"},{name:"bubble_full_width",annotation:"bool",doc:"If False, the chat bubble will fit to the content of the message. If True (default), the chat bubble will be the full width of the component.",default:"True"},{name:"line_breaks",annotation:"bool",doc:"If True (default), will enable Github-flavored Markdown line breaks in chatbot messages. If False, single new lines will be ignored. Only applies if `render_markdown` is True.",default:"True"},{name:"likeable",annotation:"bool",doc:"Whether the chat messages display a like or dislike button. Set automatically by the .like method but has to be present in the signature for it to show up in the config.",default:"False"},{name:"layout",annotation:"Literal[('panel', 'bubble')] | None",doc:"If &quot;panel&quot;, will display the chatbot in a llm style layout. If &quot;bubble&quot;, will display the chatbot with message bubbles, with the user and bot messages on alterating sides. Will default to &quot;bubble&quot;.",default:"None"},{name:"placeholder",annotation:"str | None",doc:"a placeholder message to display in the chatbot when it is empty. Centered vertically and horizontally in the Chatbot. Supports Markdown and HTML. If None, no placeholder is displayed.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the Chatbot changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Chatbot"},{fn:null,name:"select",description:"Event listener for when the user selects or deselects the Chatbot. Uses event data gradio.SelectData to carry `value` referring to the label of the Chatbot, and `selected` to refer to state of the Chatbot. See EventData documentation on how to use this event data",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Chatbot"},{fn:null,name:"like",description:"This listener is triggered when the user likes/dislikes from within the Chatbot. This event has EventData of type gradio.LikeData that carries information, accessible through LikeData.index and LikeData.value. See EventData documentation on how to use this event data.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Chatbot"}],preprocess:{parameter_doc:[{name:"payload",annotation:"ChatbotData | None",doc:"data as a ChatbotData object"}],return_doc:{annotation:"list[list[str | tuple[str] | tuple[str, str] | None]] | None",doc:"Passes the messages in the chatbot as a `list[list[str | None | tuple]]`, i.e. a list of lists. The inner list has 2 elements: the user message and the response message. Each message can be (1) a string in valid Markdown, (2) a tuple if there are displayed files: (a filepath or URL to a file, [optional string alt text]), or (3) None, if there is no message displayed."}},postprocess:{parameter_doc:[{name:"value",annotation:"list[list[str | tuple[str] | tuple[str, str] | None] | tuple] | None",doc:"expects a `list[list[str | None | tuple]]`, i.e. a list of lists. The inner list should have 2 elements: the user message and the response message. The individual messages can be (1) strings in valid Markdown, (2) tuples if sending files: (a filepath or URL to a file, [optional string alt text]) -- if the file is image/video/audio, it is displayed in the Chatbot, or (3) None, in which case the message is not displayed."}],return_doc:{annotation:"ChatbotData",doc:"an object of type ChatbotData"}},string_shortcuts:[["Chatbot","chatbot","Uses default values"]],demos:[["chatbot_simple","import gradio as gr\nimport random\nimport time\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    msg = gr.Textbox()\n    clear = gr.ClearButton([msg, chatbot])\n\n    def respond(message, chat_history):\n        bot_message = random.choice([\"How are you?\", \"I love you\", \"I'm very hungry\"])\n        chat_history.append((message, bot_message))\n        time.sleep(2)\n        return \"\", chat_history\n\n    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n\nif __name__ == \"__main__\":\n    demo.launch()\n"],["chatbot_multimodal","import gradio as gr\nimport os\nimport time\n\n# Chatbot demo with multimodal input (text, markdown, LaTeX, code blocks, image, audio, & video). Plus shows support for streaming text.\n\n\ndef print_like_dislike(x: gr.LikeData):\n    print(x.index, x.value, x.liked)\n\ndef add_message(history, message):\n    for x in message[\"files\"]:\n        history.append(((x,), None))\n    if message[\"text\"] is not None:\n        history.append((message[\"text\"], None))\n    return history, gr.MultimodalTextbox(value=None, interactive=False)\n\ndef bot(history):\n    response = \"**That's cool!**\"\n    history[-1][1] = \"\"\n    for character in response:\n        history[-1][1] += character\n        time.sleep(0.05)\n        yield history\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot(\n        [],\n        elem_id=\"chatbot\",\n        bubble_full_width=False\n    )\n\n    chat_input = gr.MultimodalTextbox(interactive=True, file_types=[\"image\"], placeholder=\"Enter message or upload file...\", show_label=False)\n\n    chat_msg = chat_input.submit(add_message, [chatbot, chat_input], [chatbot, chat_input])\n    bot_msg = chat_msg.then(bot, chatbot, chatbot, api_name=\"bot_response\")\n    bot_msg.then(lambda: gr.MultimodalTextbox(interactive=True), None, [chat_input])\n\n    chatbot.like(print_like_dislike, None, None)\n\ndemo.queue()\nif __name__ == \"__main__\":\n    demo.launch()\n"]],guides:[],parent:"gradio"},checkbox:{class:null,name:"Checkbox",description:"Creates a checkbox that can be set to `True` or `False`. Can be used as an input to pass a boolean value to a function or as an output to display a boolean value. \u003Cbr>",tags:{demos:"sentence_builder, hello_world_3"},parameters:[{name:"value",annotation:"bool | Callable",doc:"if True, checked by default. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"False"},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"info",annotation:"str | None",doc:"additional component description.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"interactive",annotation:"bool | None",doc:"if True, this checkbox can be checked; if False, checking will be disabled. If not provided, this is inferred based on whether the component is used as an input or output.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the Checkbox changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Checkbox"},{fn:null,name:"input",description:"This listener is triggered when the user changes the value of the Checkbox.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Checkbox"},{fn:null,name:"select",description:"Event listener for when the user selects or deselects the Checkbox. Uses event data gradio.SelectData to carry `value` referring to the label of the Checkbox, and `selected` to refer to state of the Checkbox. See EventData documentation on how to use this event data",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Checkbox"}],preprocess:{parameter_doc:[{name:"payload",annotation:"bool | None",doc:"the status of the checkbox"}],return_doc:{annotation:"bool | None",doc:"Passes the status of the checkbox as a `bool`."}},postprocess:{parameter_doc:[{name:"value",annotation:"bool | None",doc:"Expects a `bool` value that is set as the status of the checkbox"}],return_doc:{annotation:"bool | None",doc:"The same `bool` value that is set as the status of the checkbox"}},string_shortcuts:[["Checkbox","checkbox","Uses default values"]],demos:[["sentence_builder","import gradio as gr\n\n\ndef sentence_builder(quantity, animal, countries, place, activity_list, morning):\n    return f\"\"\"The {quantity} {animal}s from {\" and \".join(countries)} went to the {place} where they {\" and \".join(activity_list)} until the {\"morning\" if morning else \"night\"}\"\"\"\n\n\ndemo = gr.Interface(\n    sentence_builder,\n    [\n        gr.Slider(2, 20, value=4, label=\"Count\", info=\"Choose between 2 and 20\"),\n        gr.Dropdown(\n            [\"cat\", \"dog\", \"bird\"], label=\"Animal\", info=\"Will add more animals later!\"\n        ),\n        gr.CheckboxGroup([\"USA\", \"Japan\", \"Pakistan\"], label=\"Countries\", info=\"Where are they from?\"),\n        gr.Radio([\"park\", \"zoo\", \"road\"], label=\"Location\", info=\"Where did they go?\"),\n        gr.Dropdown(\n            [\"ran\", \"swam\", \"ate\", \"slept\"], value=[\"swam\", \"slept\"], multiselect=True, label=\"Activity\", info=\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed auctor, nisl eget ultricies aliquam, nunc nisl aliquet nunc, eget aliquam nisl nunc vel nisl.\"\n        ),\n        gr.Checkbox(label=\"Morning\", info=\"Did they do it in the morning?\"),\n    ],\n    \"text\",\n    examples=[\n        [2, \"cat\", [\"Japan\", \"Pakistan\"], \"park\", [\"ate\", \"swam\"], True],\n        [4, \"dog\", [\"Japan\"], \"zoo\", [\"ate\", \"swam\"], False],\n        [10, \"bird\", [\"USA\", \"Pakistan\"], \"road\", [\"ran\"], False],\n        [8, \"cat\", [\"Pakistan\"], \"zoo\", [\"ate\"], True],\n    ]\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"],["hello_world_3","import gradio as gr\n\ndef greet(name, is_morning, temperature):\n    salutation = \"Good morning\" if is_morning else \"Good evening\"\n    greeting = f\"{salutation} {name}. It is {temperature} degrees today\"\n    celsius = (temperature - 32) * 5 / 9\n    return greeting, round(celsius, 2)\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=[\"text\", \"checkbox\", gr.Slider(0, 100)],\n    outputs=[\"text\", \"number\"],\n)\nif __name__ == \"__main__\":\n    demo.launch()\n"]],parent:"gradio"},checkboxgroup:{class:null,name:"CheckboxGroup",description:"Creates a set of checkboxes. Can be used as an input to pass a set of values to a function or as an output to display values, a subset of which are selected.",tags:{demos:"sentence_builder"},parameters:[{name:"choices",annotation:"list[str | int | float | tuple[str, str | int | float]] | None",doc:"A list of string or numeric options to select from. An option can also be a tuple of the form (name, value), where name is the displayed name of the checkbox button and value is the value to be passed to the function, or returned by the function.",default:"None"},{name:"value",annotation:"list[str | float | int] | str | float | int | Callable | None",doc:"Default selected list of options. If a single choice is selected, it can be passed in as a string or numeric type. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"type",annotation:"Literal[('value', 'index')]",doc:"Type of value to be returned by component. &quot;value&quot; returns the list of strings of the choices selected, &quot;index&quot; returns the list of indices of the choices selected.",default:"\"value\""},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"info",annotation:"str | None",doc:"Additional component description.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise.sed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"If True, will display label.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"Relative width compared to adjacent Components in a Row. For example, if Component A has scale=2, and Component B has scale=1, A will be twice as wide as B. Should be an integer.",default:"None"},{name:"min_width",annotation:"int",doc:"Minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"interactive",annotation:"bool | None",doc:"If True, choices in this checkbox group will be checkable; if False, checking will be disabled. If not provided, this is inferred based on whether the component is used as an input or output.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the CheckboxGroup changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.CheckboxGroup"},{fn:null,name:"input",description:"This listener is triggered when the user changes the value of the CheckboxGroup.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.CheckboxGroup"},{fn:null,name:"select",description:"Event listener for when the user selects or deselects the CheckboxGroup. Uses event data gradio.SelectData to carry `value` referring to the label of the CheckboxGroup, and `selected` to refer to state of the CheckboxGroup. See EventData documentation on how to use this event data",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.CheckboxGroup"}],preprocess:{parameter_doc:[{name:"payload",annotation:"list[str | int | float]",doc:"the list of checked checkboxes' values"}],return_doc:{annotation:"list[str | int | float] | list[int | None]",doc:"Passes the list of checked checkboxes as a `list[str | int | float]` or their indices as a `list[int]` into the function, depending on `type`."}},postprocess:{parameter_doc:[{name:"value",annotation:"list[str | int | float] | str | int | float | None",doc:"Expects a `list[str | int | float]` of values or a single `str | int | float` value, the checkboxes with these values are checked."}],return_doc:{annotation:"list[str | int | float]",doc:"the list of checked checkboxes' values"}},string_shortcuts:[["CheckboxGroup","checkboxgroup","Uses default values"]],demos:[["sentence_builder","import gradio as gr\n\n\ndef sentence_builder(quantity, animal, countries, place, activity_list, morning):\n    return f\"\"\"The {quantity} {animal}s from {\" and \".join(countries)} went to the {place} where they {\" and \".join(activity_list)} until the {\"morning\" if morning else \"night\"}\"\"\"\n\n\ndemo = gr.Interface(\n    sentence_builder,\n    [\n        gr.Slider(2, 20, value=4, label=\"Count\", info=\"Choose between 2 and 20\"),\n        gr.Dropdown(\n            [\"cat\", \"dog\", \"bird\"], label=\"Animal\", info=\"Will add more animals later!\"\n        ),\n        gr.CheckboxGroup([\"USA\", \"Japan\", \"Pakistan\"], label=\"Countries\", info=\"Where are they from?\"),\n        gr.Radio([\"park\", \"zoo\", \"road\"], label=\"Location\", info=\"Where did they go?\"),\n        gr.Dropdown(\n            [\"ran\", \"swam\", \"ate\", \"slept\"], value=[\"swam\", \"slept\"], multiselect=True, label=\"Activity\", info=\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed auctor, nisl eget ultricies aliquam, nunc nisl aliquet nunc, eget aliquam nisl nunc vel nisl.\"\n        ),\n        gr.Checkbox(label=\"Morning\", info=\"Did they do it in the morning?\"),\n    ],\n    \"text\",\n    examples=[\n        [2, \"cat\", [\"Japan\", \"Pakistan\"], \"park\", [\"ate\", \"swam\"], True],\n        [4, \"dog\", [\"Japan\"], \"zoo\", [\"ate\", \"swam\"], False],\n        [10, \"bird\", [\"USA\", \"Pakistan\"], \"road\", [\"ran\"], False],\n        [8, \"cat\", [\"Pakistan\"], \"zoo\", [\"ate\"], True],\n    ]\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]],parent:"gradio"},clearbutton:{class:null,name:"ClearButton",description:"Button that clears the value of a component or a list of components when clicked. It is instantiated with the list of components to clear.",tags:{preprocessing:"passes the button value as a {str} into the function",postprocessing:"expects a {str} to be returned from a function, which is set as the label of the button"},parameters:[{name:"components",annotation:"None | list[Component] | Component",doc:null,default:"None"},{name:"value",annotation:"str",doc:"Default text for the button to display. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"\"Clear\""},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"variant",annotation:"Literal[('primary', 'secondary', 'stop')]",doc:"&#x27;primary&#x27; for main call-to-action, &#x27;secondary&#x27; for a more subdued style, &#x27;stop&#x27; for a stop button.",default:"\"secondary\""},{name:"size",annotation:"Literal[('sm', 'lg')] | None",doc:"Size of the button. Can be &quot;sm&quot; or &quot;lg&quot;.",default:"None"},{name:"icon",annotation:"str | None",doc:"URL or path to the icon file to display within the button. If None, no icon will be displayed.",default:"None"},{name:"link",annotation:"str | None",doc:"URL to open when the button is clicked. If None, no link will be used.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"interactive",annotation:"bool",doc:"If False, the Button will be in a disabled state.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int | None",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"None"},{name:"api_name",annotation:"str | None | Literal['False']",doc:null,default:"None"},{name:"show_api",annotation:"bool",doc:null,default:"False"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"add",description:"Adds a component or list of components to the list of components that will be cleared when the button is clicked.",tags:{},parameters:[{name:"components",annotation:"None | Component | list[Component]",doc:null}],returns:{},example:null,override_signature:null,parent:"gradio.ClearButton"},{fn:null,name:"click",description:"Triggered when the Button is clicked.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.ClearButton"}],preprocess:{parameter_doc:[{name:"payload",annotation:"str | None",doc:"string corresponding to the button label"}],return_doc:{annotation:"str | None",doc:"(Rarely used) the `str` corresponding to the button label when the button is clicked"}},postprocess:{parameter_doc:[{name:"value",annotation:"str | None",doc:"string corresponding to the button label"}],return_doc:{annotation:"str | None",doc:"Expects a `str` value that is set as the button label"}},string_shortcuts:[["ClearButton","clearbutton","Uses default values"]],parent:"gradio"},code:{class:null,name:"Code",description:"Creates a code editor for viewing code (as an output component), or for entering and editing code (as an input component).",tags:{},parameters:[{name:"value",annotation:"str | Callable | tuple[str] | None",doc:"Default value to show in the code editor. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"language",annotation:"Literal[('python', 'markdown', 'json', 'html', 'css', 'javascript', 'typescript', 'yaml', 'dockerfile', 'shell', 'r', 'sql', 'sql-msSQL', 'sql-mySQL', 'sql-mariaDB', 'sql-sqlite', 'sql-cassandra', 'sql-plSQL', 'sql-hive', 'sql-pgSQL', 'sql-gql', 'sql-gpSQL', 'sql-sparkSQL', 'sql-esper')] | None",doc:"The language to display the code as. Supported languages listed in `gr.Code.languages`.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"lines",annotation:"int",doc:null,default:"5"},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"interactive",annotation:"bool | None",doc:"Whether user should be able to enter code or only view it.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"languages",description:"[&#x27;python&#x27;, &#x27;markdown&#x27;, &#x27;json&#x27;, &#x27;html&#x27;, &#x27;css&#x27;, &#x27;javascript&#x27;, &#x27;typescript&#x27;, &#x27;yaml&#x27;, &#x27;dockerfile&#x27;, &#x27;shell&#x27;, &#x27;r&#x27;, &#x27;sql&#x27;, &#x27;sql-msSQL&#x27;, &#x27;sql-mySQL&#x27;, &#x27;sql-mariaDB&#x27;, &#x27;sql-sqlite&#x27;, &#x27;sql-cassandra&#x27;, &#x27;sql-plSQL&#x27;, &#x27;sql-hive&#x27;, &#x27;sql-pgSQL&#x27;, &#x27;sql-gql&#x27;, &#x27;sql-gpSQL&#x27;, &#x27;sql-sparkSQL&#x27;, &#x27;sql-esper&#x27;, None]",tags:{},parameters:[],returns:{},example:"",override_signature:"gr.Code.languages",parent:"gradio.Code"},{fn:null,name:"change",description:"Triggered when the value of the Code changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Code"},{fn:null,name:"input",description:"This listener is triggered when the user changes the value of the Code.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Code"},{fn:null,name:"focus",description:"This listener is triggered when the Code is focused.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Code"},{fn:null,name:"blur",description:"This listener is triggered when the Code is unfocused/blurred.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Code"}],preprocess:{parameter_doc:[{name:"payload",annotation:"str | None",doc:"string corresponding to the code"}],return_doc:{annotation:"str | None",doc:"Passes the code entered as a `str`."}},postprocess:{parameter_doc:[{name:"value",annotation:"tuple[str] | str | None",doc:"Expects a `str` of code or a single-element `tuple`: (filepath,) with the `str` path to a file containing the code."}],return_doc:{annotation:"None | str",doc:"Returns the code as a `str`."}},string_shortcuts:[["Code","code","Uses default values"]],parent:"gradio"},colorpicker:{class:null,name:"ColorPicker",description:"Creates a color picker for user to select a color as string input. Can be used as an input to pass a color value to a function or as an output to display a color value.",tags:{demos:"color_picker"},parameters:[{name:"value",annotation:"str | Callable | None",doc:"default text to provide in color picker. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"info",annotation:"str | None",doc:"additional component description.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise.sed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"interactive",annotation:"bool | None",doc:"if True, will be rendered as an editable color picker; if False, editing will be disabled. If not provided, this is inferred based on whether the component is used as an input or output.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the ColorPicker changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.ColorPicker"},{fn:null,name:"input",description:"This listener is triggered when the user changes the value of the ColorPicker.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.ColorPicker"},{fn:null,name:"submit",description:"This listener is triggered when the user presses the Enter key while the ColorPicker is focused.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.ColorPicker"},{fn:null,name:"focus",description:"This listener is triggered when the ColorPicker is focused.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.ColorPicker"},{fn:null,name:"blur",description:"This listener is triggered when the ColorPicker is unfocused/blurred.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.ColorPicker"}],preprocess:{parameter_doc:[{name:"payload",annotation:"str | None",doc:"Color as hex string"}],return_doc:{annotation:"str | None",doc:"Passes selected color value as a hex `str` into the function."}},postprocess:{parameter_doc:[{name:"value",annotation:"str | None",doc:"Expects a hex `str` returned from function and sets color picker value to it."}],return_doc:{annotation:"str | None",doc:"A `str` value that is set as the color picker value."}},string_shortcuts:[["ColorPicker","colorpicker","Uses default values"]],demos:[["color_picker","import gradio as gr\nimport numpy as np\nimport os\nfrom PIL import Image, ImageColor\n\n\ndef change_color(icon, color):\n\n    \"\"\"\n    Function that given an icon in .png format changes its color\n    Args:\n        icon: Icon whose color needs to be changed.\n        color: Chosen color with which to edit the input icon.\n    Returns:\n        edited_image: Edited icon.\n    \"\"\"\n    img = icon.convert(\"LA\")\n    img = img.convert(\"RGBA\")\n    image_np = np.array(icon)\n    _, _, _, alpha = image_np.T\n    mask = alpha > 0\n    image_np[..., :-1][mask.T] = ImageColor.getcolor(color, \"RGB\")\n    edited_image = Image.fromarray(image_np)\n    return edited_image\n\n\ninputs = [\n    gr.Image(label=\"icon\", type=\"pil\", image_mode=\"RGBA\"),\n    gr.ColorPicker(label=\"color\"),\n]\noutputs = gr.Image(label=\"colored icon\")\n\ndemo = gr.Interface(\n    fn=change_color,\n    inputs=inputs,\n    outputs=outputs\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]],parent:"gradio"},dataframe:{class:null,name:"Dataframe",description:"This component displays a table of value spreadsheet-like component. Can be used to display data as an output component, or as an input to collect data from the user.",tags:{demos:"filter_records, matrix_transpose, tax_calculator, sort_records"},parameters:[{name:"value",annotation:"pd.DataFrame | Styler | np.ndarray | pl.DataFrame | list | list[list] | dict | str | Callable | None",doc:"Default value to display in the DataFrame. If a Styler is provided, it will be used to set the displayed value in the DataFrame (e.g. to set precision of numbers) if the `interactive` is False. If a Callable function is provided, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"headers",annotation:"list[str] | None",doc:"List of str header names. If None, no headers are shown.",default:"None"},{name:"row_count",annotation:"int | tuple[int, str]",doc:"Limit number of rows for input and decide whether user can create new rows. The first element of the tuple is an `int`, the row count; the second should be &#x27;fixed&#x27; or &#x27;dynamic&#x27;, the new row behaviour. If an `int` is passed the rows default to &#x27;dynamic&#x27;",default:"(1, 'dynamic')"},{name:"col_count",annotation:"int | tuple[int, str] | None",doc:"Limit number of columns for input and decide whether user can create new columns. The first element of the tuple is an `int`, the number of columns; the second should be &#x27;fixed&#x27; or &#x27;dynamic&#x27;, the new column behaviour. If an `int` is passed the columns default to &#x27;dynamic&#x27;",default:"None"},{name:"datatype",annotation:"str | list[str]",doc:"Datatype of values in sheet. Can be provided per column as a list of strings, or for the entire sheet as a single string. Valid datatypes are &quot;str&quot;, &quot;number&quot;, &quot;bool&quot;, &quot;date&quot;, and &quot;markdown&quot;.",default:"\"str\""},{name:"type",annotation:"Literal[('pandas', 'numpy', 'array', 'polars')]",doc:"Type of value to be returned by component. &quot;pandas&quot; for pandas dataframe, &quot;numpy&quot; for numpy array, &quot;polars&quot; for polars dataframe, or &quot;array&quot; for a Python list of lists.",default:"\"pandas\""},{name:"latex_delimiters",annotation:"list[dict[str, str | bool]] | None",doc:"A list of dicts of the form {&quot;left&quot;: open delimiter (str), &quot;right&quot;: close delimiter (str), &quot;display&quot;: whether to display in newline (bool)} that will be used to render LaTeX expressions. If not provided, `latex_delimiters` is set to `[{ &quot;left&quot;: &quot;$$&quot;, &quot;right&quot;: &quot;$$&quot;, &quot;display&quot;: True }]`, so only expressions enclosed in $$ delimiters will be rendered as LaTeX, and in a new line. Pass in an empty list to disable LaTeX rendering. For more information, see the [KaTeX documentation](https://katex.org/docs/autorender.html). Only applies to columns whose datatype is &quot;markdown&quot;.",default:"None"},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"height",annotation:"int",doc:"The maximum height of the dataframe, specified in pixels if a number is passed, or in CSS units if a string is passed. If more rows are created than can fit in the height, a scrollbar will appear.",default:"500"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"interactive",annotation:"bool | None",doc:"if True, will allow users to edit the dataframe; if False, can only be used to display data. If not provided, this is inferred based on whether the component is used as an input or output.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"wrap",annotation:"bool",doc:"If True, the text in table cells will wrap when appropriate. If False and the `column_width` parameter is not set, the column widths will expand based on the cell contents and the table may need to be horizontally scrolled. If `column_width` is set, then any overflow text will be hidden.",default:"False"},{name:"line_breaks",annotation:"bool",doc:"If True (default), will enable Github-flavored Markdown line breaks in chatbot messages. If False, single new lines will be ignored. Only applies for columns of type &quot;markdown.&quot;",default:"True"},{name:"column_widths",annotation:"list[str | int] | None",doc:"An optional list representing the width of each column. The elements of the list should be in the format &quot;100px&quot; (ints are also accepted and converted to pixel values) or &quot;10%&quot;. If not provided, the column widths will be automatically determined based on the content of the cells. Setting this parameter will cause the browser to try to fit the table within the page width.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the Dataframe changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Dataframe"},{fn:null,name:"input",description:"This listener is triggered when the user changes the value of the Dataframe.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Dataframe"},{fn:null,name:"select",description:"Event listener for when the user selects or deselects the Dataframe. Uses event data gradio.SelectData to carry `value` referring to the label of the Dataframe, and `selected` to refer to state of the Dataframe. See EventData documentation on how to use this event data",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Dataframe"}],preprocess:{parameter_doc:[{name:"payload",annotation:"DataframeData",doc:"the uploaded spreadsheet data as an object with `headers` and `data` attributes"}],return_doc:{annotation:"pd.DataFrame | np.ndarray | pl.DataFrame | list[list]",doc:"Passes the uploaded spreadsheet data as a `pandas.DataFrame`, `numpy.array`, `polars.DataFrame`, or native 2D Python `list[list]` depending on `type`"}},postprocess:{parameter_doc:[{name:"value",annotation:"pd.DataFrame | Styler | np.ndarray | pl.DataFrame | list | list[list] | dict | str | None",doc:"Expects data any of these formats: `pandas.DataFrame`, `pandas.Styler`, `numpy.array`, `polars.DataFrame`, `list[list]`, `list`, or a `dict` with keys 'data' (and optionally 'headers'), or `str` path to a csv, which is rendered as the spreadsheet."}],return_doc:{annotation:"DataframeData",doc:"the uploaded spreadsheet data as an object with `headers` and `data` attributes"}},string_shortcuts:[["Dataframe","dataframe","Uses default values"],["Numpy","numpy","Uses type=\"numpy\""],["Matrix","matrix","Uses type=\"array\""],["List","list","Uses type=\"array\", col_count=1"]],demos:[["filter_records","import gradio as gr\n\n\ndef filter_records(records, gender):\n    return records[records[\"gender\"] == gender]\n\n\ndemo = gr.Interface(\n    filter_records,\n    [\n        gr.Dataframe(\n            headers=[\"name\", \"age\", \"gender\"],\n            datatype=[\"str\", \"number\", \"str\"],\n            row_count=5,\n            col_count=(3, \"fixed\"),\n        ),\n        gr.Dropdown([\"M\", \"F\", \"O\"]),\n    ],\n    \"dataframe\",\n    description=\"Enter gender as 'M', 'F', or 'O' for other.\",\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"],["matrix_transpose","import numpy as np\n\nimport gradio as gr\n\n\ndef transpose(matrix):\n    return matrix.T\n\n\ndemo = gr.Interface(\n    transpose,\n    gr.Dataframe(type=\"numpy\", datatype=\"number\", row_count=5, col_count=3),\n    \"numpy\",\n    examples=[\n        [np.zeros((3, 3)).tolist()],\n        [np.ones((2, 2)).tolist()],\n        [np.random.randint(0, 10, (3, 10)).tolist()],\n        [np.random.randint(0, 10, (10, 3)).tolist()],\n        [np.random.randint(0, 10, (10, 10)).tolist()],\n    ],\n    cache_examples=False\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"],["tax_calculator","import gradio as gr\n\ndef tax_calculator(income, marital_status, assets):\n    tax_brackets = [(10, 0), (25, 8), (60, 12), (120, 20), (250, 30)]\n    total_deductible = sum(assets[\"Cost\"])\n    taxable_income = income - total_deductible\n\n    total_tax = 0\n    for bracket, rate in tax_brackets:\n        if taxable_income > bracket:\n            total_tax += (taxable_income - bracket) * rate / 100\n\n    if marital_status == \"Married\":\n        total_tax *= 0.75\n    elif marital_status == \"Divorced\":\n        total_tax *= 0.8\n\n    return round(total_tax)\n\ndemo = gr.Interface(\n    tax_calculator,\n    [\n        \"number\",\n        gr.Radio([\"Single\", \"Married\", \"Divorced\"]),\n        gr.Dataframe(\n            headers=[\"Item\", \"Cost\"],\n            datatype=[\"str\", \"number\"],\n            label=\"Assets Purchased this Year\",\n        ),\n    ],\n    \"number\",\n    examples=[\n        [10000, \"Married\", [[\"Suit\", 5000], [\"Laptop\", 800], [\"Car\", 1800]]],\n        [80000, \"Single\", [[\"Suit\", 800], [\"Watch\", 1800], [\"Car\", 800]]],\n    ],\n)\n\ndemo.launch()\n"],["sort_records","import gradio as gr\nimport os\n\ndef sort_records(records):\n    return records.sort(\"Quantity\")\n\ndemo = gr.Interface(\n    sort_records,\n    gr.Dataframe(\n        headers=[\"Item\", \"Quantity\"],\n        datatype=[\"str\", \"number\"],\n        row_count=3,\n        col_count=(2, \"fixed\"),\n        type=\"polars\"\n    ),\n    \"dataframe\",\n    description=\"Sort by Quantity\"\n)\n\nif __name__ == \"__main__\":\n    demo.launch()"]],parent:"gradio"},dataset:{class:null,name:"Dataset",description:"Creates a gallery or table to display data samples. This component is designed for internal use to display examples.",tags:{},parameters:[{name:"label",annotation:"str | None",doc:"The label for this component, appears above the component.",default:"None"},{name:"components",annotation:"list[Component] | list[str]",doc:"Which component types to show in this dataset widget, can be passed in as a list of string names or Components instances. The following components are supported in a Dataset: Audio, Checkbox, CheckboxGroup, ColorPicker, Dataframe, Dropdown, File, HTML, Image, Markdown, Model3D, Number, Radio, Slider, Textbox, TimeSeries, Video"},{name:"component_props",annotation:"list[dict[str, Any]] | None",doc:null,default:"None"},{name:"samples",annotation:"list[list[Any]] | None",doc:"a nested list of samples. Each sublist within the outer list represents a data sample, and each element within the sublist represents an value for each component",default:"None"},{name:"headers",annotation:"list[str] | None",doc:"Column headers in the Dataset widget, should be the same len as components. If not provided, inferred from component labels",default:"None"},{name:"type",annotation:"Literal[('values', 'index')]",doc:"&#x27;values&#x27; if clicking on a sample should pass the value of the sample, or &quot;index&quot; if it should pass the index of the sample",default:"\"values\""},{name:"samples_per_page",annotation:"int",doc:"how many examples to show per page.",default:"10"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"proxy_url",annotation:"str | None",doc:"The URL of the external Space used to load this component. Set automatically when using `gr.load()`. This should not be set manually.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"click",description:"Triggered when the Dataset is clicked.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Dataset"},{fn:null,name:"select",description:"Event listener for when the user selects or deselects the Dataset. Uses event data gradio.SelectData to carry `value` referring to the label of the Dataset, and `selected` to refer to state of the Dataset. See EventData documentation on how to use this event data",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Dataset"}],preprocess:{parameter_doc:[{name:"payload",annotation:"int",doc:"the index of the selected example in the dataset"}],return_doc:{annotation:"int | list | None",doc:"Passes the selected sample either as a `list` of data corresponding to each input component (if `type` is \"value\") or as an `int` index (if `type` is \"index\")"}},postprocess:{parameter_doc:[{name:"samples",annotation:"list[list]",doc:"Expects a `list[list]` corresponding to the dataset data, can be used to update the dataset."}],return_doc:{annotation:"dict",doc:"Returns the updated dataset data as a `dict` with the key \"samples\"."}},string_shortcuts:[["Dataset","dataset","Uses default values"]],parent:"gradio"},downloadbutton:{class:null,name:"DownloadButton",description:"Creates a button, that when clicked, allows a user to download a single file of arbitrary type. \u003Cbr>",tags:{demos:"upload_and_download"},parameters:[{name:"label",annotation:"str",doc:"Text to display on the button. Defaults to &quot;Download&quot;.",default:"\"Download\""},{name:"value",annotation:"str | Path | Callable | None",doc:"A str or pathlib.Path filepath or URL to download, or a Callable that returns a str or pathlib.Path filepath or URL to download.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"variant",annotation:"Literal[('primary', 'secondary', 'stop')]",doc:"&#x27;primary&#x27; for main call-to-action, &#x27;secondary&#x27; for a more subdued style, &#x27;stop&#x27; for a stop button.",default:"\"secondary\""},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"size",annotation:"Literal[('sm', 'lg')] | None",doc:"Size of the button. Can be &quot;sm&quot; or &quot;lg&quot;.",default:"None"},{name:"icon",annotation:"str | None",doc:"URL or path to the icon file to display within the button. If None, no icon will be displayed.",default:"None"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int | None",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"None"},{name:"interactive",annotation:"bool",doc:"If False, the UploadButton will be in a disabled state.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"click",description:"Triggered when the DownloadButton is clicked.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.DownloadButton"}],preprocess:{parameter_doc:[{name:"payload",annotation:"FileData | None",doc:"File information as a FileData object,"}],return_doc:{annotation:"str | None",doc:"(Rarely used) passes the file as a `str` into the function."}},postprocess:{parameter_doc:[{name:"value",annotation:"str | Path | None",doc:"Expects a `str` or `pathlib.Path` filepath"}],return_doc:{annotation:"FileData | None",doc:"File information as a FileData object"}},string_shortcuts:[["DownloadButton","downloadbutton","Uses default values"]],demos:[["upload_and_download","from pathlib import Path\nimport gradio as gr\n\ndef upload_file(filepath):\n    name = Path(filepath).name\n    return [gr.UploadButton(visible=False), gr.DownloadButton(label=f\"Download {name}\", value=filepath, visible=True)]\n\ndef download_file():\n    return [gr.UploadButton(visible=True), gr.DownloadButton(visible=False)]\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"First upload a file and and then you'll be able download it (but only once!)\")\n    with gr.Row():\n        u = gr.UploadButton(\"Upload a file\", file_count=\"single\")\n        d = gr.DownloadButton(\"Download the file\", visible=False)\n\n    u.upload(upload_file, u, [u, d])\n    d.click(download_file, None, [u, d])\n\n\nif __name__ == \"__main__\":\n    demo.launch()"]],parent:"gradio"},dropdown:{class:null,name:"Dropdown",description:"Creates a dropdown of choices from which a single entry or multiple entries can be selected (as an input component) or displayed (as an output component). \u003Cbr>",tags:{demos:"sentence_builder"},parameters:[{name:"choices",annotation:"list[str | int | float | tuple[str, str | int | float]] | None",doc:"A list of string options to choose from. An option can also be a tuple of the form (name, value), where name is the displayed name of the dropdown choice and value is the value to be passed to the function, or returned by the function.",default:"None"},{name:"value",annotation:"str | int | float | list[str | int | float] | Callable | None",doc:"default value(s) selected in dropdown. If None, no value is selected by default. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"type",annotation:"Literal[('value', 'index')]",doc:"Type of value to be returned by component. &quot;value&quot; returns the string of the choice selected, &quot;index&quot; returns the index of the choice selected.",default:"\"value\""},{name:"multiselect",annotation:"bool | None",doc:"if True, multiple choices can be selected.",default:"None"},{name:"allow_custom_value",annotation:"bool",doc:"If True, allows user to enter a custom value that is not in the list of choices.",default:"False"},{name:"max_choices",annotation:"int | None",doc:"maximum number of choices that can be selected. If None, no limit is enforced.",default:"None"},{name:"filterable",annotation:"bool",doc:"If True, user will be able to type into the dropdown and filter the choices by typing. Can only be set to False if `allow_custom_value` is False.",default:"True"},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"info",annotation:"str | None",doc:"additional component description.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"interactive",annotation:"bool | None",doc:"if True, choices in this dropdown will be selectable; if False, selection will be disabled. If not provided, this is inferred based on whether the component is used as an input or output.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:null,default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the Dropdown changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Dropdown"},{fn:null,name:"input",description:"This listener is triggered when the user changes the value of the Dropdown.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Dropdown"},{fn:null,name:"select",description:"Event listener for when the user selects or deselects the Dropdown. Uses event data gradio.SelectData to carry `value` referring to the label of the Dropdown, and `selected` to refer to state of the Dropdown. See EventData documentation on how to use this event data",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Dropdown"},{fn:null,name:"focus",description:"This listener is triggered when the Dropdown is focused.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Dropdown"},{fn:null,name:"blur",description:"This listener is triggered when the Dropdown is unfocused/blurred.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Dropdown"},{fn:null,name:"key_up",description:"This listener is triggered when the user presses a key while the Dropdown is focused.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Dropdown"}],preprocess:{parameter_doc:[{name:"payload",annotation:"str | int | float | list[str | int | float] | None",doc:"the value of the selected dropdown choice(s)"}],return_doc:{annotation:"str | int | float | list[str | int | float] | list[int | None] | None",doc:"Passes the value of the selected dropdown choice as a `str | int | float` or its index as an `int` into the function, depending on `type`. Or, if `multiselect` is True, passes the values of the selected dropdown choices as a list of correspoding values/indices instead."}},postprocess:{parameter_doc:[{name:"value",annotation:"str | int | float | list[str | int | float] | None",doc:"Expects a `str | int | float` corresponding to the value of the dropdown entry to be selected. Or, if `multiselect` is True, expects a `list` of values corresponding to the selected dropdown entries."}],return_doc:{annotation:"str | int | float | list[str | int | float] | None",doc:"Returns the values of the selected dropdown entry or entries."}},string_shortcuts:[["Dropdown","dropdown","Uses default values"]],demos:[["sentence_builder","import gradio as gr\n\n\ndef sentence_builder(quantity, animal, countries, place, activity_list, morning):\n    return f\"\"\"The {quantity} {animal}s from {\" and \".join(countries)} went to the {place} where they {\" and \".join(activity_list)} until the {\"morning\" if morning else \"night\"}\"\"\"\n\n\ndemo = gr.Interface(\n    sentence_builder,\n    [\n        gr.Slider(2, 20, value=4, label=\"Count\", info=\"Choose between 2 and 20\"),\n        gr.Dropdown(\n            [\"cat\", \"dog\", \"bird\"], label=\"Animal\", info=\"Will add more animals later!\"\n        ),\n        gr.CheckboxGroup([\"USA\", \"Japan\", \"Pakistan\"], label=\"Countries\", info=\"Where are they from?\"),\n        gr.Radio([\"park\", \"zoo\", \"road\"], label=\"Location\", info=\"Where did they go?\"),\n        gr.Dropdown(\n            [\"ran\", \"swam\", \"ate\", \"slept\"], value=[\"swam\", \"slept\"], multiselect=True, label=\"Activity\", info=\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed auctor, nisl eget ultricies aliquam, nunc nisl aliquet nunc, eget aliquam nisl nunc vel nisl.\"\n        ),\n        gr.Checkbox(label=\"Morning\", info=\"Did they do it in the morning?\"),\n    ],\n    \"text\",\n    examples=[\n        [2, \"cat\", [\"Japan\", \"Pakistan\"], \"park\", [\"ate\", \"swam\"], True],\n        [4, \"dog\", [\"Japan\"], \"zoo\", [\"ate\", \"swam\"], False],\n        [10, \"bird\", [\"USA\", \"Pakistan\"], \"road\", [\"ran\"], False],\n        [8, \"cat\", [\"Pakistan\"], \"zoo\", [\"ate\"], True],\n    ]\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]],parent:"gradio"},duplicatebutton:{class:null,name:"DuplicateButton",description:"Button that triggers a Spaces Duplication, when the demo is on Hugging Face Spaces. Does nothing locally.",tags:{},parameters:[{name:"value",annotation:"str",doc:"Default text for the button to display. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"\"Duplicate Space\""},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"variant",annotation:"Literal[('primary', 'secondary', 'stop')]",doc:"&#x27;primary&#x27; for main call-to-action, &#x27;secondary&#x27; for a more subdued style, &#x27;stop&#x27; for a stop button.",default:"\"secondary\""},{name:"size",annotation:"Literal[('sm', 'lg')] | None",doc:"Size of the button. Can be &quot;sm&quot; or &quot;lg&quot;.",default:"\"sm\""},{name:"icon",annotation:"str | None",doc:"URL or path to the icon file to display within the button. If None, no icon will be displayed.",default:"None"},{name:"link",annotation:"str | None",doc:"URL to open when the button is clicked. If None, no link will be used.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"interactive",annotation:"bool",doc:"If False, the Button will be in a disabled state.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"0"},{name:"min_width",annotation:"int | None",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"click",description:"Triggered when the Button is clicked.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.DuplicateButton"}],preprocess:{parameter_doc:[{name:"payload",annotation:"str | None",doc:"string corresponding to the button label"}],return_doc:{annotation:"str | None",doc:"(Rarely used) the `str` corresponding to the button label when the button is clicked"}},postprocess:{parameter_doc:[{name:"value",annotation:"str | None",doc:"string corresponding to the button label"}],return_doc:{annotation:"str | None",doc:"Expects a `str` value that is set as the button label"}},string_shortcuts:[["DuplicateButton","duplicatebutton","Uses default values"]],parent:"gradio"},file:{class:null,name:"File",description:"Creates a file component that allows uploading one or more generic files (when used as an input) or displaying generic files or URLs for download (as output). \u003Cbr>",tags:{demo:"zip_files, zip_to_json"},parameters:[{name:"value",annotation:"str | list[str] | Callable | None",doc:"Default file(s) to display, given as a str file path or URL, or a list of str file paths / URLs. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"file_count",annotation:"Literal[('single', 'multiple', 'directory')]",doc:"if single, allows user to upload one file. If &quot;multiple&quot;, user uploads multiple files. If &quot;directory&quot;, user uploads all files in selected directory. Return type will be list for each file in case of &quot;multiple&quot; or &quot;directory&quot;.",default:"\"single\""},{name:"file_types",annotation:"list[str] | None",doc:"List of file extensions or types of files to be uploaded (e.g. [&#x27;image&#x27;, &#x27;.json&#x27;, &#x27;.mp4&#x27;]). &quot;file&quot; allows any file to be uploaded, &quot;image&quot; allows only image files to be uploaded, &quot;audio&quot; allows only audio files to be uploaded, &quot;video&quot; allows only video files to be uploaded, &quot;text&quot; allows only text files to be uploaded.",default:"None"},{name:"type",annotation:"Literal[('filepath', 'binary')]",doc:"Type of value to be returned by component. &quot;file&quot; returns a temporary file object with the same base name as the uploaded file, whose full path can be retrieved by file_obj.name, &quot;binary&quot; returns an bytes object.",default:"\"filepath\""},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise.sed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"height",annotation:"int | float | None",doc:"The maximum height of the file component, specified in pixels if a number is passed, or in CSS units if a string is passed. If more files are uploaded than can fit in the height, a scrollbar will appear.",default:"None"},{name:"interactive",annotation:"bool | None",doc:"if True, will allow users to upload a file; if False, can only be used to display files. If not provided, this is inferred based on whether the component is used as an input or output.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the File changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.File"},{fn:null,name:"select",description:"Event listener for when the user selects or deselects the File. Uses event data gradio.SelectData to carry `value` referring to the label of the File, and `selected` to refer to state of the File. See EventData documentation on how to use this event data",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.File"},{fn:null,name:"clear",description:"This listener is triggered when the user clears the File using the X button for the component.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.File"},{fn:null,name:"upload",description:"This listener is triggered when the user uploads a file into the File.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.File"},{fn:null,name:"delete",description:"This listener is triggered when the user deletes and item from the File. Uses event data gradio.DeletedFileData to carry `value` referring to the file that was deleted as an instance of FileData. See EventData documentation on how to use this event data",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.File"}],preprocess:{parameter_doc:[{name:"payload",annotation:"ListFiles | FileData | None",doc:"File information as a FileData object, or a list of FileData objects."}],return_doc:{annotation:"bytes | str | list[bytes] | list[str] | None",doc:"Passes the file as a `str` or `bytes` object, or a list of `str` or list of `bytes` objects, depending on `type` and `file_count`."}},postprocess:{parameter_doc:[{name:"value",annotation:"str | list[str] | None",doc:"Expects a `str` filepath or URL, or a `list[str]` of filepaths/URLs."}],return_doc:{annotation:"ListFiles | FileData | None",doc:"File information as a FileData object, or a list of FileData objects."}},string_shortcuts:[["File","file","Uses default values"],["Files","files","Uses file_count=\"multiple\""]],parent:"gradio"},fileexplorer:{class:null,name:"FileExplorer",description:"Creates a file explorer component that allows users to browse files on the machine hosting the Gradio app. As an input component, it also allows users to select files to be used as input to a function, while as an output component, it displays selected files. \u003Cbr>",tags:{demos:"file_explorer"},parameters:[{name:"glob",annotation:"str",doc:"The glob-style pattern used to select which files to display, e.g. &quot;*&quot; to match all files, &quot;*.png&quot; to match all .png files, &quot;**/*.txt&quot; to match any .txt file in any subdirectory, etc. The default value matches all files and folders recursively. See the Python glob documentation at https://docs.python.org/3/library/glob.html for more information.",default:"\"**/*\""},{name:"value",annotation:"str | list[str] | Callable | None",doc:"The file (or list of files, depending on the `file_count` parameter) to show as &quot;selected&quot; when the component is first loaded. If a callable is provided, it will be called when the app loads to set the initial value of the component. If not provided, no files are shown as selected.",default:"None"},{name:"file_count",annotation:"Literal[('single', 'multiple')]",doc:"Whether to allow single or multiple files to be selected. If &quot;single&quot;, the component will return a single absolute file path as a string. If &quot;multiple&quot;, the component will return a list of absolute file paths as a list of strings.",default:"\"multiple\""},{name:"root_dir",annotation:"str | Path",doc:"Path to root directory to select files from. If not provided, defaults to current working directory.",default:"\".\""},{name:"ignore_glob",annotation:"str | None",doc:"The glob-style, case-sensitive pattern that will be used to exclude files from the list. For example, &quot;*.py&quot; will exclude all .py files from the list. See the Python glob documentation at https://docs.python.org/3/library/glob.html for more information.",default:"None"},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise.sed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"height",annotation:"int | float | None",doc:"The maximum height of the file component, specified in pixels if a number is passed, or in CSS units if a string is passed. If more files are uploaded than can fit in the height, a scrollbar will appear.",default:"None"},{name:"interactive",annotation:"bool | None",doc:"if True, will allow users to select file(s); if False, will only display files. If not provided, this is inferred based on whether the component is used as an input or output.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"root",annotation:"None",doc:null,default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.FileExplorer"}],preprocess:{parameter_doc:[{name:"payload",annotation:"FileExplorerData | None",doc:"List of selected files as a FileExplorerData object."}],return_doc:{annotation:"list[str] | str | None",doc:"Passes the selected file or directory as a `str` path (relative to `root`) or `list[str}` depending on `file_count`"}},postprocess:{parameter_doc:[{name:"value",annotation:"str | list[str] | None",doc:"Expects function to return a `str` path to a file, or `list[str]` consisting of paths to files."}],return_doc:{annotation:"FileExplorerData | None",doc:"A FileExplorerData object containing the selected files as a list of strings."}},string_shortcuts:[["FileExplorer","fileexplorer","Uses default values"]],demos:[["file_explorer","import gradio as gr\nfrom pathlib import Path\n\ncurrent_file_path = Path(__file__).resolve()\nrelative_path = \"path/to/file\"\nabsolute_path = (current_file_path.parent / \"..\" / \"..\" / \"gradio\").resolve()\n\n\ndef get_file_content(file):\n    return (file,)\n\n\nwith gr.Blocks() as demo:\n    gr.Markdown('### `FileExplorer` to `FileExplorer` -- `file_count=\"multiple\"`')\n    submit_btn = gr.Button(\"Select\")\n    with gr.Row():\n        file = gr.FileExplorer(\n            glob=\"**/components/*.py\",\n            # value=[\"themes/utils\"],\n            root=absolute_path,\n            ignore_glob=\"**/__init__.py\",\n        )\n\n        file2 = gr.FileExplorer(\n            glob=\"**/components/**/*.py\",\n            root=absolute_path,\n            ignore_glob=\"**/__init__.py\",\n        )\n    submit_btn.click(lambda x: x, file, file2)\n\n    gr.Markdown(\"---\")\n    gr.Markdown('### `FileExplorer` to `Code` -- `file_count=\"single\"`')\n    with gr.Group():\n        with gr.Row():\n            file_3 = gr.FileExplorer(\n                scale=1,\n                glob=\"**/components/**/*.py\",\n                value=[\"themes/utils\"],\n                file_count=\"single\",\n                root=absolute_path,\n                ignore_glob=\"**/__init__.py\",\n                elem_id=\"file\",\n            )\n\n            code = gr.Code(lines=30, scale=2, language=\"python\")\n\n    file_3.change(get_file_content, file_3, code)\n\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]],parent:"gradio"},gallery:{class:null,name:"Gallery",description:"Creates a gallery component that allows displaying a grid of images, and optionally captions. If used as an input, the user can upload images to the gallery. If used as an output, the user can click on individual images to view them at a higher resolution. \u003Cbr>",tags:{demos:"fake_gan"},parameters:[{name:"value",annotation:"list[np.ndarray | PIL.Image.Image | str | Path | tuple] | Callable | None",doc:"List of images to display in the gallery by default. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"format",annotation:"str",doc:"Format to save images before they are returned to the frontend, such as &#x27;jpeg&#x27; or &#x27;png&#x27;. This parameter only applies to images that are returned from the prediction function as numpy arrays or PIL Images. The format should be supported by the PIL library.",default:"\"webp\""},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"columns",annotation:"int | tuple | None",doc:"Represents the number of images that should be shown in one row, for each of the six standard screen sizes (&lt;576px, &lt;768px, &lt;992px, &lt;1200px, &lt;1400px, &gt;1400px). If fewer than 6 are given then the last will be used for all subsequent breakpoints",default:"2"},{name:"rows",annotation:"int | tuple | None",doc:"Represents the number of rows in the image grid, for each of the six standard screen sizes (&lt;576px, &lt;768px, &lt;992px, &lt;1200px, &lt;1400px, &gt;1400px). If fewer than 6 are given then the last will be used for all subsequent breakpoints",default:"None"},{name:"height",annotation:"int | float | None",doc:"The height of the gallery component, specified in pixels if a number is passed, or in CSS units if a string is passed. If more images are displayed than can fit in the height, a scrollbar will appear.",default:"None"},{name:"allow_preview",annotation:"bool",doc:"If True, images in the gallery will be enlarged when they are clicked. Default is True.",default:"True"},{name:"preview",annotation:"bool | None",doc:"If True, Gallery will start in preview mode, which shows all of the images as thumbnails and allows the user to click on them to view them in full size. Only works if allow_preview is True.",default:"None"},{name:"selected_index",annotation:"int | None",doc:"The index of the image that should be initially selected. If None, no image will be selected at start. If provided, will set Gallery to preview mode unless allow_preview is set to False.",default:"None"},{name:"object_fit",annotation:"Literal[('contain', 'cover', 'fill', 'none', 'scale-down')] | None",doc:"CSS object-fit property for the thumbnail images in the gallery. Can be &quot;contain&quot;, &quot;cover&quot;, &quot;fill&quot;, &quot;none&quot;, or &quot;scale-down&quot;.",default:"None"},{name:"show_share_button",annotation:"bool | None",doc:"If True, will show a share icon in the corner of the component that allows user to share outputs to Hugging Face Spaces Discussions. If False, icon does not appear. If set to None (default behavior), then the icon appears if this Gradio app is launched on Spaces, but not otherwise.",default:"None"},{name:"show_download_button",annotation:"bool | None",doc:"If True, will show a download button in the corner of the selected image. If False, the icon does not appear. Default is True.",default:"True"},{name:"interactive",annotation:"bool | None",doc:"If True, the gallery will be interactive, allowing the user to upload images. If False, the gallery will be static. Default is True.",default:"None"},{name:"type",annotation:"Literal[('numpy', 'pil', 'filepath')]",doc:"The format the image is converted to before being passed into the prediction function. &quot;numpy&quot; converts the image to a numpy array with shape (height, width, 3) and values from 0 to 255, &quot;pil&quot; converts the image to a PIL image object, &quot;filepath&quot; passes a str path to a temporary file containing the image. If the image is SVG, the `type` is ignored and the filepath of the SVG is returned.",default:"\"filepath\""}],returns:{annotation:null},example:null,fns:[{fn:null,name:"select",description:"Event listener for when the user selects or deselects the Gallery. Uses event data gradio.SelectData to carry `value` referring to the label of the Gallery, and `selected` to refer to state of the Gallery. See EventData documentation on how to use this event data",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Gallery"},{fn:null,name:"upload",description:"This listener is triggered when the user uploads a file into the Gallery.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Gallery"},{fn:null,name:"change",description:"Triggered when the value of the Gallery changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Gallery"}],preprocess:{parameter_doc:[{name:"payload",annotation:"GalleryData | None",doc:"a list of images, or list of (image, caption) tuples"}],return_doc:{annotation:"List[tuple[str, str | None]] | List[tuple[PIL.Image.Image, str | None]] | List[tuple[np.ndarray, str | None]] | None",doc:"Passes the list of images as a list of (image, caption) tuples, or a list of (image, None) tuples if no captions are provided (which is usually the case). The image can be a `str` file path, a `numpy` array, or a `PIL.Image` object depending on `type`."}},postprocess:{parameter_doc:[{name:"value",annotation:"list[GalleryImageType | CaptionedGalleryImageType] | None",doc:"Expects the function to return a `list` of images, or `list` of (image, `str` caption) tuples. Each image can be a `str` file path, a `numpy` array, or a `PIL.Image` object."}],return_doc:{annotation:"GalleryData",doc:"a list of images, or list of (image, caption) tuples"}},string_shortcuts:[["Gallery","gallery","Uses default values"]],demos:[["fake_gan","# This demo needs to be run from the repo folder.\n# python demo/fake_gan/run.py\nimport random\n\nimport gradio as gr\n\n\ndef fake_gan():\n    images = [\n        (random.choice(\n            [\n                \"http://www.marketingtool.online/en/face-generator/img/faces/avatar-1151ce9f4b2043de0d2e3b7826127998.jpg\",\n                \"http://www.marketingtool.online/en/face-generator/img/faces/avatar-116b5e92936b766b7fdfc242649337f7.jpg\",\n                \"http://www.marketingtool.online/en/face-generator/img/faces/avatar-1163530ca19b5cebe1b002b8ec67b6fc.jpg\",\n                \"http://www.marketingtool.online/en/face-generator/img/faces/avatar-1116395d6e6a6581eef8b8038f4c8e55.jpg\",\n                \"http://www.marketingtool.online/en/face-generator/img/faces/avatar-11319be65db395d0e8e6855d18ddcef0.jpg\",\n            ]\n        ), f\"label {i}\")\n        for i in range(3)\n    ]\n    return images\n\n\nwith gr.Blocks() as demo:\n    gallery = gr.Gallery(\n        label=\"Generated images\", show_label=False, elem_id=\"gallery\"\n    , columns=[3], rows=[1], object_fit=\"contain\", height=\"auto\")\n    btn = gr.Button(\"Generate images\", scale=0)\n\n    btn.click(fake_gan, None, gallery)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]],parent:"gradio"},highlightedtext:{class:null,name:"HighlightedText",description:"Displays text that contains spans that are highlighted by category or numerical value. \u003Cbr>",tags:{demos:"diff_texts",guides:"named-entity-recognition"},parameters:[{name:"value",annotation:"list[tuple[str, str | float | None]] | dict | Callable | None",doc:"Default value to show. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"color_map",annotation:"dict[str, str] | None",doc:"A dictionary mapping labels to colors. The colors may be specified as hex codes or by their names. For example: {&quot;person&quot;: &quot;red&quot;, &quot;location&quot;: &quot;#FFEE22&quot;}",default:"None"},{name:"show_legend",annotation:"bool",doc:"whether to show span categories in a separate legend or inline.",default:"False"},{name:"show_inline_category",annotation:"bool",doc:"If False, will not display span category label. Only applies if show_legend=False and interactive=False.",default:"True"},{name:"combine_adjacent",annotation:"bool",doc:"If True, will merge the labels of adjacent tokens belonging to the same category.",default:"False"},{name:"adjacent_separator",annotation:"str",doc:"Specifies the separator to be used between tokens if combine_adjacent is True.",default:"\"\""},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"interactive",annotation:"bool | None",doc:"If True, the component will be editable, and allow user to select spans of text and label them.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the HighlightedText changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.HighlightedText"},{fn:null,name:"select",description:"Event listener for when the user selects or deselects the HighlightedText. Uses event data gradio.SelectData to carry `value` referring to the label of the HighlightedText, and `selected` to refer to state of the HighlightedText. See EventData documentation on how to use this event data",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.HighlightedText"}],preprocess:{parameter_doc:[{name:"payload",annotation:"HighlightedTextData | None",doc:"An instance of HighlightedTextData"}],return_doc:{annotation:"list[tuple[str, str | float | None]] | None",doc:"Passes the value as a list of tuples as a `list[tuple]` into the function. Each `tuple` consists of a `str` substring of the text (so the entire text is included) and `str | float | None` label, which is the category or confidence of that substring."}},postprocess:{parameter_doc:[{name:"value",annotation:"list[tuple[str, str | float | None]] | dict | None",doc:"Expects a list of (word, category) tuples, or a dictionary of two keys: \"text\", and \"entities\", which itself is a list of dictionaries, each of which have the keys: \"entity\" (or \"entity_group\"), \"start\", and \"end\""}],return_doc:{annotation:"HighlightedTextData | None",doc:"An instance of HighlightedTextData"}},string_shortcuts:[["HighlightedText","highlightedtext","Uses default values"]],demos:[["diff_texts","from difflib import Differ\n\nimport gradio as gr\n\n\ndef diff_texts(text1, text2):\n    d = Differ()\n    return [\n        (token[2:], token[0] if token[0] != \" \" else None)\n        for token in d.compare(text1, text2)\n    ]\n\n\ndemo = gr.Interface(\n    diff_texts,\n    [\n        gr.Textbox(\n            label=\"Text 1\",\n            info=\"Initial text\",\n            lines=3,\n            value=\"The quick brown fox jumped over the lazy dogs.\",\n        ),\n        gr.Textbox(\n            label=\"Text 2\",\n            info=\"Text to compare\",\n            lines=3,\n            value=\"The fast brown fox jumps over lazy dogs.\",\n        ),\n    ],\n    gr.HighlightedText(\n        label=\"Diff\",\n        combine_adjacent=True,\n        show_legend=True,\n        color_map={\"+\": \"red\", \"-\": \"green\"}),\n    theme=gr.themes.Base()\n)\nif __name__ == \"__main__\":\n    demo.launch()\n"]],guides:[{name:"named-entity-recognition",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:59,pretty_name:"Named Entity Recognition",content:"# Named-Entity Recognition\n\n\n\n\n## Introduction\n\nNamed-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or \"token\") into different categories, such as names of people or names of locations, or different parts of speech.\n\nFor example, given the sentence:\n\n> Does Chicago have any Pakistani restaurants?\n\nA named-entity recognition algorithm may identify:\n\n- \"Chicago\" as a **location**\n- \"Pakistani\" as an **ethnicity**\n\nand so on.\n\nUsing `gradio` (specifically the `HighlightedText` component), you can easily build a web demo of your NER model and share that with the rest of your team.\n\nHere is an example of a demo that you'll be able to build:\n\n\u003Cgradio-app space='gradio/ner_pipeline'>\u003C/gradio-app>\n\nThis tutorial will show how to take a pretrained NER model and deploy it with a Gradio interface. We will show two different ways to use the `HighlightedText` component -- depending on your NER model, either of these two ways may be easier to learn!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). You will also need a pretrained named-entity recognition model. You can use your own, while in this tutorial, we will use one from the `transformers` library.\n\n### Approach 1: List of Entity Dictionaries\n\nMany named-entity recognition models output a list of dictionaries. Each dictionary consists of an _entity_, a \"start\" index, and an \"end\" index. This is, for example, how NER models in the `transformers` library operate:\n\n```py\nfrom transformers import pipeline\nner_pipeline = pipeline(\"ner\")\nner_pipeline(\"Does Chicago have any Pakistani restaurants\")\n```\n\nOutput:\n\n```bash\n[{'entity': 'I-LOC',\n  'score': 0.9988978,\n  'index': 2,\n  'word': 'Chicago',\n  'start': 5,\n  'end': 12},\n {'entity': 'I-MISC',\n  'score': 0.9958592,\n  'index': 5,\n  'word': 'Pakistani',\n  'start': 22,\n  'end': 31}]\n```\n\nIf you have such a model, it is very easy to hook it up to Gradio's `HighlightedText` component. All you need to do is pass in this **list of entities**, along with the **original text** to the model, together as dictionary, with the keys being `\"entities\"` and `\"text\"` respectively.\n\nHere is a complete example:\n\n```python\nfrom transformers import pipeline\n\nimport gradio as gr\n\nner_pipeline = pipeline(\"ner\")\n\nexamples = [\n    \"Does Chicago have any stores and does Joe live here?\",\n]\n\ndef ner(text):\n    output = ner_pipeline(text)\n    return {\"text\": text, \"entities\": output}    \n\ndemo = gr.Interface(ner,\n             gr.Textbox(placeholder=\"Enter sentence here...\"), \n             gr.HighlightedText(),\n             examples=examples)\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/ner_pipeline'>\u003C/gradio-app>\n\n### Approach 2: List of Tuples\n\nAn alternative way to pass data into the `HighlightedText` component is a list of tuples. The first element of each tuple should be the word or words that are being classified into a particular entity. The second element should be the entity label (or `None` if they should be unlabeled). The `HighlightedText` component automatically strings together the words and labels to display the entities.\n\nIn some cases, this can be easier than the first approach. Here is a demo showing this approach using Spacy's parts-of-speech tagger:\n\n```python\nimport gradio as gr\nimport os\nos.system('python -m spacy download en_core_web_sm')\nimport spacy\nfrom spacy import displacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef text_analysis(text):\n    doc = nlp(text)\n    html = displacy.render(doc, style=\"dep\", page=True)\n    html = (\n        \"\u003Cdiv style='max-width:100%; max-height:360px; overflow:auto'>\"\n        + html\n        + \"\u003C/div>\"\n    )\n    pos_count = {\n        \"char_count\": len(text),\n        \"token_count\": 0,\n    }\n    pos_tokens = []\n\n    for token in doc:\n        pos_tokens.extend([(token.text, token.pos_), (\" \", None)])\n\n    return pos_tokens, pos_count, html\n\ndemo = gr.Interface(\n    text_analysis,\n    gr.Textbox(placeholder=\"Enter sentence here...\"),\n    [\"highlight\", \"json\", \"html\"],\n    examples=[\n        [\"What a beautiful morning for a walk!\"],\n        [\"It was the best of times, it was the worst of times.\"],\n    ],\n)\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/text_analysis'>\u003C/gradio-app>\n\n---\n\nAnd you're done! That's all you need to know to build a web-based GUI for your NER model.\n\nFun tip: you can share your NER demo instantly with others simply by setting `share=True` in `launch()`.\n",tags:["NER","TEXT","HIGHLIGHT"],spaces:["https://huggingface.co/spaces/rajistics/biobert_ner_demo","https://huggingface.co/spaces/abidlabs/ner","https://huggingface.co/spaces/rajistics/Financial_Analyst_AI"],url:"/guides/named-entity-recognition/",contributor:null}],parent:"gradio"},html:{class:null,name:"HTML",description:"Creates a component to display arbitrary HTML output. As this component does not accept user input, it is rarely used as an input component. \u003Cbr>",tags:{demos:"blocks_scroll",guides:"key-features"},parameters:[{name:"value",annotation:"str | Callable | None",doc:"Default value. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"label",annotation:"str | None",doc:"The label for this component. Is used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"This parameter has no effect.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the HTML changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.HTML"}],preprocess:{parameter_doc:[{name:"payload",annotation:"str | None",doc:"string corresponding to the HTML"}],return_doc:{annotation:"str | None",doc:"(Rarely used) passes the HTML as a `str`."}},postprocess:{parameter_doc:[{name:"value",annotation:"str | None",doc:"Expects a `str` consisting of valid HTML."}],return_doc:{annotation:"str | None",doc:"Returns the HTML string."}},string_shortcuts:[["HTML","html","Uses default values"]],demos:[["blocks_scroll","import gradio as gr\n\n\ndemo = gr.Blocks()\n\nwith demo:\n    inp = gr.Textbox(placeholder=\"Enter text.\")\n    scroll_btn = gr.Button(\"Scroll\")\n    no_scroll_btn = gr.Button(\"No Scroll\")\n    big_block = gr.HTML(\"\"\"\n    \u003Cdiv style='height: 800px; width: 100px; background-color: pink;'>\u003C/div>\n    \"\"\")\n    out = gr.Textbox()\n    \n    scroll_btn.click(lambda x: x, \n               inputs=inp, \n               outputs=out,\n                scroll_to_output=True)\n    no_scroll_btn.click(lambda x: x, \n               inputs=inp, \n               outputs=out)\n\nif __name__ == \"__main__\":\n    demo.launch()"]],guides:[{name:"key-features",category:"getting-started",pretty_category:"Getting Started",guide_index:2,absolute_index:1,pretty_name:"Key Features",content:"# Key Features\n\nLet's go through some of the key features of Gradio. This guide is intended to be a high-level overview of various things that you should be aware of as you build your demo. Where appropriate, we link to more detailed guides on specific topics.\n\n1. [Components](#components)\n2. [Queuing](#queuing)\n3. [Streaming outputs](#streaming-outputs)\n4. [Streaming inputs](#streaming-inputs)\n5. [Alert modals](#alert-modals)\n6. [Styling](#styling)\n7. [Progress bars](#progress-bars)\n8. [Batch functions](#batch-functions)\n\n## Components\n\nGradio includes more than 30 pre-built components (as well as many user-built _custom components_) that can be used as inputs or outputs in your demo with a single line of code. These components correspond to common data types in machine learning and data science, e.g. the `gr.Image` component is designed to handle input or output images, the `gr.Label` component displays classification labels and probabilities, the `gr.Plot` component displays various kinds of plots, and so on.\n\nEach component includes various constructor attributes that control the properties of the component. For example, you can control the number of lines in a `gr.Textbox` using the `lines` argument (which takes a positive integer) in its constructor. Or you can control the way that a user can provide an image in the `gr.Image` component using the `sources` parameter (which takes a list like `[\"webcam\", \"upload\"]`).\n\n**Static and Interactive Components**\n\nEvery component has a _static_ version that is designed to *display* data, and most components also have an _interactive_ version designed to let users input or modify the data. Typically, you don't need to think about this distinction, because when you build a Gradio demo, Gradio automatically figures out whether the component should be static or interactive based on whether it is being used as an input or output. However, you can set this manually using the `interactive` argument that every component supports.\n\n**Preprocessing and Postprocessing**\n\nWhen a component is used as an input, Gradio automatically handles the _preprocessing_ needed to convert the data from a type sent by the user's browser (such as an uploaded image) to a form that can be accepted by your function (such as a `numpy` array).\n\n\nSimilarly, when a component is used as an output, Gradio automatically handles the _postprocessing_ needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user's browser (a gallery of images).\n\nConsider an example demo with three input components (`gr.Textbox`, `gr.Number`, and `gr.Image`) and two outputs (`gr.Number` and `gr.Gallery`) that serve as a UI for your image-to-image generation model. Below is a diagram of what our preprocessing will send to the model and what our postprocessing will require from it.\n\n![](https://github.com/gradio-app/gradio/blob/main/guides/assets/dataflow.svg?raw=true)\n\nIn this image, the following preprocessing steps happen to send the data from the browser to your function:\n\n* The text in the textbox is converted to a Python `str` (essentially no preprocessing)\n* The number in the number input in converted to a Python `float` (essentially no preprocessing)\n* Most importantly, ihe image supplied by the user is converted to a `numpy.array` representation of the RGB values in the image\n\nImages are converted to NumPy arrays because they are a common format for machine learning workflows. You can control the _preprocessing_ using the component's parameters when constructing the component. For example, if you instantiate the `Image` component with the following parameters, it will preprocess the image to the `PIL` format instead:\n\n```py\nimg = gr.Image(type=\"pil\")\n```\n\nPostprocessing is even simpler! Gradio automatically recognizes the format of the returned data (e.g. does the user's function return a `numpy` array or a `str` filepath for the `gr.Image` component?) and postprocesses it appropriately into a format that can be displayed by the browser.\n\nSo in the image above, the following postprocessing steps happen to send the data returned from a user's function to the browser:\n\n* The `float` is displayed as a number and displayed directly to the user\n* The list of string filepaths (`list[str]`) is interpreted as a list of image filepaths and displayed as a gallery in the browser\n\nTake a look at the [Docs](https://gradio.app/docs) to see all the parameters for each Gradio component.\n\n## Queuing\n\nEvery Gradio app comes with a built-in queuing system that can scale to thousands of concurrent users. You can configure the queue by using `queue()` method which is supported by the `gr.Interface`, `gr.Blocks`, and `gr.ChatInterface` classes. \n\nFor example, you can control the number of requests processed at a single time by setting the `default_concurrency_limit` parameter of `queue()`, e.g.\n\n```python\ndemo = gr.Interface(...).queue(default_concurrency_limit=5)\ndemo.launch()\n```\n\nThis limits the number of requests processed for this event listener at a single time to 5. By default, the `default_concurrency_limit` is actually set to `1`, which means that when many users are using your app, only a single user's request will be processed at a time. This is because many machine learning functions consume a significant amount of memory and so it is only suitable to have a single user using the demo at a time. However, you can change this parameter in your demo easily.\n\nSee the [docs on queueing](https://gradio.app/docs/gradio/interface#interface-queue) for more details on configuring the queuing parameters.\n\n## Streaming outputs\n\nIn some cases, you may want to stream a sequence of outputs rather than show a single output at once. For example, you might have an image generation model and you want to show the image that is generated at each step, leading up to the final image. Or you might have a chatbot which streams its response one token at a time instead of returning it all at once.\n\nIn such cases, you can supply a **generator** function into Gradio instead of a regular function. Creating generators in Python is very simple: instead of a single `return` value, a function should `yield` a series of values instead. Usually the `yield` statement is put in some kind of loop. Here's an example of an generator that simply counts up to a given number:\n\n```python\ndef my_generator(x):\n    for i in range(x):\n        yield i\n```\n\nYou supply a generator into Gradio the same way as you would a regular function. For example, here's a a (fake) image generation model that generates noise for several steps before outputting an image using the `gr.Interface` class:\n\n```python\nimport gradio as gr\nimport numpy as np\nimport time\n\ndef fake_diffusion(steps):\n    rng = np.random.default_rng()\n    for i in range(steps):\n        time.sleep(1)\n        image = rng.random(size=(600, 600, 3))\n        yield image\n    image = np.ones((1000,1000,3), np.uint8)\n    image[:] = [255, 124, 0]\n    yield image\n\n\ndemo = gr.Interface(fake_diffusion,\n                    inputs=gr.Slider(1, 10, 3, step=1),\n                    outputs=\"image\")\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/fake_diffusion'>\u003C/gradio-app>\n\nNote that we've added a `time.sleep(1)` in the iterator to create an artificial pause between steps so that you are able to observe the steps of the iterator (in a real image generation model, this probably wouldn't be necessary).\n\n## Streaming inputs\n\nSimilarly, Gradio can handle streaming inputs, e.g. a live audio stream that can gets transcribed to text in real time, or an image generation model that reruns every time a user types a letter in a textbox. This is covered in more details in our guide on building [reactive Interfaces](/guides/reactive-interfaces). \n\n## Alert modals\n\nYou may wish to raise alerts to the user. To do so, raise a `gr.Error(\"custom message\")` to display an error message. You can also issue `gr.Warning(\"message\")` and `gr.Info(\"message\")` by having them as standalone lines in your function, which will immediately display modals while continuing the execution of your function. Queueing needs to be enabled for this to work.\n\nNote below how the `gr.Error` has to be raised, while the `gr.Warning` and `gr.Info` are single lines.\n\n```python\ndef start_process(name):\n    gr.Info(\"Starting process\")\n    if name is None:\n        gr.Warning(\"Name is empty\")\n    ...\n    if success == False:\n        raise gr.Error(\"Process failed\")\n```\n\n\n\n## Styling\n\nGradio themes are the easiest way to customize the look and feel of your app. You can choose from a variety of themes, or create your own. To do so, pass the `theme=` kwarg to the `Interface` constructor. For example:\n\n```python\ndemo = gr.Interface(..., theme=gr.themes.Monochrome())\n```\n\nGradio comes with a set of prebuilt themes which you can load from `gr.themes.*`. You can extend these themes or create your own themes from scratch - see the [theming guide](https://gradio.app/guides/theming-guide) for more details.\n\nFor additional styling ability, you can pass any CSS (as well as custom JavaScript) to your Gradio application. This is discussed in more detail in our [custom JS and CSS guide](/guides/custom-CSS-and-JS).\n\n\n## Progress bars\n\nGradio supports the ability to create custom Progress Bars so that you have customizability and control over the progress update that you show to the user. In order to enable this, simply add an argument to your method that has a default value of a `gr.Progress` instance. Then you can update the progress levels by calling this instance directly with a float between 0 and 1, or using the `tqdm()` method of the `Progress` instance to track progress over an iterable, as shown below.\n\n```python\nimport gradio as gr\nimport time\n\ndef slowly_reverse(word, progress=gr.Progress()):\n    progress(0, desc=\"Starting\")\n    time.sleep(1)\n    progress(0.05)\n    new_string = \"\"\n    for letter in progress.tqdm(word, desc=\"Reversing\"):\n        time.sleep(0.25)\n        new_string = letter + new_string\n    return new_string\n\ndemo = gr.Interface(slowly_reverse, gr.Text(), gr.Text())\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/progress_simple'>\u003C/gradio-app>\n\nIf you use the `tqdm` library, you can even report progress updates automatically from any `tqdm.tqdm` that already exists within your function by setting the default argument as `gr.Progress(track_tqdm=True)`!\n\n## Batch functions\n\nGradio supports the ability to pass _batch_ functions. Batch functions are just\nfunctions which take in a list of inputs and return a list of predictions.\n\nFor example, here is a batched function that takes in two lists of inputs (a list of\nwords and a list of ints), and returns a list of trimmed words as output:\n\n```py\nimport time\n\ndef trim_words(words, lens):\n    trimmed_words = []\n    time.sleep(5)\n    for w, l in zip(words, lens):\n        trimmed_words.append(w[:int(l)])\n    return [trimmed_words]\n```\n\nThe advantage of using batched functions is that if you enable queuing, the Gradio server can automatically _batch_ incoming requests and process them in parallel,\npotentially speeding up your demo. Here's what the Gradio code looks like (notice the `batch=True` and `max_batch_size=16`)\n\nWith the `gr.Interface` class:\n\n```python\ndemo = gr.Interface(\n    fn=trim_words, \n    inputs=[\"textbox\", \"number\"], \n    outputs=[\"output\"],\n    batch=True, \n    max_batch_size=16\n)\n\ndemo.launch()\n```\n\nWith the `gr.Blocks` class:\n\n```py\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        word = gr.Textbox(label=\"word\")\n        leng = gr.Number(label=\"leng\")\n        output = gr.Textbox(label=\"Output\")\n    with gr.Row():\n        run = gr.Button()\n\n    event = run.click(trim_words, [word, leng], output, batch=True, max_batch_size=16)\n\ndemo.launch()\n```\n\nIn the example above, 16 requests could be processed in parallel (for a total inference time of 5 seconds), instead of each request being processed separately (for a total\ninference time of 80 seconds). Many Hugging Face `transformers` and `diffusers` models work very naturally with Gradio's batch mode: here's [an example demo using diffusers to\ngenerate images in batches](https://github.com/gradio-app/gradio/blob/main/demo/diffusers_with_batching/run.py)\n\n\n\n",tags:[],spaces:[],url:"/guides/key-features/",contributor:null}],parent:"gradio"},image:{class:null,name:"Image",description:"Creates an image component that can be used to upload images (as an input) or display images (as an output). \u003Cbr>",tags:{demos:"sepia_filter, fake_diffusion",guides:"image-classification-in-pytorch, image-classification-in-tensorflow, image-classification-with-vision-transformers, create-your-own-friends-with-a-gan"},parameters:[{name:"value",annotation:"str | PIL.Image.Image | np.ndarray | None",doc:"A PIL Image, numpy array, path or URL for the default value that Image component is going to take. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"format",annotation:"str",doc:"Format to save image if it does not already have a valid format (e.g. if the image is being returned to the frontend as a numpy array or PIL Image).  The format should be supported by the PIL library. This parameter has no effect on SVG files.",default:"\"webp\""},{name:"height",annotation:"int | str | None",doc:"The height of the displayed image, specified in pixels if a number is passed, or in CSS units if a string is passed.",default:"None"},{name:"width",annotation:"int | str | None",doc:"The width of the displayed image, specified in pixels if a number is passed, or in CSS units if a string is passed.",default:"None"},{name:"image_mode",annotation:"Literal[('1', 'L', 'P', 'RGB', 'RGBA', 'CMYK', 'YCbCr', 'LAB', 'HSV', 'I', 'F')]",doc:"&quot;RGB&quot; if color, or &quot;L&quot; if black and white. See https://pillow.readthedocs.io/en/stable/handbook/concepts.html for other supported image modes and their meaning.",default:"\"RGB\""},{name:"sources",annotation:"list[Literal[('upload', 'webcam', 'clipboard')]] | None",doc:"List of sources for the image. &quot;upload&quot; creates a box where user can drop an image file, &quot;webcam&quot; allows user to take snapshot from their webcam, &quot;clipboard&quot; allows users to paste an image from the clipboard. If None, defaults to [&quot;upload&quot;, &quot;webcam&quot;, &quot;clipboard&quot;] if streaming is False, otherwise defaults to [&quot;webcam&quot;].",default:"None"},{name:"type",annotation:"Literal[('numpy', 'pil', 'filepath')]",doc:"The format the image is converted before being passed into the prediction function. &quot;numpy&quot; converts the image to a numpy array with shape (height, width, 3) and values from 0 to 255, &quot;pil&quot; converts the image to a PIL image object, &quot;filepath&quot; passes a str path to a temporary file containing the image. If the image is SVG, the `type` is ignored and the filepath of the SVG is returned.",default:"\"numpy\""},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"show_download_button",annotation:"bool",doc:"If True, will display button to download image.",default:"True"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"interactive",annotation:"bool | None",doc:"if True, will allow users to upload and edit an image; if False, can only be used to display images. If not provided, this is inferred based on whether the component is used as an input or output.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"streaming",annotation:"bool",doc:"If True when used in a `live` interface, will automatically stream webcam feed. Only valid is source is &#x27;webcam&#x27;.",default:"False"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"mirror_webcam",annotation:"bool",doc:"If True webcam will be mirrored. Default is True.",default:"True"},{name:"show_share_button",annotation:"bool | None",doc:"If True, will show a share icon in the corner of the component that allows user to share outputs to Hugging Face Spaces Discussions. If False, icon does not appear. If set to None (default behavior), then the icon appears if this Gradio app is launched on Spaces, but not otherwise.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"clear",description:"This listener is triggered when the user clears the Image using the X button for the component.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Image"},{fn:null,name:"change",description:"Triggered when the value of the Image changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Image"},{fn:null,name:"stream",description:"This listener is triggered when the user streams the Image.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"hidden\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Image"},{fn:null,name:"select",description:"Event listener for when the user selects or deselects the Image. Uses event data gradio.SelectData to carry `value` referring to the label of the Image, and `selected` to refer to state of the Image. See EventData documentation on how to use this event data",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Image"},{fn:null,name:"upload",description:"This listener is triggered when the user uploads a file into the Image.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Image"}],preprocess:{parameter_doc:[{name:"payload",annotation:"FileData | None",doc:"image data in the form of a FileData object"}],return_doc:{annotation:"np.ndarray | PIL.Image.Image | str | None",doc:"Passes the uploaded image as a `numpy.array`, `PIL.Image` or `str` filepath depending on `type`. For SVGs, the `type` parameter is ignored and the filepath of the SVG is returned."}},postprocess:{parameter_doc:[{name:"value",annotation:"np.ndarray | PIL.Image.Image | str | Path | None",doc:"Expects a `numpy.array`, `PIL.Image`, or `str` or `pathlib.Path` filepath to an image which is displayed."}],return_doc:{annotation:"FileData | None",doc:"Returns the image as a `FileData` object."}},string_shortcuts:[["Image","image","Uses default values"]],demos:[["sepia_filter","import numpy as np\nimport gradio as gr\n\ndef sepia(input_img):\n    sepia_filter = np.array([\n        [0.393, 0.769, 0.189], \n        [0.349, 0.686, 0.168], \n        [0.272, 0.534, 0.131]\n    ])\n    sepia_img = input_img.dot(sepia_filter.T)\n    sepia_img /= sepia_img.max()\n    return sepia_img\n\ndemo = gr.Interface(sepia, gr.Image(), \"image\")\nif __name__ == \"__main__\":\n    demo.launch()\n"],["fake_diffusion","import gradio as gr\nimport numpy as np\nimport time\n\ndef fake_diffusion(steps):\n    rng = np.random.default_rng()\n    for i in range(steps):\n        time.sleep(1)\n        image = rng.random(size=(600, 600, 3))\n        yield image\n    image = np.ones((1000,1000,3), np.uint8)\n    image[:] = [255, 124, 0]\n    yield image\n\n\ndemo = gr.Interface(fake_diffusion,\n                    inputs=gr.Slider(1, 10, 3, step=1),\n                    outputs=\"image\")\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]],guides:[{name:"image-classification-in-pytorch",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:55,pretty_name:"Image Classification In Pytorch",content:"# Image Classification in PyTorch\n\n\n\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from autonomous vehicles to medical imaging.\n\nSuch models are perfect to use with Gradio's _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like the demo on the bottom of the page.\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). We will be using a pretrained image classification model, so you should also have `torch` installed.\n\n## Step 1 ‚Äî Setting up the Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a pretrained Resnet-18 model, as it is easily downloadable from [PyTorch Hub](https://pytorch.org/hub/pytorch_vision_resnet/). You can use a different pretrained model or train your own.\n\n```python\nimport torch\n\nmodel = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()\n```\n\nBecause we will be using the model for inference, we have called the `.eval()` method.\n\n## Step 2 ‚Äî Defining a `predict` function\n\nNext, we will need to define a function that takes in the _user input_, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this [text file](https://git.io/JJkYN).\n\nIn the case of our pretrained model, it will look like this:\n\n```python\nimport requests\nfrom PIL import Image\nfrom torchvision import transforms\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef predict(inp):\n  inp = transforms.ToTensor()(inp).unsqueeze(0)\n  with torch.no_grad():\n    prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n    confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n  return confidences\n```\n\nLet's break this down. The function takes one parameter:\n\n- `inp`: the input image as a `PIL` image\n\nThen, the function converts the image to a PIL Image and then eventually a PyTorch `tensor`, passes it through the model, and returns:\n\n- `confidences`: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities\n\n## Step 3 ‚Äî Creating a Gradio Interface\n\nNow that we have our predictive function set up, we can create a Gradio Interface around it.\n\nIn this case, the input component is a drag-and-drop image component. To create this input, we use `Image(type=\"pil\")` which creates the component and handles the preprocessing to convert that to a `PIL` image.\n\nThe output component will be a `Label`, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images by constructing it as `Label(num_top_classes=3)`.\n\nFinally, we'll add one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:\n\n```python\nimport gradio as gr\n\ngr.Interface(fn=predict,\n             inputs=gr.Image(type=\"pil\"),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"lion.jpg\", \"cheetah.jpg\"]).launch()\n```\n\nThis produces the following interface, which you can try right here in your browser (try uploading your own examples!):\n\n\u003Cgradio-app space=\"gradio/pytorch-image-classifier\">\n\n\n---\n\nAnd you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n",tags:["VISION","RESNET","PYTORCH"],spaces:["https://huggingface.co/spaces/abidlabs/pytorch-image-classifier","https://huggingface.co/spaces/pytorch/ResNet","https://huggingface.co/spaces/pytorch/ResNext","https://huggingface.co/spaces/pytorch/SqueezeNet"],url:"/guides/image-classification-in-pytorch/",contributor:null},{name:"image-classification-in-tensorflow",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:56,pretty_name:"Image Classification In Tensorflow",content:"# Image Classification in TensorFlow and Keras\n\n\n\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from traffic control systems to satellite imaging.\n\nSuch models are perfect to use with Gradio's _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like the demo on the bottom of the page.\n\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). We will be using a pretrained Keras image classification model, so you should also have `tensorflow` installed.\n\n## Step 1 ‚Äî Setting up the Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a pretrained Mobile Net model, as it is easily downloadable from [Keras](https://keras.io/api/applications/mobilenet/). You can use a different pretrained model or train your own.\n\n```python\nimport tensorflow as tf\n\ninception_net = tf.keras.applications.MobileNetV2()\n```\n\nThis line automatically downloads the MobileNet model and weights using the Keras library.\n\n## Step 2 ‚Äî Defining a `predict` function\n\nNext, we will need to define a function that takes in the _user input_, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this [text file](https://git.io/JJkYN).\n\nIn the case of our pretrained model, it will look like this:\n\n```python\nimport requests\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef classify_image(inp):\n  inp = inp.reshape((-1, 224, 224, 3))\n  inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n  prediction = inception_net.predict(inp).flatten()\n  confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n  return confidences\n```\n\nLet's break this down. The function takes one parameter:\n\n- `inp`: the input image as a `numpy` array\n\nThen, the function adds a batch dimension, passes it through the model, and returns:\n\n- `confidences`: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities\n\n## Step 3 ‚Äî Creating a Gradio Interface\n\nNow that we have our predictive function set up, we can create a Gradio Interface around it.\n\nIn this case, the input component is a drag-and-drop image component. To create this input, we can use the `\"gradio.inputs.Image\"` class, which creates the component and handles the preprocessing to convert that to a numpy array. We will instantiate the class with a parameter that automatically preprocesses the input image to be 224 pixels by 224 pixels, which is the size that MobileNet expects.\n\nThe output component will be a `\"label\"`, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images.\n\nFinally, we'll add one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:\n\n```python\nimport gradio as gr\n\ngr.Interface(fn=classify_image,\n             inputs=gr.Image(shape=(224, 224)),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"banana.jpg\", \"car.jpg\"]).launch()\n```\n\nThis produces the following interface, which you can try right here in your browser (try uploading your own examples!):\n\n\u003Cgradio-app space=\"gradio/keras-image-classifier\">\n\n---\n\nAnd you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n",tags:["VISION","MOBILENET","TENSORFLOW"],spaces:["https://huggingface.co/spaces/abidlabs/keras-image-classifier"],url:"/guides/image-classification-in-tensorflow/",contributor:null},{name:"image-classification-with-vision-transformers",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:57,pretty_name:"Image Classification With Vision Transformers",content:"# Image Classification with Vision Transformers\n\n\n\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from facial recognition to manufacturing quality control.\n\nState-of-the-art image classifiers are based on the _transformers_ architectures, originally popularized for NLP tasks. Such architectures are typically called vision transformers (ViT). Such models are perfect to use with Gradio's _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in a **single line of Python**, and it will look like the demo on the bottom of the page.\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started).\n\n## Step 1 ‚Äî Choosing a Vision Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a model from the [Hugging Face Model Hub](https://huggingface.co/models?pipeline_tag=image-classification). The Hub contains thousands of models covering dozens of different machine learning tasks.\n\nExpand the Tasks category on the left sidebar and select \"Image Classification\" as our task of interest. You will then see all of the models on the Hub that are designed to classify images.\n\nAt the time of writing, the most popular one is `google/vit-base-patch16-224`, which has been trained on ImageNet images at a resolution of 224x224 pixels. We will use this model for our demo.\n\n## Step 2 ‚Äî Loading the Vision Transformer Model with Gradio\n\nWhen using a model from the Hugging Face Hub, we do not need to define the input or output components for the demo. Similarly, we do not need to be concerned with the details of preprocessing or postprocessing.\nAll of these are automatically inferred from the model tags.\n\nBesides the import statement, it only takes a single line of Python to load and launch the demo.\n\nWe use the `gr.Interface.load()` method and pass in the path to the model including the `huggingface/` to designate that it is from the Hugging Face Hub.\n\n```python\nimport gradio as gr\n\ngr.Interface.load(\n             \"huggingface/google/vit-base-patch16-224\",\n             examples=[\"alligator.jpg\", \"laptop.jpg\"]).launch()\n```\n\nNotice that we have added one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples.\n\nThis produces the following interface, which you can try right here in your browser. When you input an image, it is automatically preprocessed and sent to the Hugging Face Hub API, where it is passed through the model and returned as a human-interpretable prediction. Try uploading your own image!\n\n\u003Cgradio-app space=\"gradio/vision-transformer\">\n\n---\n\nAnd you're done! In one line of code, you have built a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n",tags:["VISION","TRANSFORMERS","HUB"],spaces:["https://huggingface.co/spaces/abidlabs/vision-transformer"],url:"/guides/image-classification-with-vision-transformers/",contributor:null},{name:"create-your-own-friends-with-a-gan",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:51,pretty_name:"Create Your Own Friends With A Gan",content:"# Create Your Own Friends with a GAN\n\n\n\n\n\n\n## Introduction\n\nIt seems that cryptocurrencies, [NFTs](https://www.nytimes.com/interactive/2022/03/18/technology/nft-guide.html), and the web3 movement are all the rage these days! Digital assets are being listed on marketplaces for astounding amounts of money, and just about every celebrity is debuting their own NFT collection. While your crypto assets [may be taxable, such as in Canada](https://www.canada.ca/en/revenue-agency/programs/about-canada-revenue-agency-cra/compliance/digital-currency/cryptocurrency-guide.html), today we'll explore some fun and tax-free ways to generate your own assortment of procedurally generated [CryptoPunks](https://www.larvalabs.com/cryptopunks).\n\nGenerative Adversarial Networks, often known just as _GANs_, are a specific class of deep-learning models that are designed to learn from an input dataset to create (_generate!_) new material that is convincingly similar to elements of the original training set. Famously, the website [thispersondoesnotexist.com](https://thispersondoesnotexist.com/) went viral with lifelike, yet synthetic, images of people generated with a model called StyleGAN2. GANs have gained traction in the machine learning world, and are now being used to generate all sorts of images, text, and even [music](https://salu133445.github.io/musegan/)!\n\nToday we'll briefly look at the high-level intuition behind GANs, and then we'll build a small demo around a pre-trained GAN to see what all the fuss is about. Here's a [peek](https://nimaboscarino-cryptopunks.hf.space) at what we're going to be putting together.\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). To use the pretrained model, also install `torch` and `torchvision`.\n\n## GANs: a very brief introduction\n\nOriginally proposed in [Goodfellow et al. 2014](https://arxiv.org/abs/1406.2661), GANs are made up of neural networks which compete with the intention of outsmarting each other. One network, known as the _generator_, is responsible for generating images. The other network, the _discriminator_, receives an image at a time from the generator along with a **real** image from the training data set. The discriminator then has to guess: which image is the fake?\n\nThe generator is constantly training to create images which are trickier for the discriminator to identify, while the discriminator raises the bar for the generator every time it correctly detects a fake. As the networks engage in this competitive (_adversarial!_) relationship, the images that get generated improve to the point where they become indistinguishable to human eyes!\n\nFor a more in-depth look at GANs, you can take a look at [this excellent post on Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/06/a-detailed-explanation-of-gan-with-implementation-using-tensorflow-and-keras/) or this [PyTorch tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html). For now, though, we'll dive into a demo!\n\n## Step 1 ‚Äî Create the Generator model\n\nTo generate new images with a GAN, you only need the generator model. There are many different architectures that the generator could use, but for this demo we'll use a pretrained GAN generator model with the following architecture:\n\n```python\nfrom torch import nn\n\nclass Generator(nn.Module):\n    # Refer to the link below for explanations about nc, nz, and ngf\n    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs\n    def __init__(self, nc=4, nz=100, ngf=64):\n        super(Generator, self).__init__()\n        self.network = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, input):\n        output = self.network(input)\n        return output\n```\n\nWe're taking the generator from [this repo by @teddykoker](https://github.com/teddykoker/cryptopunks-gan/blob/main/train.py#L90), where you can also see the original discriminator model structure.\n\nAfter instantiating the model, we'll load in the weights from the Hugging Face Hub, stored at [nateraw/cryptopunks-gan](https://huggingface.co/nateraw/cryptopunks-gan):\n\n```python\nfrom huggingface_hub import hf_hub_download\nimport torch\n\nmodel = Generator()\nweights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')\nmodel.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available\n```\n\n## Step 2 ‚Äî Defining a `predict` function\n\nThe `predict` function is the key to making Gradio work! Whatever inputs we choose through the Gradio interface will get passed through our `predict` function, which should operate on the inputs and generate outputs that we can display with Gradio output components. For GANs it's common to pass random noise into our model as the input, so we'll generate a tensor of random numbers and pass that through the model. We can then use `torchvision`'s `save_image` function to save the output of the model as a `png` file, and return the file name:\n\n```python\nfrom torchvision.utils import save_image\n\ndef predict(seed):\n    num_punks = 4\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n```\n\nWe're giving our `predict` function a `seed` parameter, so that we can fix the random tensor generation with a seed. We'll then be able to reproduce punks if we want to see them again by passing in the same seed.\n\n_Note!_ Our model needs an input tensor of dimensions 100x1x1 to do a single inference, or (BatchSize)x100x1x1 for generating a batch of images. In this demo we'll start by generating 4 punks at a time.\n\n## Step 3 ‚Äî Creating a Gradio interface\n\nAt this point you can even run the code you have with `predict(\u003CSOME_NUMBER>)`, and you'll find your freshly generated punks in your file system at `./punks.png`. To make a truly interactive demo, though, we'll build out a simple interface with Gradio. Our goals here are to:\n\n- Set a slider input so users can choose the \"seed\" value\n- Use an image component for our output to showcase the generated punks\n- Use our `predict()` to take the seed and generate the images\n\nWith `gr.Interface()`, we can define all of that with a single function call:\n\n```python\nimport gradio as gr\n\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n    ],\n    outputs=\"image\",\n).launch()\n```\n\n\n## Step 4 ‚Äî Even more punks!\n\nGenerating 4 punks at a time is a good start, but maybe we'd like to control how many we want to make each time. Adding more inputs to our Gradio interface is as simple as adding another item to the `inputs` list that we pass to `gr.Interface`:\n\n```python\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n        gr.Slider(4, 64, label='Number of Punks', step=1, default=10), # Adding another slider!\n    ],\n    outputs=\"image\",\n).launch()\n```\n\nThe new input will be passed to our `predict()` function, so we have to make some changes to that function to accept a new parameter:\n\n```python\ndef predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n```\n\nWhen you relaunch your interface, you should see a second slider that'll let you control the number of punks!\n\n## Step 5 - Polishing it up\n\nYour Gradio app is pretty much good to go, but you can add a few extra things to really make it ready for the spotlight ‚ú®\n\nWe can add some examples that users can easily try out by adding this to the `gr.Interface`:\n\n```python\ngr.Interface(\n    # ...\n    # keep everything as it is, and then add\n    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],\n).launch(cache_examples=True) # cache_examples is optional\n```\n\nThe `examples` parameter takes a list of lists, where each item in the sublists is ordered in the same order that we've listed the `inputs`. So in our case, `[seed, num_punks]`. Give it a try!\n\nYou can also try adding a `title`, `description`, and `article` to the `gr.Interface`. Each of those parameters accepts a string, so try it out and see what happens üëÄ `article` will also accept HTML, as [explored in a previous guide](/guides/key-features/#descriptive-content)!\n\nWhen you're all done, you may end up with something like [this](https://nimaboscarino-cryptopunks.hf.space).\n\nFor reference, here is our full code:\n\n```python\nimport torch\nfrom torch import nn\nfrom huggingface_hub import hf_hub_download\nfrom torchvision.utils import save_image\nimport gradio as gr\n\nclass Generator(nn.Module):\n    # Refer to the link below for explanations about nc, nz, and ngf\n    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs\n    def __init__(self, nc=4, nz=100, ngf=64):\n        super(Generator, self).__init__()\n        self.network = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, input):\n        output = self.network(input)\n        return output\n\nmodel = Generator()\nweights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')\nmodel.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available\n\ndef predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n        gr.Slider(4, 64, label='Number of Punks', step=1, default=10),\n    ],\n    outputs=\"image\",\n    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],\n).launch(cache_examples=True)\n```\n\n---\n\nCongratulations! You've built out your very own GAN-powered CryptoPunks generator, with a fancy Gradio interface that makes it easy for anyone to use. Now you can [scour the Hub for more GANs](https://huggingface.co/models?other=gan) (or train your own) and continue making even more awesome demos ü§ó\n",tags:["GAN","IMAGE","HUB"],spaces:["https://huggingface.co/spaces/NimaBoscarino/cryptopunks","https://huggingface.co/spaces/nateraw/cryptopunks-generator"],url:"/guides/create-your-own-friends-with-a-gan/",contributor:"\u003Ca href=\"https://huggingface.co/NimaBoscarino\">Nima Boscarino\u003C/a> and \u003Ca href=\"https://huggingface.co/nateraw\">Nate Raw\u003C/a>"}],parent:"gradio"},imageeditor:{class:null,name:"ImageEditor",description:"Creates an image component that, as an input, can be used to upload and edit images using simple editing tools such as brushes, strokes, cropping, and layers. Or, as an output, this component can be used to display images. \u003Cbr>",tags:{demos:"image_editor"},parameters:[{name:"value",annotation:"EditorValue | ImageType | None",doc:"Optional initial image(s) to populate the image editor. Should be a dictionary with keys: `background`, `layers`, and `composite`. The values corresponding to `background` and `composite` should be images or None, while `layers` should be a list of images. Images can be of type PIL.Image, np.array, or str filepath/URL. Or, the value can be a callable, in which case the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"height",annotation:"int | str | None",doc:"The height of the component container, specified in pixels if a number is passed, or in CSS units if a string is passed.",default:"None"},{name:"width",annotation:"int | str | None",doc:"The width of the component container, specified in pixels if a number is passed, or in CSS units if a string is passed.",default:"None"},{name:"image_mode",annotation:"Literal[('1', 'L', 'P', 'RGB', 'RGBA', 'CMYK', 'YCbCr', 'LAB', 'HSV', 'I', 'F')]",doc:"&quot;RGB&quot; if color, or &quot;L&quot; if black and white. See https://pillow.readthedocs.io/en/stable/handbook/concepts.html for other supported image modes and their meaning.",default:"\"RGBA\""},{name:"sources",annotation:"Iterable[Literal[('upload', 'webcam', 'clipboard')]] | None",doc:"List of sources that can be used to set the background image. &quot;upload&quot; creates a box where user can drop an image file, &quot;webcam&quot; allows user to take snapshot from their webcam, &quot;clipboard&quot; allows users to paste an image from the clipboard.",default:"('upload', 'webcam', 'clipboard')"},{name:"type",annotation:"Literal[('numpy', 'pil', 'filepath')]",doc:"The format the images are converted to before being passed into the prediction function. &quot;numpy&quot; converts the images to numpy arrays with shape (height, width, 3) and values from 0 to 255, &quot;pil&quot; converts the images to PIL image objects, &quot;filepath&quot; passes images as str filepaths to temporary copies of the images.",default:"\"numpy\""},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"show_download_button",annotation:"bool",doc:"If True, will display button to download image.",default:"True"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"interactive",annotation:"bool | None",doc:"if True, will allow users to upload and edit an image; if False, can only be used to display images. If not provided, this is inferred based on whether the component is used as an input or output.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"mirror_webcam",annotation:"bool",doc:"If True webcam will be mirrored. Default is True.",default:"True"},{name:"show_share_button",annotation:"bool | None",doc:"If True, will show a share icon in the corner of the component that allows user to share outputs to Hugging Face Spaces Discussions. If False, icon does not appear. If set to None (default behavior), then the icon appears if this Gradio app is launched on Spaces, but not otherwise.",default:"None"},{name:"crop_size",annotation:"tuple[int | float, int | float] | str | None",doc:"The size of the crop box in pixels. If a tuple, the first value is the width and the second value is the height. If a string, the value must be a ratio in the form `width:height` (e.g. &quot;16:9&quot;).",default:"None"},{name:"transforms",annotation:"Iterable[Literal['crop']]",doc:"The transforms tools to make available to users. &quot;crop&quot; allows the user to crop the image.",default:"('crop',)"},{name:"eraser",annotation:"Eraser | None | Literal[False]",doc:"The options for the eraser tool in the image editor. Should be an instance of the `gr.Eraser` class, or None to use the default settings. Can also be False to hide the eraser tool.",default:"None"},{name:"brush",annotation:"Brush | None | Literal[False]",doc:"The options for the brush tool in the image editor. Should be an instance of the `gr.Brush` class, or None to use the default settings. Can also be False to hide the brush tool, which will also hide the eraser tool.",default:"None"},{name:"format",annotation:"str",doc:"Format to save image if it does not already have a valid format (e.g. if the image is being returned to the frontend as a numpy array or PIL Image).  The format should be supported by the PIL library. This parameter has no effect on SVG files.",default:"\"webp\""},{name:"layers",annotation:"bool",doc:"If True, will allow users to add layers to the image. If False, the layers option will be hidden.",default:"True"},{name:"canvas_size",annotation:"tuple[int, int] | None",doc:"The size of the default canvas in pixels. If a tuple, the first value is the width and the second value is the height. If None, the canvas size will be the same as the background image or 800 x 600 if no background image is provided.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"clear",description:"This listener is triggered when the user clears the ImageEditor using the X button for the component.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.ImageEditor"},{fn:null,name:"change",description:"Triggered when the value of the ImageEditor changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.ImageEditor"},{fn:null,name:"input",description:"This listener is triggered when the user changes the value of the ImageEditor.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.ImageEditor"},{fn:null,name:"select",description:"Event listener for when the user selects or deselects the ImageEditor. Uses event data gradio.SelectData to carry `value` referring to the label of the ImageEditor, and `selected` to refer to state of the ImageEditor. See EventData documentation on how to use this event data",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.ImageEditor"},{fn:null,name:"upload",description:"This listener is triggered when the user uploads a file into the ImageEditor.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.ImageEditor"},{fn:null,name:"apply",description:"This listener is triggered when the user applies changes to the ImageEditor through an integrated UI action.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.ImageEditor"}],preprocess:{parameter_doc:[{name:"payload",annotation:"EditorData | None",doc:"An instance of `EditorData` consisting of the background image, layers, and composite image."}],return_doc:{annotation:"EditorValue | None",doc:"Passes the uploaded images as an instance of EditorValue, which is just a `dict` with keys: 'background', 'layers', and 'composite'. The values corresponding to 'background' and 'composite' are images, while 'layers' is a `list` of images. The images are of type `PIL.Image`, `np.array`, or `str` filepath, depending on the `type` parameter."}},postprocess:{parameter_doc:[{name:"value",annotation:"EditorValue | ImageType | None",doc:"Expects a EditorValue, which is just a dictionary with keys: 'background', 'layers', and 'composite'. The values corresponding to 'background' and 'composite' should be images or None, while `layers` should be a list of images. Images can be of type `PIL.Image`, `np.array`, or `str` filepath/URL. Or, the value can be simply a single image (`ImageType`), in which case it will be used as the background."}],return_doc:{annotation:"EditorData | None",doc:"An instance of `EditorData` consisting of the background image, layers, and composite image."}},string_shortcuts:[["ImageEditor","imageeditor","Uses default values"],["Sketchpad","sketchpad","Uses sources=(), brush=Brush(colors=[\"#000000\"], color_mode=\"fixed\")"],["Paint","paint","Uses sources=()"],["ImageMask","imagemask","Uses brush=Brush(colors=[\"#000000\"], color_mode=\"fixed\")"]],demos:[["image_editor","import gradio as gr\nimport time\n\n\ndef sleep(im):\n    time.sleep(5)\n    return [im[\"background\"], im[\"layers\"][0], im[\"layers\"][1], im[\"composite\"]]\n\n\ndef predict(im):\n    return im[\"composite\"]\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        im = gr.ImageEditor(\n            type=\"numpy\",\n            crop_size=\"1:1\",\n        )\n        im_preview = gr.Image()\n    n_upload = gr.Number(0, label=\"Number of upload events\", step=1)\n    n_change = gr.Number(0, label=\"Number of change events\", step=1)\n    n_input = gr.Number(0, label=\"Number of input events\", step=1)\n\n    im.upload(lambda x: x + 1, outputs=n_upload, inputs=n_upload)\n    im.change(lambda x: x + 1, outputs=n_change, inputs=n_change)\n    im.input(lambda x: x + 1, outputs=n_input, inputs=n_input)\n    im.change(predict, outputs=im_preview, inputs=im, show_progress=\"hidden\")\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]],parent:"gradio"},json:{class:null,name:"JSON",description:"Used to display arbitrary JSON output prettily. As this component does not accept user input, it is rarely used as an input component. \u003Cbr>",tags:{demos:"zip_to_json, blocks_xray"},parameters:[{name:"value",annotation:"str | dict | list | Callable | None",doc:"Default value as a valid JSON `str` -- or a `list` or `dict` that can be serialized to a JSON string. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the JSON changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.JSON"}],preprocess:{parameter_doc:[{name:"payload",annotation:"dict | list | None",doc:"JSON value as a `dict` or `list`"}],return_doc:{annotation:"dict | list | None",doc:"Passes the JSON value as a `dict` or `list` depending on the value."}},postprocess:{parameter_doc:[{name:"value",annotation:"dict | list | str | None",doc:"Expects a valid JSON `str` -- or a `list` or `dict` that can be serialized to a JSON string. The `list` or `dict` value can contain numpy arrays."}],return_doc:{annotation:"JsonData | None",doc:"Returns the JSON as a `list` or `dict`."}},string_shortcuts:[["JSON","json","Uses default values"]],demos:[["zip_to_json","from zipfile import ZipFile\n\nimport gradio as gr\n\n\ndef zip_to_json(file_obj):\n    files = []\n    with ZipFile(file_obj.name) as zfile:\n        for zinfo in zfile.infolist():\n            files.append(\n                {\n                    \"name\": zinfo.filename,\n                    \"file_size\": zinfo.file_size,\n                    \"compressed_size\": zinfo.compress_size,\n                }\n            )\n    return files\n\n\ndemo = gr.Interface(zip_to_json, \"file\", \"json\")\n\nif __name__ == \"__main__\":\n    demo.launch()\n"],["blocks_xray","import gradio as gr\nimport time\n\ndisease_values = [0.25, 0.5, 0.75]\n\ndef xray_model(diseases, img):\n    return [{disease: disease_values[idx] for idx,disease in enumerate(diseases)}]\n\n\ndef ct_model(diseases, img):\n    return [{disease: 0.1 for disease in diseases}]\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\n        \"\"\"\n# Detect Disease From Scan\nWith this model you can lorem ipsum\n- ipsum 1\n- ipsum 2\n\"\"\"\n    )\n    gr.DuplicateButton()\n    disease = gr.CheckboxGroup(\n        info=\"Select the diseases you want to scan for.\",\n        choices=[\"Covid\", \"Malaria\", \"Lung Cancer\"], label=\"Disease to Scan For\"\n    )\n    slider = gr.Slider(0, 100)\n\n    with gr.Tab(\"X-ray\") as x_tab:\n        with gr.Row():\n            xray_scan = gr.Image()\n            xray_results = gr.JSON()\n        xray_run = gr.Button(\"Run\")\n        xray_run.click(\n            xray_model,\n            inputs=[disease, xray_scan],\n            outputs=xray_results,\n            api_name=\"xray_model\"\n        )\n\n    with gr.Tab(\"CT Scan\"):\n        with gr.Row():\n            ct_scan = gr.Image()\n            ct_results = gr.JSON()\n        ct_run = gr.Button(\"Run\")\n        ct_run.click(\n            ct_model,\n            inputs=[disease, ct_scan],\n            outputs=ct_results,\n            api_name=\"ct_model\"\n        )\n\n    upload_btn = gr.Button(\"Upload Results\", variant=\"primary\")\n    upload_btn.click(\n        lambda ct, xr: None,\n        inputs=[ct_results, xray_results],\n        outputs=[],\n    )\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]],parent:"gradio"},label:{class:null,name:"Label",description:"Displays a classification label, along with confidence scores of top categories, if provided. As this component does not accept user input, it is rarely used as an input component. \u003Cbr>",tags:{guides:"image-classification-in-pytorch, image-classification-in-tensorflow, image-classification-with-vision-transformers"},parameters:[{name:"value",annotation:"dict[str, float] | str | float | Callable | None",doc:"Default value to show in the component. If a str or number is provided, simply displays the string or number. If a {Dict[str, float]} of classes and confidences is provided, displays the top class on top and the `num_top_classes` below, along with their confidence bars. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"num_top_classes",annotation:"int | None",doc:"number of most confident classes to show.",default:"None"},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"color",annotation:"str | None",doc:"The background color of the label (either a valid css color name or hexadecimal string).",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the Label changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Label"},{fn:null,name:"select",description:"Event listener for when the user selects or deselects the Label. Uses event data gradio.SelectData to carry `value` referring to the label of the Label, and `selected` to refer to state of the Label. See EventData documentation on how to use this event data",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Label"}],preprocess:{parameter_doc:[{name:"payload",annotation:"LabelData | None",doc:"An instance of `LabelData` containing the label and confidences."}],return_doc:{annotation:"dict[str, float] | str | int | float | None",doc:"Depending on the value, passes the label as a `str | int | float`, or the labels and confidences as a `dict[str, float]`."}},postprocess:{parameter_doc:[{name:"value",annotation:"dict[str, float] | str | int | float | None",doc:"Expects a `dict[str, float]` of classes and confidences, or `str` with just the class or an `int | float` for regression outputs, or a `str` path to a .json file containing a json dictionary in one of the preceding formats."}],return_doc:{annotation:"LabelData | dict | None",doc:"Returns a `LabelData` object with the label and confidences, or a `dict` of the same format, or a `str` or `int` or `float` if the input was a single label."}},string_shortcuts:[["Label","label","Uses default values"]],guides:[{name:"image-classification-in-pytorch",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:55,pretty_name:"Image Classification In Pytorch",content:"# Image Classification in PyTorch\n\n\n\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from autonomous vehicles to medical imaging.\n\nSuch models are perfect to use with Gradio's _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like the demo on the bottom of the page.\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). We will be using a pretrained image classification model, so you should also have `torch` installed.\n\n## Step 1 ‚Äî Setting up the Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a pretrained Resnet-18 model, as it is easily downloadable from [PyTorch Hub](https://pytorch.org/hub/pytorch_vision_resnet/). You can use a different pretrained model or train your own.\n\n```python\nimport torch\n\nmodel = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()\n```\n\nBecause we will be using the model for inference, we have called the `.eval()` method.\n\n## Step 2 ‚Äî Defining a `predict` function\n\nNext, we will need to define a function that takes in the _user input_, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this [text file](https://git.io/JJkYN).\n\nIn the case of our pretrained model, it will look like this:\n\n```python\nimport requests\nfrom PIL import Image\nfrom torchvision import transforms\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef predict(inp):\n  inp = transforms.ToTensor()(inp).unsqueeze(0)\n  with torch.no_grad():\n    prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n    confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n  return confidences\n```\n\nLet's break this down. The function takes one parameter:\n\n- `inp`: the input image as a `PIL` image\n\nThen, the function converts the image to a PIL Image and then eventually a PyTorch `tensor`, passes it through the model, and returns:\n\n- `confidences`: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities\n\n## Step 3 ‚Äî Creating a Gradio Interface\n\nNow that we have our predictive function set up, we can create a Gradio Interface around it.\n\nIn this case, the input component is a drag-and-drop image component. To create this input, we use `Image(type=\"pil\")` which creates the component and handles the preprocessing to convert that to a `PIL` image.\n\nThe output component will be a `Label`, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images by constructing it as `Label(num_top_classes=3)`.\n\nFinally, we'll add one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:\n\n```python\nimport gradio as gr\n\ngr.Interface(fn=predict,\n             inputs=gr.Image(type=\"pil\"),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"lion.jpg\", \"cheetah.jpg\"]).launch()\n```\n\nThis produces the following interface, which you can try right here in your browser (try uploading your own examples!):\n\n\u003Cgradio-app space=\"gradio/pytorch-image-classifier\">\n\n\n---\n\nAnd you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n",tags:["VISION","RESNET","PYTORCH"],spaces:["https://huggingface.co/spaces/abidlabs/pytorch-image-classifier","https://huggingface.co/spaces/pytorch/ResNet","https://huggingface.co/spaces/pytorch/ResNext","https://huggingface.co/spaces/pytorch/SqueezeNet"],url:"/guides/image-classification-in-pytorch/",contributor:null},{name:"image-classification-in-tensorflow",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:56,pretty_name:"Image Classification In Tensorflow",content:"# Image Classification in TensorFlow and Keras\n\n\n\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from traffic control systems to satellite imaging.\n\nSuch models are perfect to use with Gradio's _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like the demo on the bottom of the page.\n\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). We will be using a pretrained Keras image classification model, so you should also have `tensorflow` installed.\n\n## Step 1 ‚Äî Setting up the Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a pretrained Mobile Net model, as it is easily downloadable from [Keras](https://keras.io/api/applications/mobilenet/). You can use a different pretrained model or train your own.\n\n```python\nimport tensorflow as tf\n\ninception_net = tf.keras.applications.MobileNetV2()\n```\n\nThis line automatically downloads the MobileNet model and weights using the Keras library.\n\n## Step 2 ‚Äî Defining a `predict` function\n\nNext, we will need to define a function that takes in the _user input_, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this [text file](https://git.io/JJkYN).\n\nIn the case of our pretrained model, it will look like this:\n\n```python\nimport requests\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef classify_image(inp):\n  inp = inp.reshape((-1, 224, 224, 3))\n  inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n  prediction = inception_net.predict(inp).flatten()\n  confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n  return confidences\n```\n\nLet's break this down. The function takes one parameter:\n\n- `inp`: the input image as a `numpy` array\n\nThen, the function adds a batch dimension, passes it through the model, and returns:\n\n- `confidences`: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities\n\n## Step 3 ‚Äî Creating a Gradio Interface\n\nNow that we have our predictive function set up, we can create a Gradio Interface around it.\n\nIn this case, the input component is a drag-and-drop image component. To create this input, we can use the `\"gradio.inputs.Image\"` class, which creates the component and handles the preprocessing to convert that to a numpy array. We will instantiate the class with a parameter that automatically preprocesses the input image to be 224 pixels by 224 pixels, which is the size that MobileNet expects.\n\nThe output component will be a `\"label\"`, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images.\n\nFinally, we'll add one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:\n\n```python\nimport gradio as gr\n\ngr.Interface(fn=classify_image,\n             inputs=gr.Image(shape=(224, 224)),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"banana.jpg\", \"car.jpg\"]).launch()\n```\n\nThis produces the following interface, which you can try right here in your browser (try uploading your own examples!):\n\n\u003Cgradio-app space=\"gradio/keras-image-classifier\">\n\n---\n\nAnd you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n",tags:["VISION","MOBILENET","TENSORFLOW"],spaces:["https://huggingface.co/spaces/abidlabs/keras-image-classifier"],url:"/guides/image-classification-in-tensorflow/",contributor:null},{name:"image-classification-with-vision-transformers",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:57,pretty_name:"Image Classification With Vision Transformers",content:"# Image Classification with Vision Transformers\n\n\n\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from facial recognition to manufacturing quality control.\n\nState-of-the-art image classifiers are based on the _transformers_ architectures, originally popularized for NLP tasks. Such architectures are typically called vision transformers (ViT). Such models are perfect to use with Gradio's _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in a **single line of Python**, and it will look like the demo on the bottom of the page.\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started).\n\n## Step 1 ‚Äî Choosing a Vision Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a model from the [Hugging Face Model Hub](https://huggingface.co/models?pipeline_tag=image-classification). The Hub contains thousands of models covering dozens of different machine learning tasks.\n\nExpand the Tasks category on the left sidebar and select \"Image Classification\" as our task of interest. You will then see all of the models on the Hub that are designed to classify images.\n\nAt the time of writing, the most popular one is `google/vit-base-patch16-224`, which has been trained on ImageNet images at a resolution of 224x224 pixels. We will use this model for our demo.\n\n## Step 2 ‚Äî Loading the Vision Transformer Model with Gradio\n\nWhen using a model from the Hugging Face Hub, we do not need to define the input or output components for the demo. Similarly, we do not need to be concerned with the details of preprocessing or postprocessing.\nAll of these are automatically inferred from the model tags.\n\nBesides the import statement, it only takes a single line of Python to load and launch the demo.\n\nWe use the `gr.Interface.load()` method and pass in the path to the model including the `huggingface/` to designate that it is from the Hugging Face Hub.\n\n```python\nimport gradio as gr\n\ngr.Interface.load(\n             \"huggingface/google/vit-base-patch16-224\",\n             examples=[\"alligator.jpg\", \"laptop.jpg\"]).launch()\n```\n\nNotice that we have added one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples.\n\nThis produces the following interface, which you can try right here in your browser. When you input an image, it is automatically preprocessed and sent to the Hugging Face Hub API, where it is passed through the model and returned as a human-interpretable prediction. Try uploading your own image!\n\n\u003Cgradio-app space=\"gradio/vision-transformer\">\n\n---\n\nAnd you're done! In one line of code, you have built a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n",tags:["VISION","TRANSFORMERS","HUB"],spaces:["https://huggingface.co/spaces/abidlabs/vision-transformer"],url:"/guides/image-classification-with-vision-transformers/",contributor:null}],parent:"gradio"},lineplot:{class:null,name:"LinePlot",description:"Creates a line plot component to display data from a pandas DataFrame (as output). As this component does not accept user input, it is rarely used as an input component. \u003Cbr>",tags:{demos:"live_dashboard"},parameters:[{name:"value",annotation:"pd.DataFrame | Callable | None",doc:"The pandas dataframe containing the data to display in a scatter plot.",default:"None"},{name:"x",annotation:"str | None",doc:"Column corresponding to the x axis.",default:"None"},{name:"y",annotation:"str | None",doc:"Column corresponding to the y axis.",default:"None"},{name:"color",annotation:"str | None",doc:"The column to determine the point color. If the column contains numeric data, gradio will interpolate the column data so that small values correspond to light colors and large values correspond to dark values.",default:"None"},{name:"stroke_dash",annotation:"str | None",doc:"The column to determine the symbol used to draw the line, e.g. dashed lines, dashed lines with points.",default:"None"},{name:"overlay_point",annotation:"bool | None",doc:"Whether to draw a point on the line for each (x, y) coordinate pair.",default:"None"},{name:"title",annotation:"str | None",doc:"The title to display on top of the chart.",default:"None"},{name:"tooltip",annotation:"list[str] | str | None",doc:"The column (or list of columns) to display on the tooltip when a user hovers a point on the plot.",default:"None"},{name:"x_title",annotation:"str | None",doc:"The title given to the x axis. By default, uses the value of the x parameter.",default:"None"},{name:"y_title",annotation:"str | None",doc:"The title given to the y axis. By default, uses the value of the y parameter.",default:"None"},{name:"x_label_angle",annotation:"float | None",doc:"The angle for the x axis labels. Positive values are clockwise, and negative values are counter-clockwise.",default:"None"},{name:"y_label_angle",annotation:"float | None",doc:"The angle for the y axis labels. Positive values are clockwise, and negative values are counter-clockwise.",default:"None"},{name:"color_legend_title",annotation:"str | None",doc:"The title given to the color legend. By default, uses the value of color parameter.",default:"None"},{name:"stroke_dash_legend_title",annotation:"str | None",doc:"The title given to the stroke_dash legend. By default, uses the value of the stroke_dash parameter.",default:"None"},{name:"color_legend_position",annotation:"Literal[('left', 'right', 'top', 'bottom', 'top-left', 'top-right', 'bottom-left', 'bottom-right', 'none')] | None",doc:"The position of the color legend. If the string value &#x27;none&#x27; is passed, this legend is omitted. For other valid position values see: https://vega.github.io/vega/docs/legends/#orientation.",default:"None"},{name:"stroke_dash_legend_position",annotation:"Literal[('left', 'right', 'top', 'bottom', 'top-left', 'top-right', 'bottom-left', 'bottom-right', 'none')] | None",doc:"The position of the stoke_dash legend. If the string value &#x27;none&#x27; is passed, this legend is omitted. For other valid position values see: https://vega.github.io/vega/docs/legends/#orientation.",default:"None"},{name:"height",annotation:"int | str | None",doc:"The height of the plot, specified in pixels if a number is passed, or in CSS units if a string is passed.",default:"None"},{name:"width",annotation:"int | str | None",doc:"The width of the plot, specified in pixels if a number is passed, or in CSS units if a string is passed.",default:"None"},{name:"x_lim",annotation:"list[int] | None",doc:"A tuple or list containing the limits for the x-axis, specified as [x_min, x_max].",default:"None"},{name:"y_lim",annotation:"list[int] | None",doc:"A tuple of list containing the limits for the y-axis, specified as [y_min, y_max].",default:"None"},{name:"caption",annotation:"str | None",doc:"The (optional) caption to display below the plot.",default:"None"},{name:"interactive",annotation:"bool | None",doc:"Whether users should be able to interact with the plot by panning or zooming with their mouse or trackpad.",default:"True"},{name:"label",annotation:"str | None",doc:"The (optional) label to display on the top left corner of the plot.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"Whether the label should be displayed.",default:"None"},{name:"container",annotation:"bool",doc:null,default:"True"},{name:"scale",annotation:"int | None",doc:null,default:"None"},{name:"min_width",annotation:"int",doc:null,default:"160"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"visible",annotation:"bool",doc:"Whether the plot should be visible.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"show_actions_button",annotation:"bool",doc:"Whether to show the actions button on the top right corner of the plot.",default:"False"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the Plot changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.LinePlot"},{fn:null,name:"clear",description:"This listener is triggered when the user clears the Plot using the X button for the component.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.LinePlot"}],preprocess:{parameter_doc:[{name:"payload",annotation:"AltairPlotData | None",doc:"The data to display in a line plot."}],return_doc:{annotation:"AltairPlotData | None",doc:"(Rarely used) passes the data displayed in the line plot as an AltairPlotData dataclass, which includes the plot information as a JSON string, as well as the type of plot (in this case, \"line\")."}},postprocess:{parameter_doc:[{name:"value",annotation:"pd.DataFrame | dict | None",doc:"Expects a pandas DataFrame containing the data to display in the line plot. The DataFrame should contain at least two columns, one for the x-axis (corresponding to this component's `x` argument) and one for the y-axis (corresponding to `y`)."}],return_doc:{annotation:"AltairPlotData | dict | None",doc:"The data to display in a line plot, in the form of an AltairPlotData dataclass, which includes the plot information as a JSON string, as well as the type of plot (in this case, \"line\")."}},string_shortcuts:[["LinePlot","lineplot","Uses default values"]],demos:[["live_dashboard","import math\n\nimport pandas as pd\n\nimport gradio as gr\nimport datetime\nimport numpy as np\n\n\ndef get_time():\n    return datetime.datetime.now()\n\n\nplot_end = 2 * math.pi\n\n\ndef get_plot(period=1):\n    global plot_end\n    x = np.arange(plot_end - 2 * math.pi, plot_end, 0.02)\n    y = np.sin(2 * math.pi * period * x)\n    update = gr.LinePlot(\n        value=pd.DataFrame({\"x\": x, \"y\": y}),\n        x=\"x\",\n        y=\"y\",\n        title=\"Plot (updates every second)\",\n        width=600,\n        height=350,\n    )\n    plot_end += 2 * math.pi\n    if plot_end > 1000:\n        plot_end = 2 * math.pi\n    return update\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            c_time2 = gr.Textbox(label=\"Current Time refreshed every second\")\n            gr.Textbox(\n                \"Change the value of the slider to automatically update the plot\",\n                label=\"\",\n            )\n            period = gr.Slider(\n                label=\"Period of plot\", value=1, minimum=0, maximum=10, step=1\n            )\n            plot = gr.LinePlot(show_label=False)\n        with gr.Column():\n            name = gr.Textbox(label=\"Enter your name\")\n            greeting = gr.Textbox(label=\"Greeting\")\n            button = gr.Button(value=\"Greet\")\n            button.click(lambda s: f\"Hello {s}\", name, greeting)\n\n    demo.load(lambda: datetime.datetime.now(), None, c_time2, every=1)\n    dep = demo.load(get_plot, None, plot, every=1)\n    period.change(get_plot, period, plot, every=1, cancels=[dep])\n\nif __name__ == \"__main__\":\n    demo.queue().launch()\n"]],parent:"gradio"},loginbutton:{class:null,name:"LoginButton",description:"Creates a button that redirects the user to Sign with Hugging Face using OAuth.",tags:{},parameters:[{name:"value",annotation:"str",doc:null,default:"\"Sign in with Hugging Face\""},{name:"logout_value",annotation:"str",doc:"The text to display when the user is signed in. The string should contain a placeholder for the username with a call-to-action to logout, e.g. &quot;Logout ({})&quot;.",default:"\"Logout ({})\""},{name:"every",annotation:"float | None",doc:null,default:"None"},{name:"variant",annotation:"Literal[('primary', 'secondary', 'stop')]",doc:null,default:"\"secondary\""},{name:"size",annotation:"Literal[('sm', 'lg')] | None",doc:null,default:"None"},{name:"icon",annotation:"str | None",doc:null,default:"\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\""},{name:"link",annotation:"str | None",doc:null,default:"None"},{name:"visible",annotation:"bool",doc:null,default:"True"},{name:"interactive",annotation:"bool",doc:null,default:"True"},{name:"elem_id",annotation:"str | None",doc:null,default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:null,default:"None"},{name:"render",annotation:"bool",doc:null,default:"True"},{name:"key",annotation:"int | str | None",doc:null,default:"None"},{name:"scale",annotation:"int | None",doc:null,default:"0"},{name:"min_width",annotation:"int | None",doc:null,default:"None"},{name:"signed_in_value",annotation:"str",doc:null,default:"\"Signed in as {}\""}],returns:{annotation:null},example:null,fns:[{fn:null,name:"click",description:"Triggered when the Button is clicked.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.LoginButton"}],preprocess:{parameter_doc:[{name:"payload",annotation:"str | None",doc:"string corresponding to the button label"}],return_doc:{annotation:"str | None",doc:"(Rarely used) the `str` corresponding to the button label when the button is clicked"}},postprocess:{parameter_doc:[{name:"value",annotation:"str | None",doc:"string corresponding to the button label"}],return_doc:{annotation:"str | None",doc:"Expects a `str` value that is set as the button label"}},string_shortcuts:[["LoginButton","loginbutton","Uses default values"]],parent:"gradio"},logoutbutton:{class:null,name:"LogoutButton",description:"Creates a Button to log out a user from a Space using OAuth. \u003Cbr>       which handles both the login and logout processes.",tags:{note:"`LogoutButton` component is deprecated. Please use `gr.LoginButton` instead"},parameters:[{name:"value",annotation:"str",doc:"Default text for the button to display. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"\"Logout\""},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"variant",annotation:"Literal[('primary', 'secondary', 'stop')]",doc:"&#x27;primary&#x27; for main call-to-action, &#x27;secondary&#x27; for a more subdued style, &#x27;stop&#x27; for a stop button.",default:"\"secondary\""},{name:"size",annotation:"Literal[('sm', 'lg')] | None",doc:"Size of the button. Can be &quot;sm&quot; or &quot;lg&quot;.",default:"None"},{name:"icon",annotation:"str | None",doc:"URL or path to the icon file to display within the button. If None, no icon will be displayed.",default:"\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\""},{name:"link",annotation:"str | None",doc:"URL to open when the button is clicked. If None, no link will be used.",default:"\"/logout\""},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"interactive",annotation:"bool",doc:"If False, the Button will be in a disabled state.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"0"},{name:"min_width",annotation:"int | None",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"click",description:"Triggered when the Button is clicked.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.LogoutButton"}],preprocess:{parameter_doc:[{name:"payload",annotation:"str | None",doc:"string corresponding to the button label"}],return_doc:{annotation:"str | None",doc:"(Rarely used) the `str` corresponding to the button label when the button is clicked"}},postprocess:{parameter_doc:[{name:"value",annotation:"str | None",doc:"string corresponding to the button label"}],return_doc:{annotation:"str | None",doc:"Expects a `str` value that is set as the button label"}},string_shortcuts:[["LogoutButton","logoutbutton","Uses default values"]],parent:"gradio"},markdown:{class:null,name:"Markdown",description:"Used to render arbitrary Markdown output. Can also render latex enclosed by dollar signs. As this component does not accept user input, it is rarely used as an input component. \u003Cbr>",tags:{demos:"blocks_hello, blocks_kinematics",guides:"key-features"},parameters:[{name:"value",annotation:"str | Callable | None",doc:"Value to show in Markdown component. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"label",annotation:"str | None",doc:"The label for this component. Is used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"This parameter has no effect.",default:"None"},{name:"rtl",annotation:"bool",doc:"If True, sets the direction of the rendered text to right-to-left. Default is False, which renders text left-to-right.",default:"False"},{name:"latex_delimiters",annotation:"list[dict[str, str | bool]] | None",doc:"A list of dicts of the form {&quot;left&quot;: open delimiter (str), &quot;right&quot;: close delimiter (str), &quot;display&quot;: whether to display in newline (bool)} that will be used to render LaTeX expressions. If not provided, `latex_delimiters` is set to `[{ &quot;left&quot;: &quot;$$&quot;, &quot;right&quot;: &quot;$$&quot;, &quot;display&quot;: True }]`, so only expressions enclosed in $$ delimiters will be rendered as LaTeX, and in a new line. Pass in an empty list to disable LaTeX rendering. For more information, see the [KaTeX documentation](https://katex.org/docs/autorender.html).",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"sanitize_html",annotation:"bool",doc:"If False, will disable HTML sanitization when converted from markdown. This is not recommended, as it can lead to security vulnerabilities.",default:"True"},{name:"line_breaks",annotation:"bool",doc:"If True, will enable Github-flavored Markdown line breaks in chatbot messages. If False (default), single new lines will be ignored.",default:"False"},{name:"header_links",annotation:"bool",doc:"If True, will automatically create anchors for headings, displaying a link icon on hover.",default:"False"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the Markdown changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Markdown"}],preprocess:{parameter_doc:[{name:"payload",annotation:"str | None",doc:"the `str` of Markdown corresponding to the displayed value."}],return_doc:{annotation:"str | None",doc:"Passes the `str` of Markdown corresponding to the displayed value."}},postprocess:{parameter_doc:[{name:"value",annotation:"str | None",doc:"Expects a valid `str` that can be rendered as Markdown."}],return_doc:{annotation:"str | None",doc:"The same `str` as the input, but with leading and trailing whitespace removed."}},string_shortcuts:[["Markdown","markdown","Uses default values"]],demos:[["blocks_hello","import gradio as gr\n\ndef welcome(name):\n    return f\"Welcome to Gradio, {name}!\"\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\n    \"\"\"\n    # Hello World!\n    Start typing below to see the output.\n    \"\"\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n    inp.change(welcome, inp, out)\n\nif __name__ == \"__main__\":\n    demo.launch()"],["blocks_kinematics","import pandas as pd\nimport numpy as np\n\nimport gradio as gr\n\n\ndef plot(v, a):\n    g = 9.81\n    theta = a / 180 * 3.14\n    tmax = ((2 * v) * np.sin(theta)) / g\n    timemat = tmax * np.linspace(0, 1, 40)\n\n    x = (v * timemat) * np.cos(theta)\n    y = ((v * timemat) * np.sin(theta)) - ((0.5 * g) * (timemat**2))\n    df = pd.DataFrame({\"x\": x, \"y\": y})\n    return df\n\n\ndemo = gr.Blocks()\n\nwith demo:\n    gr.Markdown(\n        r\"Let's do some kinematics! Choose the speed and angle to see the trajectory. Remember that the range $R = v_0^2 \\cdot \\frac{\\sin(2\\theta)}{g}$\"\n    )\n\n    with gr.Row():\n        speed = gr.Slider(1, 30, 25, label=\"Speed\")\n        angle = gr.Slider(0, 90, 45, label=\"Angle\")\n    output = gr.LinePlot(\n        x=\"x\",\n        y=\"y\",\n        overlay_point=True,\n        tooltip=[\"x\", \"y\"],\n        x_lim=[0, 100],\n        y_lim=[0, 60],\n        width=350,\n        height=300,\n    )\n    btn = gr.Button(value=\"Run\")\n    btn.click(plot, [speed, angle], output)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]],guides:[{name:"key-features",category:"getting-started",pretty_category:"Getting Started",guide_index:2,absolute_index:1,pretty_name:"Key Features",content:"# Key Features\n\nLet's go through some of the key features of Gradio. This guide is intended to be a high-level overview of various things that you should be aware of as you build your demo. Where appropriate, we link to more detailed guides on specific topics.\n\n1. [Components](#components)\n2. [Queuing](#queuing)\n3. [Streaming outputs](#streaming-outputs)\n4. [Streaming inputs](#streaming-inputs)\n5. [Alert modals](#alert-modals)\n6. [Styling](#styling)\n7. [Progress bars](#progress-bars)\n8. [Batch functions](#batch-functions)\n\n## Components\n\nGradio includes more than 30 pre-built components (as well as many user-built _custom components_) that can be used as inputs or outputs in your demo with a single line of code. These components correspond to common data types in machine learning and data science, e.g. the `gr.Image` component is designed to handle input or output images, the `gr.Label` component displays classification labels and probabilities, the `gr.Plot` component displays various kinds of plots, and so on.\n\nEach component includes various constructor attributes that control the properties of the component. For example, you can control the number of lines in a `gr.Textbox` using the `lines` argument (which takes a positive integer) in its constructor. Or you can control the way that a user can provide an image in the `gr.Image` component using the `sources` parameter (which takes a list like `[\"webcam\", \"upload\"]`).\n\n**Static and Interactive Components**\n\nEvery component has a _static_ version that is designed to *display* data, and most components also have an _interactive_ version designed to let users input or modify the data. Typically, you don't need to think about this distinction, because when you build a Gradio demo, Gradio automatically figures out whether the component should be static or interactive based on whether it is being used as an input or output. However, you can set this manually using the `interactive` argument that every component supports.\n\n**Preprocessing and Postprocessing**\n\nWhen a component is used as an input, Gradio automatically handles the _preprocessing_ needed to convert the data from a type sent by the user's browser (such as an uploaded image) to a form that can be accepted by your function (such as a `numpy` array).\n\n\nSimilarly, when a component is used as an output, Gradio automatically handles the _postprocessing_ needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user's browser (a gallery of images).\n\nConsider an example demo with three input components (`gr.Textbox`, `gr.Number`, and `gr.Image`) and two outputs (`gr.Number` and `gr.Gallery`) that serve as a UI for your image-to-image generation model. Below is a diagram of what our preprocessing will send to the model and what our postprocessing will require from it.\n\n![](https://github.com/gradio-app/gradio/blob/main/guides/assets/dataflow.svg?raw=true)\n\nIn this image, the following preprocessing steps happen to send the data from the browser to your function:\n\n* The text in the textbox is converted to a Python `str` (essentially no preprocessing)\n* The number in the number input in converted to a Python `float` (essentially no preprocessing)\n* Most importantly, ihe image supplied by the user is converted to a `numpy.array` representation of the RGB values in the image\n\nImages are converted to NumPy arrays because they are a common format for machine learning workflows. You can control the _preprocessing_ using the component's parameters when constructing the component. For example, if you instantiate the `Image` component with the following parameters, it will preprocess the image to the `PIL` format instead:\n\n```py\nimg = gr.Image(type=\"pil\")\n```\n\nPostprocessing is even simpler! Gradio automatically recognizes the format of the returned data (e.g. does the user's function return a `numpy` array or a `str` filepath for the `gr.Image` component?) and postprocesses it appropriately into a format that can be displayed by the browser.\n\nSo in the image above, the following postprocessing steps happen to send the data returned from a user's function to the browser:\n\n* The `float` is displayed as a number and displayed directly to the user\n* The list of string filepaths (`list[str]`) is interpreted as a list of image filepaths and displayed as a gallery in the browser\n\nTake a look at the [Docs](https://gradio.app/docs) to see all the parameters for each Gradio component.\n\n## Queuing\n\nEvery Gradio app comes with a built-in queuing system that can scale to thousands of concurrent users. You can configure the queue by using `queue()` method which is supported by the `gr.Interface`, `gr.Blocks`, and `gr.ChatInterface` classes. \n\nFor example, you can control the number of requests processed at a single time by setting the `default_concurrency_limit` parameter of `queue()`, e.g.\n\n```python\ndemo = gr.Interface(...).queue(default_concurrency_limit=5)\ndemo.launch()\n```\n\nThis limits the number of requests processed for this event listener at a single time to 5. By default, the `default_concurrency_limit` is actually set to `1`, which means that when many users are using your app, only a single user's request will be processed at a time. This is because many machine learning functions consume a significant amount of memory and so it is only suitable to have a single user using the demo at a time. However, you can change this parameter in your demo easily.\n\nSee the [docs on queueing](https://gradio.app/docs/gradio/interface#interface-queue) for more details on configuring the queuing parameters.\n\n## Streaming outputs\n\nIn some cases, you may want to stream a sequence of outputs rather than show a single output at once. For example, you might have an image generation model and you want to show the image that is generated at each step, leading up to the final image. Or you might have a chatbot which streams its response one token at a time instead of returning it all at once.\n\nIn such cases, you can supply a **generator** function into Gradio instead of a regular function. Creating generators in Python is very simple: instead of a single `return` value, a function should `yield` a series of values instead. Usually the `yield` statement is put in some kind of loop. Here's an example of an generator that simply counts up to a given number:\n\n```python\ndef my_generator(x):\n    for i in range(x):\n        yield i\n```\n\nYou supply a generator into Gradio the same way as you would a regular function. For example, here's a a (fake) image generation model that generates noise for several steps before outputting an image using the `gr.Interface` class:\n\n```python\nimport gradio as gr\nimport numpy as np\nimport time\n\ndef fake_diffusion(steps):\n    rng = np.random.default_rng()\n    for i in range(steps):\n        time.sleep(1)\n        image = rng.random(size=(600, 600, 3))\n        yield image\n    image = np.ones((1000,1000,3), np.uint8)\n    image[:] = [255, 124, 0]\n    yield image\n\n\ndemo = gr.Interface(fake_diffusion,\n                    inputs=gr.Slider(1, 10, 3, step=1),\n                    outputs=\"image\")\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/fake_diffusion'>\u003C/gradio-app>\n\nNote that we've added a `time.sleep(1)` in the iterator to create an artificial pause between steps so that you are able to observe the steps of the iterator (in a real image generation model, this probably wouldn't be necessary).\n\n## Streaming inputs\n\nSimilarly, Gradio can handle streaming inputs, e.g. a live audio stream that can gets transcribed to text in real time, or an image generation model that reruns every time a user types a letter in a textbox. This is covered in more details in our guide on building [reactive Interfaces](/guides/reactive-interfaces). \n\n## Alert modals\n\nYou may wish to raise alerts to the user. To do so, raise a `gr.Error(\"custom message\")` to display an error message. You can also issue `gr.Warning(\"message\")` and `gr.Info(\"message\")` by having them as standalone lines in your function, which will immediately display modals while continuing the execution of your function. Queueing needs to be enabled for this to work.\n\nNote below how the `gr.Error` has to be raised, while the `gr.Warning` and `gr.Info` are single lines.\n\n```python\ndef start_process(name):\n    gr.Info(\"Starting process\")\n    if name is None:\n        gr.Warning(\"Name is empty\")\n    ...\n    if success == False:\n        raise gr.Error(\"Process failed\")\n```\n\n\n\n## Styling\n\nGradio themes are the easiest way to customize the look and feel of your app. You can choose from a variety of themes, or create your own. To do so, pass the `theme=` kwarg to the `Interface` constructor. For example:\n\n```python\ndemo = gr.Interface(..., theme=gr.themes.Monochrome())\n```\n\nGradio comes with a set of prebuilt themes which you can load from `gr.themes.*`. You can extend these themes or create your own themes from scratch - see the [theming guide](https://gradio.app/guides/theming-guide) for more details.\n\nFor additional styling ability, you can pass any CSS (as well as custom JavaScript) to your Gradio application. This is discussed in more detail in our [custom JS and CSS guide](/guides/custom-CSS-and-JS).\n\n\n## Progress bars\n\nGradio supports the ability to create custom Progress Bars so that you have customizability and control over the progress update that you show to the user. In order to enable this, simply add an argument to your method that has a default value of a `gr.Progress` instance. Then you can update the progress levels by calling this instance directly with a float between 0 and 1, or using the `tqdm()` method of the `Progress` instance to track progress over an iterable, as shown below.\n\n```python\nimport gradio as gr\nimport time\n\ndef slowly_reverse(word, progress=gr.Progress()):\n    progress(0, desc=\"Starting\")\n    time.sleep(1)\n    progress(0.05)\n    new_string = \"\"\n    for letter in progress.tqdm(word, desc=\"Reversing\"):\n        time.sleep(0.25)\n        new_string = letter + new_string\n    return new_string\n\ndemo = gr.Interface(slowly_reverse, gr.Text(), gr.Text())\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/progress_simple'>\u003C/gradio-app>\n\nIf you use the `tqdm` library, you can even report progress updates automatically from any `tqdm.tqdm` that already exists within your function by setting the default argument as `gr.Progress(track_tqdm=True)`!\n\n## Batch functions\n\nGradio supports the ability to pass _batch_ functions. Batch functions are just\nfunctions which take in a list of inputs and return a list of predictions.\n\nFor example, here is a batched function that takes in two lists of inputs (a list of\nwords and a list of ints), and returns a list of trimmed words as output:\n\n```py\nimport time\n\ndef trim_words(words, lens):\n    trimmed_words = []\n    time.sleep(5)\n    for w, l in zip(words, lens):\n        trimmed_words.append(w[:int(l)])\n    return [trimmed_words]\n```\n\nThe advantage of using batched functions is that if you enable queuing, the Gradio server can automatically _batch_ incoming requests and process them in parallel,\npotentially speeding up your demo. Here's what the Gradio code looks like (notice the `batch=True` and `max_batch_size=16`)\n\nWith the `gr.Interface` class:\n\n```python\ndemo = gr.Interface(\n    fn=trim_words, \n    inputs=[\"textbox\", \"number\"], \n    outputs=[\"output\"],\n    batch=True, \n    max_batch_size=16\n)\n\ndemo.launch()\n```\n\nWith the `gr.Blocks` class:\n\n```py\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        word = gr.Textbox(label=\"word\")\n        leng = gr.Number(label=\"leng\")\n        output = gr.Textbox(label=\"Output\")\n    with gr.Row():\n        run = gr.Button()\n\n    event = run.click(trim_words, [word, leng], output, batch=True, max_batch_size=16)\n\ndemo.launch()\n```\n\nIn the example above, 16 requests could be processed in parallel (for a total inference time of 5 seconds), instead of each request being processed separately (for a total\ninference time of 80 seconds). Many Hugging Face `transformers` and `diffusers` models work very naturally with Gradio's batch mode: here's [an example demo using diffusers to\ngenerate images in batches](https://github.com/gradio-app/gradio/blob/main/demo/diffusers_with_batching/run.py)\n\n\n\n",tags:[],spaces:[],url:"/guides/key-features/",contributor:null}],parent:"gradio"},model3d:{class:null,name:"Model3D",description:"Creates a component allows users to upload or view 3D Model files (.obj, .glb, .stl, .gltf, .splat, or .ply). \u003Cbr>",tags:{guides:"how-to-use-3D-model-component"},parameters:[{name:"value",annotation:"str | Callable | None",doc:"path to (.obj, .glb, .stl, .gltf, .splat, or .ply) file to show in model3D viewer. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"clear_color",annotation:"tuple[float, float, float, float] | None",doc:"background color of scene, should be a tuple of 4 floats between 0 and 1 representing RGBA values.",default:"None"},{name:"camera_position",annotation:"tuple[int | float | None, int | float | None, int | float | None]",doc:"initial camera position of scene, provided as a tuple of `(alpha, beta, radius)`. Each value is optional. If provided, `alpha` and `beta` should be in degrees reflecting the angular position along the longitudinal and latitudinal axes, respectively. Radius corresponds to the distance from the center of the object to the camera.",default:"(None, None, None)"},{name:"zoom_speed",annotation:"float",doc:"the speed of zooming in and out of the scene when the cursor wheel is rotated or when screen is pinched on a mobile device. Should be a positive float, increase this value to make zooming faster, decrease to make it slower. Affects the wheelPrecision property of the camera.",default:"1"},{name:"pan_speed",annotation:"float",doc:"the speed of panning the scene when the cursor is dragged or when the screen is dragged on a mobile device. Should be a positive float, increase this value to make panning faster, decrease to make it slower. Affects the panSensibility property of the camera.",default:"1"},{name:"height",annotation:"int | str | None",doc:"The height of the model3D component, specified in pixels if a number is passed, or in CSS units if a string is passed.",default:"None"},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"interactive",annotation:"bool | None",doc:"if True, will allow users to upload a file; if False, can only be used to display files. If not provided, this is inferred based on whether the component is used as an input or output.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the Model3D changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Model3D"},{fn:null,name:"upload",description:"This listener is triggered when the user uploads a file into the Model3D.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Model3D"},{fn:null,name:"edit",description:"This listener is triggered when the user edits the Model3D (e.g. image) using the built-in editor.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Model3D"},{fn:null,name:"clear",description:"This listener is triggered when the user clears the Model3D using the X button for the component.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Model3D"}],preprocess:{parameter_doc:[{name:"payload",annotation:"FileData | None",doc:"the uploaded file as an instance of `FileData`."}],return_doc:{annotation:"str | None",doc:"Passes the uploaded file as a {str} filepath to the function."}},postprocess:{parameter_doc:[{name:"value",annotation:"str | Path | None",doc:"Expects function to return a {str} or {pathlib.Path} filepath of type (.obj, .glb, .stl, or .gltf)"}],return_doc:{annotation:"FileData | None",doc:"The uploaded file as an instance of `FileData`."}},string_shortcuts:[["Model3D","model3d","Uses default values"]],guides:[{name:"how-to-use-3D-model-component",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:54,pretty_name:"How To Use 3D Model Component",content:"# How to Use the 3D Model Component\n\n\n\n\n## Introduction\n\n3D models are becoming more popular in machine learning and make for some of the most fun demos to experiment with. Using `gradio`, you can easily build a demo of your 3D image model and share it with anyone. The Gradio 3D Model component accepts 3 file types including: _.obj_, _.glb_, & _.gltf_.\n\nThis guide will show you how to build a demo for your 3D image model in a few lines of code; like the one below. Play around with 3D object by clicking around, dragging and zooming:\n\n\u003Cgradio-app space=\"gradio/Model3D\"> \u003C/gradio-app>\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](https://gradio.app/guides/quickstart).\n\n## Taking a Look at the Code\n\nLet's take a look at how to create the minimal interface above. The prediction function in this case will just return the original 3D model mesh, but you can change this function to run inference on your machine learning model. We'll take a look at more complex examples below.\n\n```python\nimport gradio as gr\nimport os\n\n\ndef load_mesh(mesh_file_name):\n    return mesh_file_name\n\n\ndemo = gr.Interface(\n    fn=load_mesh,\n    inputs=gr.Model3D(),\n    outputs=gr.Model3D(\n            clear_color=[0.0, 0.0, 0.0, 0.0],  label=\"3D Model\"),\n    examples=[\n        [os.path.join(os.path.dirname(__file__), \"files/Bunny.obj\")],\n        [os.path.join(os.path.dirname(__file__), \"files/Duck.glb\")],\n        [os.path.join(os.path.dirname(__file__), \"files/Fox.gltf\")],\n        [os.path.join(os.path.dirname(__file__), \"files/face.obj\")],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\nLet's break down the code above:\n\n`load_mesh`: This is our 'prediction' function and for simplicity, this function will take in the 3D model mesh and return it.\n\nCreating the Interface:\n\n- `fn`: the prediction function that is used when the user clicks submit. In our case this is the `load_mesh` function.\n- `inputs`: create a model3D input component. The input expects an uploaded file as a {str} filepath.\n- `outputs`: create a model3D output component. The output component also expects a file as a {str} filepath.\n  - `clear_color`: this is the background color of the 3D model canvas. Expects RGBa values.\n  - `label`: the label that appears on the top left of the component.\n- `examples`: list of 3D model files. The 3D model component can accept _.obj_, _.glb_, & _.gltf_ file types.\n- `cache_examples`: saves the predicted output for the examples, to save time on inference.\n\n## Exploring a more complex Model3D Demo:\n\nBelow is a demo that uses the DPT model to predict the depth of an image and then uses 3D Point Cloud to create a 3D object. Take a look at the [app.py](https://huggingface.co/spaces/gradio/dpt-depth-estimation-3d-obj/blob/main/app.py) file for a peek into the code and the model prediction function.\n\u003Cgradio-app space=\"gradio/dpt-depth-estimation-3d-obj\"> \u003C/gradio-app>\n\n---\n\nAnd you're done! That's all the code you need to build an interface for your Model3D model. Here are some references that you may find useful:\n\n- Gradio's [\"Getting Started\" guide](https://gradio.app/getting_started/)\n- The first [3D Model Demo](https://huggingface.co/spaces/gradio/Model3D) and [complete code](https://huggingface.co/spaces/gradio/Model3D/tree/main) (on Hugging Face Spaces)\n",tags:["VISION","IMAGE"],spaces:["https://huggingface.co/spaces/gradio/Model3D","https://huggingface.co/spaces/gradio/PIFu-Clothed-Human-Digitization","https://huggingface.co/spaces/gradio/dpt-depth-estimation-3d-obj"],url:"/guides/how-to-use-3D-model-component/",contributor:null}],parent:"gradio"},multimodaltextbox:{class:null,name:"MultimodalTextbox",description:"Creates a textarea for users to enter string input or display string output and also allows for the uploading of multimedia files. \u003Cbr>",tags:{demos:"chatbot_multimodal",guides:"creating-a-chatbot"},parameters:[{name:"value",annotation:"dict[str, str | list] | Callable | None",doc:"Default value to show in MultimodalTextbox. A dictionary of the form {&quot;text&quot;: &quot;sample text&quot;, &quot;files&quot;: [{path: &quot;files/file.jpg&quot;, orig_name: &quot;file.jpg&quot;, url: &quot;http://image_url.jpg&quot;, size: 100}]}. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"file_types",annotation:"list[str] | None",doc:"List of file extensions or types of files to be uploaded (e.g. [&#x27;image&#x27;, &#x27;.json&#x27;, &#x27;.mp4&#x27;]). &quot;file&quot; allows any file to be uploaded, &quot;image&quot; allows only image files to be uploaded, &quot;audio&quot; allows only audio files to be uploaded, &quot;video&quot; allows only video files to be uploaded, &quot;text&quot; allows only text files to be uploaded.",default:"None"},{name:"lines",annotation:"int",doc:"minimum number of line rows to provide in textarea.",default:"1"},{name:"max_lines",annotation:"int",doc:"maximum number of line rows to provide in textarea.",default:"20"},{name:"placeholder",annotation:"str | None",doc:"placeholder hint to provide behind textarea.",default:"None"},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there is a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"info",annotation:"str | None",doc:"additional component description.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"interactive",annotation:"bool | None",doc:"if True, will be rendered as an editable textbox; if False, editing will be disabled. If not provided, this is inferred based on whether the component is used as an input or output.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"autofocus",annotation:"bool",doc:"If True, will focus on the textbox when the page loads. Use this carefully, as it can cause usability issues for sighted and non-sighted users.",default:"False"},{name:"autoscroll",annotation:"bool",doc:"If True, will automatically scroll to the bottom of the textbox when the value changes, unless the user scrolls up. If False, will not scroll to the bottom of the textbox when the value changes.",default:"True"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"text_align",annotation:"Literal[('left', 'right')] | None",doc:"How to align the text in the textbox, can be: &quot;left&quot;, &quot;right&quot;, or None (default). If None, the alignment is left if `rtl` is False, or right if `rtl` is True. Can only be changed if `type` is &quot;text&quot;.",default:"None"},{name:"rtl",annotation:"bool",doc:"If True and `type` is &quot;text&quot;, sets the direction of the text to right-to-left (cursor appears on the left of the text). Default is False, which renders cursor on the right.",default:"False"},{name:"submit_btn",annotation:"str | Literal[False] | None",doc:"If False, will not show a submit button. If a string, will use that string as the submit button text. Only applies if `interactive` is True.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the MultimodalTextbox changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.MultimodalTextbox"},{fn:null,name:"input",description:"This listener is triggered when the user changes the value of the MultimodalTextbox.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.MultimodalTextbox"},{fn:null,name:"select",description:"Event listener for when the user selects or deselects the MultimodalTextbox. Uses event data gradio.SelectData to carry `value` referring to the label of the MultimodalTextbox, and `selected` to refer to state of the MultimodalTextbox. See EventData documentation on how to use this event data",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.MultimodalTextbox"},{fn:null,name:"submit",description:"This listener is triggered when the user presses the Enter key while the MultimodalTextbox is focused.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.MultimodalTextbox"},{fn:null,name:"focus",description:"This listener is triggered when the MultimodalTextbox is focused.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.MultimodalTextbox"},{fn:null,name:"blur",description:"This listener is triggered when the MultimodalTextbox is unfocused/blurred.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.MultimodalTextbox"}],preprocess:{parameter_doc:[{name:"payload",annotation:"MultimodalData | None",doc:"the text and list of file(s) entered in the multimodal textbox."}],return_doc:{annotation:"MultimodalValue | None",doc:"Passes text value and list of file(s) as a {dict} into the function."}},postprocess:{parameter_doc:[{name:"value",annotation:"MultimodalValue | None",doc:"Expects a {dict} with \"text\" and \"files\", both optional. The files array is a list of file paths or URLs."}],return_doc:{annotation:"MultimodalData",doc:"The value to display in the multimodal textbox. Files information as a list of FileData objects."}},string_shortcuts:[["MultimodalTextbox","multimodaltextbox","Uses default values"]],demos:[["chatbot_multimodal","import gradio as gr\nimport os\nimport time\n\n# Chatbot demo with multimodal input (text, markdown, LaTeX, code blocks, image, audio, & video). Plus shows support for streaming text.\n\n\ndef print_like_dislike(x: gr.LikeData):\n    print(x.index, x.value, x.liked)\n\ndef add_message(history, message):\n    for x in message[\"files\"]:\n        history.append(((x,), None))\n    if message[\"text\"] is not None:\n        history.append((message[\"text\"], None))\n    return history, gr.MultimodalTextbox(value=None, interactive=False)\n\ndef bot(history):\n    response = \"**That's cool!**\"\n    history[-1][1] = \"\"\n    for character in response:\n        history[-1][1] += character\n        time.sleep(0.05)\n        yield history\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot(\n        [],\n        elem_id=\"chatbot\",\n        bubble_full_width=False\n    )\n\n    chat_input = gr.MultimodalTextbox(interactive=True, file_types=[\"image\"], placeholder=\"Enter message or upload file...\", show_label=False)\n\n    chat_msg = chat_input.submit(add_message, [chatbot, chat_input], [chatbot, chat_input])\n    bot_msg = chat_msg.then(bot, chatbot, chatbot, api_name=\"bot_response\")\n    bot_msg.then(lambda: gr.MultimodalTextbox(interactive=True), None, [chat_input])\n\n    chatbot.like(print_like_dislike, None, None)\n\ndemo.queue()\nif __name__ == \"__main__\":\n    demo.launch()\n"]],guides:[],parent:"gradio"},number:{class:null,name:"Number",description:"Creates a numeric field for user to enter numbers as input or display numeric output. \u003Cbr>",tags:{demos:"tax_calculator, blocks_simple_squares"},parameters:[{name:"value",annotation:"float | Callable | None",doc:"default value. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"info",annotation:"str | None",doc:"additional component description.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"interactive",annotation:"bool | None",doc:"if True, will be editable; if False, editing will be disabled. If not provided, this is inferred based on whether the component is used as an input or output.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"precision",annotation:"int | None",doc:"Precision to round input/output to. If set to 0, will round to nearest integer and convert type to int. If None, no rounding happens.",default:"None"},{name:"minimum",annotation:"float | None",doc:"Minimum value. Only applied when component is used as an input. If a user provides a smaller value, a gr.Error exception is raised by the backend.",default:"None"},{name:"maximum",annotation:"float | None",doc:"Maximum value. Only applied when component is used as an input. If a user provides a larger value, a gr.Error exception is raised by the backend.",default:"None"},{name:"step",annotation:"float",doc:"The interval between allowed numbers in the component. Can be used along with optional parameters `minimum` and `maximum` to create a range of legal values starting from `minimum` and incrementing according to this parameter.",default:"1"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the Number changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Number"},{fn:null,name:"input",description:"This listener is triggered when the user changes the value of the Number.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Number"},{fn:null,name:"submit",description:"This listener is triggered when the user presses the Enter key while the Number is focused.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Number"},{fn:null,name:"focus",description:"This listener is triggered when the Number is focused.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Number"}],preprocess:{parameter_doc:[{name:"payload",annotation:"float | None",doc:"the field value."}],return_doc:{annotation:"float | int | None",doc:"Passes field value as a `float` or `int` into the function, depending on `precision`."}},postprocess:{parameter_doc:[{name:"value",annotation:"float | int | None",doc:"Expects an `int` or `float` returned from the function and sets field value to it."}],return_doc:{annotation:"float | int | None",doc:"The (optionally rounded) field value as a `float` or `int` depending on `precision`."}},string_shortcuts:[["Number","number","Uses default values"]],demos:[["tax_calculator","import gradio as gr\n\ndef tax_calculator(income, marital_status, assets):\n    tax_brackets = [(10, 0), (25, 8), (60, 12), (120, 20), (250, 30)]\n    total_deductible = sum(assets[\"Cost\"])\n    taxable_income = income - total_deductible\n\n    total_tax = 0\n    for bracket, rate in tax_brackets:\n        if taxable_income > bracket:\n            total_tax += (taxable_income - bracket) * rate / 100\n\n    if marital_status == \"Married\":\n        total_tax *= 0.75\n    elif marital_status == \"Divorced\":\n        total_tax *= 0.8\n\n    return round(total_tax)\n\ndemo = gr.Interface(\n    tax_calculator,\n    [\n        \"number\",\n        gr.Radio([\"Single\", \"Married\", \"Divorced\"]),\n        gr.Dataframe(\n            headers=[\"Item\", \"Cost\"],\n            datatype=[\"str\", \"number\"],\n            label=\"Assets Purchased this Year\",\n        ),\n    ],\n    \"number\",\n    examples=[\n        [10000, \"Married\", [[\"Suit\", 5000], [\"Laptop\", 800], [\"Car\", 1800]]],\n        [80000, \"Single\", [[\"Suit\", 800], [\"Watch\", 1800], [\"Car\", 800]]],\n    ],\n)\n\ndemo.launch()\n"],["blocks_simple_squares","import gradio as gr\n\ndemo = gr.Blocks(css=\"\"\"#btn {color: red} .abc {font-family: \"Comic Sans MS\", \"Comic Sans\", cursive !important}\"\"\")\n\nwith demo:\n    default_json = {\"a\": \"a\"}\n\n    num = gr.State(value=0)\n    squared = gr.Number(value=0)\n    btn = gr.Button(\"Next Square\", elem_id=\"btn\", elem_classes=[\"abc\", \"def\"])\n\n    stats = gr.State(value=default_json)\n    table = gr.JSON()\n\n    def increase(var, stats_history):\n        var += 1\n        stats_history[str(var)] = var**2\n        return var, var**2, stats_history, stats_history\n\n    btn.click(increase, [num, stats], [num, squared, stats, table])\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]],parent:"gradio"},paramviewer:{class:null,name:"ParamViewer",description:"Displays an interactive table of parameters and their descriptions and default values with syntax highlighting. For each parameter, the user should provide a type (e.g. a `str`), a human-readable description, and a default value. As this component does not accept user input, it is rarely used as an input component.Internally, this component is used to display the parameters of components in the Custom Component Gallery (https://www.gradio.app/custom-components/gallery).",tags:{},parameters:[{name:"value",annotation:"dict[str, Parameter] | None",doc:"A list of dictionaries with keys &quot;type&quot;, &quot;description&quot;, and &quot;default&quot; for each parameter.",default:"None"},{name:"language",annotation:"Literal[('python', 'typescript')]",doc:"The language to display the code in. One of &quot;python&quot; or &quot;typescript&quot;.",default:"\"python\""},{name:"linkify",annotation:"list[str] | None",doc:"A list of strings to linkify. If any of these strings is found in the description, it will be rendered as a link.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the ParamViewer changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.ParamViewer"},{fn:null,name:"upload",description:"This listener is triggered when the user uploads a file into the ParamViewer.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.ParamViewer"}],preprocess:{parameter_doc:[{name:"payload",annotation:"dict[str, Parameter]",doc:"A `dict[str, dict]`. The key in the outer dictionary is the parameter name, while the inner dictionary has keys \"type\", \"description\", and (optionally) \"default\" for each parameter."}],return_doc:{annotation:"dict[str, Parameter]",doc:"(Rarely used) passes value as a `dict[str, dict]`. The key in the outer dictionary is the parameter name, while the inner dictionary has keys \"type\", \"description\", and (optionally) \"default\" for each parameter."}},postprocess:{parameter_doc:[{name:"value",annotation:"dict[str, Parameter]",doc:"Expects value as a `dict[str, dict]`. The key in the outer dictionary is the parameter name, while the inner dictionary has keys \"type\", \"description\", and (optionally) \"default\" for each parameter."}],return_doc:{annotation:"dict[str, Parameter]",doc:"The same value."}},string_shortcuts:[["ParamViewer","paramviewer","Uses default values"]],parent:"gradio"},radio:{class:null,name:"Radio",description:"Creates a set of (string or numeric type) radio buttons of which only one can be selected. \u003Cbr>",tags:{demos:"sentence_builder, blocks_essay"},parameters:[{name:"choices",annotation:"list[str | int | float | tuple[str, str | int | float]] | None",doc:"A list of string or numeric options to select from. An option can also be a tuple of the form (name, value), where name is the displayed name of the radio button and value is the value to be passed to the function, or returned by the function.",default:"None"},{name:"value",annotation:"str | int | float | Callable | None",doc:"The option selected by default. If None, no option is selected by default. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"type",annotation:"str",doc:"Type of value to be returned by component. &quot;value&quot; returns the string of the choice selected, &quot;index&quot; returns the index of the choice selected.",default:"\"value\""},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"info",annotation:"str | None",doc:"Additional component description.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"Relative width compared to adjacent Components in a Row. For example, if Component A has scale=2, and Component B has scale=1, A will be twice as wide as B. Should be an integer.",default:"None"},{name:"min_width",annotation:"int",doc:"Minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"interactive",annotation:"bool | None",doc:"If True, choices in this radio group will be selectable; if False, selection will be disabled. If not provided, this is inferred based on whether the component is used as an input or output.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"select",description:"Event listener for when the user selects or deselects the Radio. Uses event data gradio.SelectData to carry `value` referring to the label of the Radio, and `selected` to refer to state of the Radio. See EventData documentation on how to use this event data",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Radio"},{fn:null,name:"change",description:"Triggered when the value of the Radio changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Radio"},{fn:null,name:"input",description:"This listener is triggered when the user changes the value of the Radio.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Radio"}],preprocess:{parameter_doc:[{name:"payload",annotation:"str | int | float | None",doc:"Selected choice in the radio group"}],return_doc:{annotation:"str | int | float | None",doc:"Passes the value of the selected radio button as a `str | int | float`, or its index as an `int` into the function, depending on `type`."}},postprocess:{parameter_doc:[{name:"value",annotation:"str | int | float | None",doc:"Expects a `str | int | float` corresponding to the value of the radio button to be selected"}],return_doc:{annotation:"str | int | float | None",doc:"The same value"}},string_shortcuts:[["Radio","radio","Uses default values"]],demos:[["sentence_builder","import gradio as gr\n\n\ndef sentence_builder(quantity, animal, countries, place, activity_list, morning):\n    return f\"\"\"The {quantity} {animal}s from {\" and \".join(countries)} went to the {place} where they {\" and \".join(activity_list)} until the {\"morning\" if morning else \"night\"}\"\"\"\n\n\ndemo = gr.Interface(\n    sentence_builder,\n    [\n        gr.Slider(2, 20, value=4, label=\"Count\", info=\"Choose between 2 and 20\"),\n        gr.Dropdown(\n            [\"cat\", \"dog\", \"bird\"], label=\"Animal\", info=\"Will add more animals later!\"\n        ),\n        gr.CheckboxGroup([\"USA\", \"Japan\", \"Pakistan\"], label=\"Countries\", info=\"Where are they from?\"),\n        gr.Radio([\"park\", \"zoo\", \"road\"], label=\"Location\", info=\"Where did they go?\"),\n        gr.Dropdown(\n            [\"ran\", \"swam\", \"ate\", \"slept\"], value=[\"swam\", \"slept\"], multiselect=True, label=\"Activity\", info=\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed auctor, nisl eget ultricies aliquam, nunc nisl aliquet nunc, eget aliquam nisl nunc vel nisl.\"\n        ),\n        gr.Checkbox(label=\"Morning\", info=\"Did they do it in the morning?\"),\n    ],\n    \"text\",\n    examples=[\n        [2, \"cat\", [\"Japan\", \"Pakistan\"], \"park\", [\"ate\", \"swam\"], True],\n        [4, \"dog\", [\"Japan\"], \"zoo\", [\"ate\", \"swam\"], False],\n        [10, \"bird\", [\"USA\", \"Pakistan\"], \"road\", [\"ran\"], False],\n        [8, \"cat\", [\"Pakistan\"], \"zoo\", [\"ate\"], True],\n    ]\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"],["blocks_essay","import gradio as gr\n\ncountries_cities_dict = {\n    \"USA\": [\"New York\", \"Los Angeles\", \"Chicago\"],\n    \"Canada\": [\"Toronto\", \"Montreal\", \"Vancouver\"],\n    \"Pakistan\": [\"Karachi\", \"Lahore\", \"Islamabad\"],\n}\n\n\ndef change_textbox(choice):\n    if choice == \"short\":\n        return gr.Textbox(lines=2, visible=True), gr.Button(interactive=True)\n    elif choice == \"long\":\n        return gr.Textbox(lines=8, visible=True, value=\"Lorem ipsum dolor sit amet\"), gr.Button(interactive=True)\n    else:\n        return gr.Textbox(visible=False), gr.Button(interactive=False)\n\n\nwith gr.Blocks() as demo:\n    radio = gr.Radio(\n        [\"short\", \"long\", \"none\"], label=\"What kind of essay would you like to write?\"\n    )\n    text = gr.Textbox(lines=2, interactive=True, show_copy_button=True)\n\n    with gr.Row():\n        num = gr.Number(minimum=0, maximum=100, label=\"input\")\n        out = gr.Number(label=\"output\")\n    minimum_slider = gr.Slider(0, 100, 0, label=\"min\")\n    maximum_slider = gr.Slider(0, 100, 100, label=\"max\")\n    submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n\n    with gr.Row():\n        country = gr.Dropdown(list(countries_cities_dict.keys()), label=\"Country\")\n        cities = gr.Dropdown([], label=\"Cities\")\n        \n    @country.change(inputs=country, outputs=cities)\n    def update_cities(country):\n        cities = list(countries_cities_dict[country])\n        return gr.Dropdown(choices=cities, value=cities[0], interactive=True)\n\n    def reset_bounds(minimum, maximum):\n        return gr.Number(minimum=minimum, maximum=maximum)\n\n    radio.change(fn=change_textbox, inputs=radio, outputs=[text, submit_btn])\n    gr.on(\n        [minimum_slider.change, maximum_slider.change],\n        reset_bounds,\n        [minimum_slider, maximum_slider],\n        outputs=num,\n    )\n    num.submit(lambda x: x, num, out)\n\n\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]],parent:"gradio"},scatterplot:{class:null,name:"ScatterPlot",description:"Creates a scatter plot component to display data from a pandas DataFrame (as output). As this component does not accept user input, it is rarely used as an input component. \u003Cbr>",tags:{guides:"creating-a-dashboard-from-bigquery-data"},parameters:[{name:"value",annotation:"pd.DataFrame | Callable | None",doc:"The pandas dataframe containing the data to display in a scatter plot, or a callable. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"x",annotation:"str | None",doc:"Column corresponding to the x axis.",default:"None"},{name:"y",annotation:"str | None",doc:"Column corresponding to the y axis.",default:"None"},{name:"color",annotation:"str | None",doc:"The column to determine the point color. If the column contains numeric data, gradio will interpolate the column data so that small values correspond to light colors and large values correspond to dark values.",default:"None"},{name:"size",annotation:"str | None",doc:"The column used to determine the point size. Should contain numeric data so that gradio can map the data to the point size.",default:"None"},{name:"shape",annotation:"str | None",doc:"The column used to determine the point shape. Should contain categorical data. Gradio will map each unique value to a different shape.",default:"None"},{name:"title",annotation:"str | None",doc:"The title to display on top of the chart.",default:"None"},{name:"tooltip",annotation:"list[str] | str | None",doc:"The column (or list of columns) to display on the tooltip when a user hovers a point on the plot.",default:"None"},{name:"x_title",annotation:"str | None",doc:"The title given to the x-axis. By default, uses the value of the x parameter.",default:"None"},{name:"y_title",annotation:"str | None",doc:"The title given to the y-axis. By default, uses the value of the y parameter.",default:"None"},{name:"x_label_angle",annotation:"float | None",doc:" The angle for the x axis labels rotation. Positive values are clockwise, and negative values are counter-clockwise.",default:"None"},{name:"y_label_angle",annotation:"float | None",doc:" The angle for the y axis labels rotation. Positive values are clockwise, and negative values are counter-clockwise.",default:"None"},{name:"color_legend_title",annotation:"str | None",doc:"The title given to the color legend. By default, uses the value of color parameter.",default:"None"},{name:"size_legend_title",annotation:"str | None",doc:"The title given to the size legend. By default, uses the value of the size parameter.",default:"None"},{name:"shape_legend_title",annotation:"str | None",doc:"The title given to the shape legend. By default, uses the value of the shape parameter.",default:"None"},{name:"color_legend_position",annotation:"Literal[('left', 'right', 'top', 'bottom', 'top-left', 'top-right', 'bottom-left', 'bottom-right', 'none')] | None",doc:"The position of the color legend. If the string value &#x27;none&#x27; is passed, this legend is omitted. For other valid position values see: https://vega.github.io/vega/docs/legends/#orientation.",default:"None"},{name:"size_legend_position",annotation:"Literal[('left', 'right', 'top', 'bottom', 'top-left', 'top-right', 'bottom-left', 'bottom-right', 'none')] | None",doc:"The position of the size legend. If the string value &#x27;none&#x27; is passed, this legend is omitted. For other valid position values see: https://vega.github.io/vega/docs/legends/#orientation.",default:"None"},{name:"shape_legend_position",annotation:"Literal[('left', 'right', 'top', 'bottom', 'top-left', 'top-right', 'bottom-left', 'bottom-right', 'none')] | None",doc:"The position of the shape legend. If the string value &#x27;none&#x27; is passed, this legend is omitted. For other valid position values see: https://vega.github.io/vega/docs/legends/#orientation.",default:"None"},{name:"height",annotation:"int | str | None",doc:"The height of the plot, specified in pixels if a number is passed, or in CSS units if a string is passed.",default:"None"},{name:"width",annotation:"int | str | None",doc:"The width of the plot, specified in pixels if a number is passed, or in CSS units if a string is passed.",default:"None"},{name:"x_lim",annotation:"list[int | float] | None",doc:"A tuple or list containing the limits for the x-axis, specified as [x_min, x_max].",default:"None"},{name:"y_lim",annotation:"list[int | float] | None",doc:"A tuple of list containing the limits for the y-axis, specified as [y_min, y_max].",default:"None"},{name:"caption",annotation:"str | None",doc:"The (optional) caption to display below the plot.",default:"None"},{name:"interactive",annotation:"bool | None",doc:"Whether users should be able to interact with the plot by panning or zooming with their mouse or trackpad.",default:"True"},{name:"label",annotation:"str | None",doc:"The (optional) label to display on the top left corner of the plot.",default:"None"},{name:"every",annotation:"float | None",doc:" If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"Whether the label should be displayed.",default:"None"},{name:"container",annotation:"bool",doc:null,default:"True"},{name:"scale",annotation:"int | None",doc:null,default:"None"},{name:"min_width",annotation:"int",doc:null,default:"160"},{name:"visible",annotation:"bool",doc:"Whether the plot should be visible.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"show_actions_button",annotation:"bool",doc:"Whether to show the actions button on the top right corner of the plot.",default:"False"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the Plot changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.ScatterPlot"},{fn:null,name:"clear",description:"This listener is triggered when the user clears the Plot using the X button for the component.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.ScatterPlot"}],preprocess:{parameter_doc:[{name:"payload",annotation:"AltairPlotData | None",doc:"The data to display in a scatter plot."}],return_doc:{annotation:"AltairPlotData | None",doc:"(Rarely used) passes the data displayed in the scatter plot as an AltairPlotData dataclass, which includes the plot information as a JSON string, as well as the type of plot (in this case, \"scatter\")."}},postprocess:{parameter_doc:[{name:"value",annotation:"pd.DataFrame | dict | None",doc:"Expects a pandas DataFrame containing the data to display in the scatter plot. The DataFrame should contain at least two columns, one for the x-axis (corresponding to this component's `x` argument) and one for the y-axis (corresponding to `y`)."}],return_doc:{annotation:"AltairPlotData | dict | None",doc:"The data to display in a scatter plot, in the form of an AltairPlotData dataclass, which includes the plot information as a JSON string, as well as the type of plot (in this case, \"scatter\")."}},string_shortcuts:[["ScatterPlot","scatterplot","Uses default values"]],guides:[{name:"creating-a-dashboard-from-bigquery-data",category:"tabular-data-science-and-plots",pretty_category:"Tabular Data Science And Plots",guide_index:null,absolute_index:34,pretty_name:"Creating A Dashboard From Bigquery Data",content:"# Creating a Real-Time Dashboard from BigQuery Data\n\n\n\n[Google BigQuery](https://cloud.google.com/bigquery) is a cloud-based service for processing very large data sets. It is a serverless and highly scalable data warehousing solution that enables users to analyze data [using SQL-like queries](https://www.oreilly.com/library/view/google-bigquery-the/9781492044451/ch01.html).\n\nIn this tutorial, we will show you how to query a BigQuery dataset in Python and display the data in a dashboard that updates in real time using `gradio`. The dashboard will look like this:\n\n\u003Cimg src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/bigquery-dashboard.gif\">\n\nWe'll cover the following steps in this Guide:\n\n1. Setting up your BigQuery credentials\n2. Using the BigQuery client\n3. Building the real-time dashboard (in just _7 lines of Python_)\n\nWe'll be working with the [New York Times' COVID dataset](https://www.nytimes.com/interactive/2021/us/covid-cases.html) that is available as a public dataset on BigQuery. The dataset, named `covid19_nyt.us_counties` contains the latest information about the number of confirmed cases and deaths from COVID across US counties.\n\n**Prerequisites**: This Guide uses [Gradio Blocks](/guides/quickstart/#blocks-more-flexibility-and-control), so make your are familiar with the Blocks class.\n\n## Setting up your BigQuery Credentials\n\nTo use Gradio with BigQuery, you will need to obtain your BigQuery credentials and use them with the [BigQuery Python client](https://pypi.org/project/google-cloud-bigquery/). If you already have BigQuery credentials (as a `.json` file), you can skip this section. If not, you can do this for free in just a couple of minutes.\n\n1. First, log in to your Google Cloud account and go to the Google Cloud Console (https://console.cloud.google.com/)\n\n2. In the Cloud Console, click on the hamburger menu in the top-left corner and select \"APIs & Services\" from the menu. If you do not have an existing project, you will need to create one.\n\n3. Then, click the \"+ Enabled APIs & services\" button, which allows you to enable specific services for your project. Search for \"BigQuery API\", click on it, and click the \"Enable\" button. If you see the \"Manage\" button, then the BigQuery is already enabled, and you're all set.\n\n4. In the APIs & Services menu, click on the \"Credentials\" tab and then click on the \"Create credentials\" button.\n\n5. In the \"Create credentials\" dialog, select \"Service account key\" as the type of credentials to create, and give it a name. Also grant the service account permissions by giving it a role such as \"BigQuery User\", which will allow you to run queries.\n\n6. After selecting the service account, select the \"JSON\" key type and then click on the \"Create\" button. This will download the JSON key file containing your credentials to your computer. It will look something like this:\n\n```json\n{\n\t\"type\": \"service_account\",\n\t\"project_id\": \"your project\",\n\t\"private_key_id\": \"your private key id\",\n\t\"private_key\": \"private key\",\n\t\"client_email\": \"email\",\n\t\"client_id\": \"client id\",\n\t\"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n\t\"token_uri\": \"https://accounts.google.com/o/oauth2/token\",\n\t\"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n\t\"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/email_id\"\n}\n```\n\n## Using the BigQuery Client\n\nOnce you have the credentials, you will need to use the BigQuery Python client to authenticate using your credentials. To do this, you will need to install the BigQuery Python client by running the following command in the terminal:\n\n```bash\npip install google-cloud-bigquery[pandas]\n```\n\nYou'll notice that we've installed the pandas add-on, which will be helpful for processing the BigQuery dataset as a pandas dataframe. Once the client is installed, you can authenticate using your credentials by running the following code:\n\n```py\nfrom google.cloud import bigquery\n\nclient = bigquery.Client.from_service_account_json(\"path/to/key.json\")\n```\n\nWith your credentials authenticated, you can now use the BigQuery Python client to interact with your BigQuery datasets.\n\nHere is an example of a function which queries the `covid19_nyt.us_counties` dataset in BigQuery to show the top 20 counties with the most confirmed cases as of the current day:\n\n```py\nimport numpy as np\n\nQUERY = (\n    'SELECT * FROM `bigquery-public-data.covid19_nyt.us_counties` '\n    'ORDER BY date DESC,confirmed_cases DESC '\n    'LIMIT 20')\n\ndef run_query():\n    query_job = client.query(QUERY)\n    query_result = query_job.result()\n    df = query_result.to_dataframe()\n    # Select a subset of columns\n    df = df[[\"confirmed_cases\", \"deaths\", \"county\", \"state_name\"]]\n    # Convert numeric columns to standard numpy types\n    df = df.astype({\"deaths\": np.int64, \"confirmed_cases\": np.int64})\n    return df\n```\n\n## Building the Real-Time Dashboard\n\nOnce you have a function to query the data, you can use the `gr.DataFrame` component from the Gradio library to display the results in a tabular format. This is a useful way to inspect the data and make sure that it has been queried correctly.\n\nHere is an example of how to use the `gr.DataFrame` component to display the results. By passing in the `run_query` function to `gr.DataFrame`, we instruct Gradio to run the function as soon as the page loads and show the results. In addition, you also pass in the keyword `every` to tell the dashboard to refresh every hour (60\\*60 seconds).\n\n```py\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.DataFrame(run_query, every=60*60)\n\ndemo.queue().launch()  # Run the demo using queuing\n```\n\nPerhaps you'd like to add a visualization to our dashboard. You can use the `gr.ScatterPlot()` component to visualize the data in a scatter plot. This allows you to see the relationship between different variables such as case count and case deaths in the dataset and can be useful for exploring the data and gaining insights. Again, we can do this in real-time\nby passing in the `every` parameter.\n\nHere is a complete example showing how to use the `gr.ScatterPlot` to visualize in addition to displaying data with the `gr.DataFrame`\n\n```py\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# üíâ Covid Dashboard (Updated Hourly)\")\n    with gr.Row():\n        gr.DataFrame(run_query, every=60*60)\n        gr.ScatterPlot(run_query, every=60*60, x=\"confirmed_cases\",\n                        y=\"deaths\", tooltip=\"county\", width=500, height=500)\n\ndemo.queue().launch()  # Run the demo with queuing enabled\n```\n",tags:["TABULAR","DASHBOARD","PLOTS"],spaces:[],url:"/guides/creating-a-dashboard-from-bigquery-data/",contributor:null}],parent:"gradio"},slider:{class:null,name:"Slider",description:"Creates a slider that ranges from {minimum} to {maximum} with a step size of {step}. \u003Cbr>",tags:{demos:"sentence_builder, slider_release, interface_random_slider, blocks_random_slider",guides:"create-your-own-friends-with-a-gan"},parameters:[{name:"minimum",annotation:"float",doc:"minimum value for slider.",default:"0"},{name:"maximum",annotation:"float",doc:"maximum value for slider.",default:"100"},{name:"value",annotation:"float | Callable | None",doc:"default value. If callable, the function will be called whenever the app loads to set the initial value of the component. Ignored if randomized=True.",default:"None"},{name:"step",annotation:"float | None",doc:"increment between slider values.",default:"None"},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"info",annotation:"str | None",doc:"additional component description.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"interactive",annotation:"bool | None",doc:"if True, slider will be adjustable; if False, adjusting will be disabled. If not provided, this is inferred based on whether the component is used as an input or output.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"randomize",annotation:"bool",doc:"If True, the value of the slider when the app loads is taken uniformly at random from the range given by the minimum and maximum.",default:"False"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the Slider changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Slider"},{fn:null,name:"input",description:"This listener is triggered when the user changes the value of the Slider.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Slider"},{fn:null,name:"release",description:"This listener is triggered when the user releases the mouse on this Slider.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Slider"}],preprocess:{parameter_doc:[{name:"payload",annotation:"float",doc:"slider value"}],return_doc:{annotation:"float",doc:"Passes slider value as a {float} into the function."}},postprocess:{parameter_doc:[{name:"value",annotation:"float | None",doc:"Expects an {int} or {float} returned from function and sets slider value to it as long as it is within range (otherwise, sets to minimum value)."}],return_doc:{annotation:"float",doc:"The value of the slider within the range."}},string_shortcuts:[["Slider","slider","Uses default values"]],demos:[["sentence_builder","import gradio as gr\n\n\ndef sentence_builder(quantity, animal, countries, place, activity_list, morning):\n    return f\"\"\"The {quantity} {animal}s from {\" and \".join(countries)} went to the {place} where they {\" and \".join(activity_list)} until the {\"morning\" if morning else \"night\"}\"\"\"\n\n\ndemo = gr.Interface(\n    sentence_builder,\n    [\n        gr.Slider(2, 20, value=4, label=\"Count\", info=\"Choose between 2 and 20\"),\n        gr.Dropdown(\n            [\"cat\", \"dog\", \"bird\"], label=\"Animal\", info=\"Will add more animals later!\"\n        ),\n        gr.CheckboxGroup([\"USA\", \"Japan\", \"Pakistan\"], label=\"Countries\", info=\"Where are they from?\"),\n        gr.Radio([\"park\", \"zoo\", \"road\"], label=\"Location\", info=\"Where did they go?\"),\n        gr.Dropdown(\n            [\"ran\", \"swam\", \"ate\", \"slept\"], value=[\"swam\", \"slept\"], multiselect=True, label=\"Activity\", info=\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed auctor, nisl eget ultricies aliquam, nunc nisl aliquet nunc, eget aliquam nisl nunc vel nisl.\"\n        ),\n        gr.Checkbox(label=\"Morning\", info=\"Did they do it in the morning?\"),\n    ],\n    \"text\",\n    examples=[\n        [2, \"cat\", [\"Japan\", \"Pakistan\"], \"park\", [\"ate\", \"swam\"], True],\n        [4, \"dog\", [\"Japan\"], \"zoo\", [\"ate\", \"swam\"], False],\n        [10, \"bird\", [\"USA\", \"Pakistan\"], \"road\", [\"ran\"], False],\n        [8, \"cat\", [\"Pakistan\"], \"zoo\", [\"ate\"], True],\n    ]\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"],["slider_release","import gradio as gr\n\n\ndef identity(x, state):\n    state += 1\n    return x, state, state\n\n\nwith gr.Blocks() as demo:\n    slider = gr.Slider(0, 100, step=0.1)\n    state = gr.State(value=0)\n    with gr.Row():\n        number = gr.Number(label=\"On release\")\n        number2 = gr.Number(label=\"Number of events fired\")\n    slider.release(identity, inputs=[slider, state], outputs=[number, state, number2], api_name=\"predict\")\n\nif __name__ == \"__main__\":\n    print(\"here\")\n    demo.launch()\n    print(demo.server_port)\n"],["interface_random_slider","import gradio as gr\n\n\ndef func(slider_1, slider_2, *args):\n    return slider_1 + slider_2 * 5\n\n\ndemo = gr.Interface(\n    func,\n    [\n        gr.Slider(minimum=1.5, maximum=250000.89, randomize=True, label=\"Random Big Range\"),\n        gr.Slider(minimum=-1, maximum=1, randomize=True, step=0.05, label=\"Random only multiple of 0.05 allowed\"),\n        gr.Slider(minimum=0, maximum=1, randomize=True, step=0.25, label=\"Random only multiples of 0.25 allowed\"),\n        gr.Slider(minimum=-100, maximum=100, randomize=True, step=3, label=\"Random between -100 and 100 step 3\"),\n        gr.Slider(minimum=-100, maximum=100, randomize=True, label=\"Random between -100 and 100\"),\n        gr.Slider(value=0.25, minimum=5, maximum=30, step=-1),\n    ],\n    \"number\",\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"],["blocks_random_slider","\nimport gradio as gr\n\n\ndef func(slider_1, slider_2):\n    return slider_1 * 5 + slider_2\n\n\nwith gr.Blocks() as demo:\n    slider = gr.Slider(minimum=-10.2, maximum=15, label=\"Random Slider (Static)\", randomize=True)\n    slider_1 = gr.Slider(minimum=100, maximum=200, label=\"Random Slider (Input 1)\", randomize=True)\n    slider_2 = gr.Slider(minimum=10, maximum=23.2, label=\"Random Slider (Input 2)\", randomize=True)\n    slider_3 = gr.Slider(value=3, label=\"Non random slider\")\n    btn = gr.Button(\"Run\")\n    btn.click(func, inputs=[slider_1, slider_2], outputs=gr.Number())\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]],guides:[{name:"create-your-own-friends-with-a-gan",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:51,pretty_name:"Create Your Own Friends With A Gan",content:"# Create Your Own Friends with a GAN\n\n\n\n\n\n\n## Introduction\n\nIt seems that cryptocurrencies, [NFTs](https://www.nytimes.com/interactive/2022/03/18/technology/nft-guide.html), and the web3 movement are all the rage these days! Digital assets are being listed on marketplaces for astounding amounts of money, and just about every celebrity is debuting their own NFT collection. While your crypto assets [may be taxable, such as in Canada](https://www.canada.ca/en/revenue-agency/programs/about-canada-revenue-agency-cra/compliance/digital-currency/cryptocurrency-guide.html), today we'll explore some fun and tax-free ways to generate your own assortment of procedurally generated [CryptoPunks](https://www.larvalabs.com/cryptopunks).\n\nGenerative Adversarial Networks, often known just as _GANs_, are a specific class of deep-learning models that are designed to learn from an input dataset to create (_generate!_) new material that is convincingly similar to elements of the original training set. Famously, the website [thispersondoesnotexist.com](https://thispersondoesnotexist.com/) went viral with lifelike, yet synthetic, images of people generated with a model called StyleGAN2. GANs have gained traction in the machine learning world, and are now being used to generate all sorts of images, text, and even [music](https://salu133445.github.io/musegan/)!\n\nToday we'll briefly look at the high-level intuition behind GANs, and then we'll build a small demo around a pre-trained GAN to see what all the fuss is about. Here's a [peek](https://nimaboscarino-cryptopunks.hf.space) at what we're going to be putting together.\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). To use the pretrained model, also install `torch` and `torchvision`.\n\n## GANs: a very brief introduction\n\nOriginally proposed in [Goodfellow et al. 2014](https://arxiv.org/abs/1406.2661), GANs are made up of neural networks which compete with the intention of outsmarting each other. One network, known as the _generator_, is responsible for generating images. The other network, the _discriminator_, receives an image at a time from the generator along with a **real** image from the training data set. The discriminator then has to guess: which image is the fake?\n\nThe generator is constantly training to create images which are trickier for the discriminator to identify, while the discriminator raises the bar for the generator every time it correctly detects a fake. As the networks engage in this competitive (_adversarial!_) relationship, the images that get generated improve to the point where they become indistinguishable to human eyes!\n\nFor a more in-depth look at GANs, you can take a look at [this excellent post on Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/06/a-detailed-explanation-of-gan-with-implementation-using-tensorflow-and-keras/) or this [PyTorch tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html). For now, though, we'll dive into a demo!\n\n## Step 1 ‚Äî Create the Generator model\n\nTo generate new images with a GAN, you only need the generator model. There are many different architectures that the generator could use, but for this demo we'll use a pretrained GAN generator model with the following architecture:\n\n```python\nfrom torch import nn\n\nclass Generator(nn.Module):\n    # Refer to the link below for explanations about nc, nz, and ngf\n    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs\n    def __init__(self, nc=4, nz=100, ngf=64):\n        super(Generator, self).__init__()\n        self.network = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, input):\n        output = self.network(input)\n        return output\n```\n\nWe're taking the generator from [this repo by @teddykoker](https://github.com/teddykoker/cryptopunks-gan/blob/main/train.py#L90), where you can also see the original discriminator model structure.\n\nAfter instantiating the model, we'll load in the weights from the Hugging Face Hub, stored at [nateraw/cryptopunks-gan](https://huggingface.co/nateraw/cryptopunks-gan):\n\n```python\nfrom huggingface_hub import hf_hub_download\nimport torch\n\nmodel = Generator()\nweights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')\nmodel.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available\n```\n\n## Step 2 ‚Äî Defining a `predict` function\n\nThe `predict` function is the key to making Gradio work! Whatever inputs we choose through the Gradio interface will get passed through our `predict` function, which should operate on the inputs and generate outputs that we can display with Gradio output components. For GANs it's common to pass random noise into our model as the input, so we'll generate a tensor of random numbers and pass that through the model. We can then use `torchvision`'s `save_image` function to save the output of the model as a `png` file, and return the file name:\n\n```python\nfrom torchvision.utils import save_image\n\ndef predict(seed):\n    num_punks = 4\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n```\n\nWe're giving our `predict` function a `seed` parameter, so that we can fix the random tensor generation with a seed. We'll then be able to reproduce punks if we want to see them again by passing in the same seed.\n\n_Note!_ Our model needs an input tensor of dimensions 100x1x1 to do a single inference, or (BatchSize)x100x1x1 for generating a batch of images. In this demo we'll start by generating 4 punks at a time.\n\n## Step 3 ‚Äî Creating a Gradio interface\n\nAt this point you can even run the code you have with `predict(\u003CSOME_NUMBER>)`, and you'll find your freshly generated punks in your file system at `./punks.png`. To make a truly interactive demo, though, we'll build out a simple interface with Gradio. Our goals here are to:\n\n- Set a slider input so users can choose the \"seed\" value\n- Use an image component for our output to showcase the generated punks\n- Use our `predict()` to take the seed and generate the images\n\nWith `gr.Interface()`, we can define all of that with a single function call:\n\n```python\nimport gradio as gr\n\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n    ],\n    outputs=\"image\",\n).launch()\n```\n\n\n## Step 4 ‚Äî Even more punks!\n\nGenerating 4 punks at a time is a good start, but maybe we'd like to control how many we want to make each time. Adding more inputs to our Gradio interface is as simple as adding another item to the `inputs` list that we pass to `gr.Interface`:\n\n```python\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n        gr.Slider(4, 64, label='Number of Punks', step=1, default=10), # Adding another slider!\n    ],\n    outputs=\"image\",\n).launch()\n```\n\nThe new input will be passed to our `predict()` function, so we have to make some changes to that function to accept a new parameter:\n\n```python\ndef predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n```\n\nWhen you relaunch your interface, you should see a second slider that'll let you control the number of punks!\n\n## Step 5 - Polishing it up\n\nYour Gradio app is pretty much good to go, but you can add a few extra things to really make it ready for the spotlight ‚ú®\n\nWe can add some examples that users can easily try out by adding this to the `gr.Interface`:\n\n```python\ngr.Interface(\n    # ...\n    # keep everything as it is, and then add\n    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],\n).launch(cache_examples=True) # cache_examples is optional\n```\n\nThe `examples` parameter takes a list of lists, where each item in the sublists is ordered in the same order that we've listed the `inputs`. So in our case, `[seed, num_punks]`. Give it a try!\n\nYou can also try adding a `title`, `description`, and `article` to the `gr.Interface`. Each of those parameters accepts a string, so try it out and see what happens üëÄ `article` will also accept HTML, as [explored in a previous guide](/guides/key-features/#descriptive-content)!\n\nWhen you're all done, you may end up with something like [this](https://nimaboscarino-cryptopunks.hf.space).\n\nFor reference, here is our full code:\n\n```python\nimport torch\nfrom torch import nn\nfrom huggingface_hub import hf_hub_download\nfrom torchvision.utils import save_image\nimport gradio as gr\n\nclass Generator(nn.Module):\n    # Refer to the link below for explanations about nc, nz, and ngf\n    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs\n    def __init__(self, nc=4, nz=100, ngf=64):\n        super(Generator, self).__init__()\n        self.network = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, input):\n        output = self.network(input)\n        return output\n\nmodel = Generator()\nweights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')\nmodel.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available\n\ndef predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n        gr.Slider(4, 64, label='Number of Punks', step=1, default=10),\n    ],\n    outputs=\"image\",\n    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],\n).launch(cache_examples=True)\n```\n\n---\n\nCongratulations! You've built out your very own GAN-powered CryptoPunks generator, with a fancy Gradio interface that makes it easy for anyone to use. Now you can [scour the Hub for more GANs](https://huggingface.co/models?other=gan) (or train your own) and continue making even more awesome demos ü§ó\n",tags:["GAN","IMAGE","HUB"],spaces:["https://huggingface.co/spaces/NimaBoscarino/cryptopunks","https://huggingface.co/spaces/nateraw/cryptopunks-generator"],url:"/guides/create-your-own-friends-with-a-gan/",contributor:"\u003Ca href=\"https://huggingface.co/NimaBoscarino\">Nima Boscarino\u003C/a> and \u003Ca href=\"https://huggingface.co/nateraw\">Nate Raw\u003C/a>"}],parent:"gradio"},state:{class:null,name:"State",description:"A base class for defining methods that all input/output components should have.",tags:{},parameters:[{name:"value",annotation:"Any",doc:"the initial value (of arbitrary type) of the state. The provided argument is deepcopied. If a callable is provided, the function will be called whenever the app loads to set the initial value of the state.",default:"None"},{name:"render",annotation:"bool",doc:"has no effect, but is included for consistency with other components.",default:"True"},{name:"time_to_live",annotation:"int | float | None",doc:"The number of seconds the state should be stored for after it is created or updated. If None, the state will be stored indefinitely. Gradio automatically deletes state variables after a user closes the browser tab or refreshes the page, so this is useful for clearing state for potentially long running sessions.",default:"None"},{name:"delete_callback",annotation:"Callable[[Any], None] | None",doc:"A function that is called when the state is deleted. The function should take the state value as an argument.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the State changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.State"}],preprocess:{parameter_doc:[{name:"payload",annotation:"Any",doc:"Value"}],return_doc:{annotation:"Any",doc:"Passes a value of arbitrary type through."}},postprocess:{parameter_doc:[{name:"value",annotation:"Any",doc:"Expects a value of arbitrary type, as long as it can be deepcopied."}],return_doc:{annotation:"Any",doc:"Passes a value of arbitrary type through."}},parent:"gradio"},textbox:{class:null,name:"Textbox",description:"Creates a textarea for user to enter string input or display string output. \u003Cbr>",tags:{demos:"hello_world, diff_texts, sentence_builder",guides:"creating-a-chatbot, real-time-speech-recognition"},parameters:[{name:"value",annotation:"str | Callable | None",doc:"default text to provide in textarea. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"lines",annotation:"int",doc:"minimum number of line rows to provide in textarea.",default:"1"},{name:"max_lines",annotation:"int",doc:"maximum number of line rows to provide in textarea.",default:"20"},{name:"placeholder",annotation:"str | None",doc:"placeholder hint to provide behind textarea.",default:"None"},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"info",annotation:"str | None",doc:"additional component description.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label. If False, the copy button is hidden as well as well as the label.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"interactive",annotation:"bool | None",doc:"if True, will be rendered as an editable textbox; if False, editing will be disabled. If not provided, this is inferred based on whether the component is used as an input or output.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"autofocus",annotation:"bool",doc:"If True, will focus on the textbox when the page loads. Use this carefully, as it can cause usability issues for sighted and non-sighted users.",default:"False"},{name:"autoscroll",annotation:"bool",doc:"If True, will automatically scroll to the bottom of the textbox when the value changes, unless the user scrolls up. If False, will not scroll to the bottom of the textbox when the value changes.",default:"True"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"type",annotation:"Literal[('text', 'password', 'email')]",doc:"The type of textbox. One of: &#x27;text&#x27;, &#x27;password&#x27;, &#x27;email&#x27;, Default is &#x27;text&#x27;.",default:"\"text\""},{name:"text_align",annotation:"Literal[('left', 'right')] | None",doc:"How to align the text in the textbox, can be: &quot;left&quot;, &quot;right&quot;, or None (default). If None, the alignment is left if `rtl` is False, or right if `rtl` is True. Can only be changed if `type` is &quot;text&quot;.",default:"None"},{name:"rtl",annotation:"bool",doc:"If True and `type` is &quot;text&quot;, sets the direction of the text to right-to-left (cursor appears on the left of the text). Default is False, which renders cursor on the right.",default:"False"},{name:"show_copy_button",annotation:"bool",doc:"If True, includes a copy button to copy the text in the textbox. Only applies if show_label is True.",default:"False"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the Textbox changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Textbox"},{fn:null,name:"input",description:"This listener is triggered when the user changes the value of the Textbox.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Textbox"},{fn:null,name:"select",description:"Event listener for when the user selects or deselects the Textbox. Uses event data gradio.SelectData to carry `value` referring to the label of the Textbox, and `selected` to refer to state of the Textbox. See EventData documentation on how to use this event data",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Textbox"},{fn:null,name:"submit",description:"This listener is triggered when the user presses the Enter key while the Textbox is focused.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Textbox"},{fn:null,name:"focus",description:"This listener is triggered when the Textbox is focused.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Textbox"},{fn:null,name:"blur",description:"This listener is triggered when the Textbox is unfocused/blurred.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Textbox"}],preprocess:{parameter_doc:[{name:"payload",annotation:"str | None",doc:"the text entered in the textarea."}],return_doc:{annotation:"str | None",doc:"Passes text value as a {str} into the function."}},postprocess:{parameter_doc:[{name:"value",annotation:"str | None",doc:"Expects a {str} returned from function and sets textarea value to it."}],return_doc:{annotation:"str | None",doc:"The value to display in the textarea."}},string_shortcuts:[["Textbox","textbox","Uses default values"],["TextArea","textarea","Uses lines=7"]],demos:[["hello_world","import gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(fn=greet, inputs=\"textbox\", outputs=\"textbox\")\n    \nif __name__ == \"__main__\":\n    demo.launch()   "],["diff_texts","from difflib import Differ\n\nimport gradio as gr\n\n\ndef diff_texts(text1, text2):\n    d = Differ()\n    return [\n        (token[2:], token[0] if token[0] != \" \" else None)\n        for token in d.compare(text1, text2)\n    ]\n\n\ndemo = gr.Interface(\n    diff_texts,\n    [\n        gr.Textbox(\n            label=\"Text 1\",\n            info=\"Initial text\",\n            lines=3,\n            value=\"The quick brown fox jumped over the lazy dogs.\",\n        ),\n        gr.Textbox(\n            label=\"Text 2\",\n            info=\"Text to compare\",\n            lines=3,\n            value=\"The fast brown fox jumps over lazy dogs.\",\n        ),\n    ],\n    gr.HighlightedText(\n        label=\"Diff\",\n        combine_adjacent=True,\n        show_legend=True,\n        color_map={\"+\": \"red\", \"-\": \"green\"}),\n    theme=gr.themes.Base()\n)\nif __name__ == \"__main__\":\n    demo.launch()\n"],["sentence_builder","import gradio as gr\n\n\ndef sentence_builder(quantity, animal, countries, place, activity_list, morning):\n    return f\"\"\"The {quantity} {animal}s from {\" and \".join(countries)} went to the {place} where they {\" and \".join(activity_list)} until the {\"morning\" if morning else \"night\"}\"\"\"\n\n\ndemo = gr.Interface(\n    sentence_builder,\n    [\n        gr.Slider(2, 20, value=4, label=\"Count\", info=\"Choose between 2 and 20\"),\n        gr.Dropdown(\n            [\"cat\", \"dog\", \"bird\"], label=\"Animal\", info=\"Will add more animals later!\"\n        ),\n        gr.CheckboxGroup([\"USA\", \"Japan\", \"Pakistan\"], label=\"Countries\", info=\"Where are they from?\"),\n        gr.Radio([\"park\", \"zoo\", \"road\"], label=\"Location\", info=\"Where did they go?\"),\n        gr.Dropdown(\n            [\"ran\", \"swam\", \"ate\", \"slept\"], value=[\"swam\", \"slept\"], multiselect=True, label=\"Activity\", info=\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed auctor, nisl eget ultricies aliquam, nunc nisl aliquet nunc, eget aliquam nisl nunc vel nisl.\"\n        ),\n        gr.Checkbox(label=\"Morning\", info=\"Did they do it in the morning?\"),\n    ],\n    \"text\",\n    examples=[\n        [2, \"cat\", [\"Japan\", \"Pakistan\"], \"park\", [\"ate\", \"swam\"], True],\n        [4, \"dog\", [\"Japan\"], \"zoo\", [\"ate\", \"swam\"], False],\n        [10, \"bird\", [\"USA\", \"Pakistan\"], \"road\", [\"ran\"], False],\n        [8, \"cat\", [\"Pakistan\"], \"zoo\", [\"ate\"], True],\n    ]\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]],guides:[{name:"real-time-speech-recognition",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:60,pretty_name:"Real Time Speech Recognition",content:"# Real Time Speech Recognition\n\n\n\n## Introduction\n\nAutomatic speech recognition (ASR), the conversion of spoken speech to text, is a very important and thriving area of machine learning. ASR algorithms run on practically every smartphone, and are becoming increasingly embedded in professional workflows, such as digital assistants for nurses and doctors. Because ASR algorithms are designed to be used directly by customers and end users, it is important to validate that they are behaving as expected when confronted with a wide variety of speech patterns (different accents, pitches, and background audio conditions).\n\nUsing `gradio`, you can easily build a demo of your ASR model and share that with a testing team, or test it yourself by speaking through the microphone on your device.\n\nThis tutorial will show how to take a pretrained speech-to-text model and deploy it with a Gradio interface. We will start with a **_full-context_** model, in which the user speaks the entire audio before the prediction runs. Then we will adapt the demo to make it **_streaming_**, meaning that the audio model will convert speech as you speak. \n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). You will also need a pretrained speech recognition model. In this tutorial, we will build demos from 2 ASR libraries:\n\n- Transformers (for this, `pip install transformers` and `pip install torch`)\n\nMake sure you have at least one of these installed so that you can follow along the tutorial. You will also need `ffmpeg` [installed on your system](https://www.ffmpeg.org/download.html), if you do not already have it, to process files from the microphone.\n\nHere's how to build a real time speech recognition (ASR) app:\n\n1. [Set up the Transformers ASR Model](#1-set-up-the-transformers-asr-model)\n2. [Create a Full-Context ASR Demo with Transformers](#2-create-a-full-context-asr-demo-with-transformers)\n3. [Create a Streaming ASR Demo with Transformers](#3-create-a-streaming-asr-demo-with-transformers)\n\n## 1. Set up the Transformers ASR Model\n\nFirst, you will need to have an ASR model that you have either trained yourself or you will need to download a pretrained model. In this tutorial, we will start by using a pretrained ASR model from the model, `whisper`.\n\nHere is the code to load `whisper` from Hugging Face `transformers`.\n\n```python\nfrom transformers import pipeline\n\np = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n```\n\nThat's it!\n\n## 2. Create a Full-Context ASR Demo with Transformers\n\nWe will start by creating a _full-context_ ASR demo, in which the user speaks the full audio before using the ASR model to run inference. This is very easy with Gradio -- we simply create a function around the `pipeline` object above.\n\nWe will use `gradio`'s built in `Audio` component, configured to take input from the user's microphone and return a filepath for the recorded audio. The output component will be a plain `Textbox`.\n\n```python\nimport gradio as gr\nfrom transformers import pipeline\nimport numpy as np\n\ntranscriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n\ndef transcribe(audio):\n    sr, y = audio\n    y = y.astype(np.float32)\n    y /= np.max(np.abs(y))\n\n    return transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]\n\n\ndemo = gr.Interface(\n    transcribe,\n    gr.Audio(sources=[\"microphone\"]),\n    \"text\",\n)\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/asr'>\u003C/gradio-app>\n\nThe `transcribe` function takes a single parameter, `audio`, which is a numpy array of the audio the user recorded. The `pipeline` object expects this in float32 format, so we convert it first to float32, and then extract the transcribed text.\n\n## 3. Create a Streaming ASR Demo with Transformers\n\nTo make this a *streaming* demo, we need to make these changes:\n\n1. Set `streaming=True` in the `Audio` component\n2. Set `live=True` in the `Interface`\n3. Add a `state` to the interface to store the recorded audio of a user\n\nTake a look below.\n\n```python\nimport gradio as gr\nfrom transformers import pipeline\nimport numpy as np\n\ntranscriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n\ndef transcribe(stream, new_chunk):\n    sr, y = new_chunk\n    y = y.astype(np.float32)\n    y /= np.max(np.abs(y))\n\n    if stream is not None:\n        stream = np.concatenate([stream, y])\n    else:\n        stream = y\n    return stream, transcriber({\"sampling_rate\": sr, \"raw\": stream})[\"text\"]\n\n\ndemo = gr.Interface(\n    transcribe,\n    [\"state\", gr.Audio(sources=[\"microphone\"], streaming=True)],\n    [\"state\", \"text\"],\n    live=True,\n)\n\ndemo.launch()\n\n```\n\nNotice now we have a state variable now, because we need to track all the audio history. `transcribe` gets called whenever there is a new small chunk of audio, but we also need to keep track of all the audio that has been spoken so far in state. \nAs the interface runs, the `transcribe` function gets called, with a record of all the previously spoken audio in `stream`, as well as the new chunk of audio as `new_chunk`. We return the new full audio so that can be stored back in state, and we also return the transcription.\nHere we naively append the audio together and simply call the `transcriber` object on the entire audio. You can imagine more efficient ways of handling this, such as re-processing only the last 5 seconds of audio whenever a new chunk of audio received. \n\n\u003Cgradio-app space='gradio/stream_asr'>\u003C/gradio-app>\n\nNow the ASR model will run inference as you speak! ",tags:["ASR","SPEECH","STREAMING"],spaces:[],url:"/guides/real-time-speech-recognition/",contributor:null}],parent:"gradio"},uploadbutton:{class:null,name:"UploadButton",description:"Used to create an upload button, when clicked allows a user to upload files that satisfy the specified file type or generic files (if file_type not set). \u003Cbr>",tags:{demos:"upload_and_download, upload_button"},parameters:[{name:"label",annotation:"str",doc:"Text to display on the button. Defaults to &quot;Upload a File&quot;.",default:"\"Upload a File\""},{name:"value",annotation:"str | list[str] | Callable | None",doc:"File or list of files to upload by default.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"variant",annotation:"Literal[('primary', 'secondary', 'stop')]",doc:"&#x27;primary&#x27; for main call-to-action, &#x27;secondary&#x27; for a more subdued style, &#x27;stop&#x27; for a stop button.",default:"\"secondary\""},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"size",annotation:"Literal[('sm', 'lg')] | None",doc:"Size of the button. Can be &quot;sm&quot; or &quot;lg&quot;.",default:"None"},{name:"icon",annotation:"str | None",doc:"URL or path to the icon file to display within the button. If None, no icon will be displayed.",default:"None"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int | None",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"None"},{name:"interactive",annotation:"bool",doc:"If False, the UploadButton will be in a disabled state.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"type",annotation:"Literal[('filepath', 'bytes')]",doc:"Type of value to be returned by component. &quot;file&quot; returns a temporary file object with the same base name as the uploaded file, whose full path can be retrieved by file_obj.name, &quot;binary&quot; returns an bytes object.",default:"\"filepath\""},{name:"file_count",annotation:"Literal[('single', 'multiple', 'directory')]",doc:"if single, allows user to upload one file. If &quot;multiple&quot;, user uploads multiple files. If &quot;directory&quot;, user uploads all files in selected directory. Return type will be list for each file in case of &quot;multiple&quot; or &quot;directory&quot;.",default:"\"single\""},{name:"file_types",annotation:"list[str] | None",doc:"List of type of files to be uploaded. &quot;file&quot; allows any file to be uploaded, &quot;image&quot; allows only image files to be uploaded, &quot;audio&quot; allows only audio files to be uploaded, &quot;video&quot; allows only video files to be uploaded, &quot;text&quot; allows only text files to be uploaded.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"click",description:"Triggered when the UploadButton is clicked.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.UploadButton"},{fn:null,name:"upload",description:"This listener is triggered when the user uploads a file into the UploadButton.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.UploadButton"}],preprocess:{parameter_doc:[{name:"payload",annotation:"ListFiles | FileData | None",doc:"File information as a FileData object, or a list of FileData objects."}],return_doc:{annotation:"bytes | str | list[bytes] | list[str] | None",doc:"Passes the file as a `str` or `bytes` object, or a list of `str` or list of `bytes` objects, depending on `type` and `file_count`."}},postprocess:{parameter_doc:[{name:"value",annotation:"str | list[str] | None",doc:"Expects a `str` filepath or URL, or a `list[str]` of filepaths/URLs."}],return_doc:{annotation:"ListFiles | FileData | None",doc:"File information as a FileData object, or a list of FileData objects."}},string_shortcuts:[["UploadButton","uploadbutton","Uses default values"]],demos:[["upload_and_download","from pathlib import Path\nimport gradio as gr\n\ndef upload_file(filepath):\n    name = Path(filepath).name\n    return [gr.UploadButton(visible=False), gr.DownloadButton(label=f\"Download {name}\", value=filepath, visible=True)]\n\ndef download_file():\n    return [gr.UploadButton(visible=True), gr.DownloadButton(visible=False)]\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"First upload a file and and then you'll be able download it (but only once!)\")\n    with gr.Row():\n        u = gr.UploadButton(\"Upload a file\", file_count=\"single\")\n        d = gr.DownloadButton(\"Download the file\", visible=False)\n\n    u.upload(upload_file, u, [u, d])\n    d.click(download_file, None, [u, d])\n\n\nif __name__ == \"__main__\":\n    demo.launch()"],["upload_button","import gradio as gr\n\ndef upload_file(files):\n    file_paths = [file.name for file in files]\n    return file_paths\n\nwith gr.Blocks() as demo:\n    file_output = gr.File()\n    upload_button = gr.UploadButton(\"Click to Upload a File\", file_types=[\"image\", \"video\"], file_count=\"multiple\")\n    upload_button.upload(upload_file, upload_button, file_output)\n\ndemo.launch()\n"]],parent:"gradio"},video:{class:null,name:"Video",description:"Creates a video component that can be used to upload/record videos (as an input) or display videos (as an output). For the video to be playable in the browser it must have a compatible container and codec combination. Allowed combinations are .mp4 with h264 codec, .ogg with theora codec, and .webm with vp9 codec. If the component detects that the output video would not be playable in the browser it will attempt to convert it to a playable mp4 video. If the conversion fails, the original video is returned. \u003Cbr>",tags:{demos:"video_identity_2"},parameters:[{name:"value",annotation:"str | Path | tuple[str | Path, str | Path | None] | Callable | None",doc:"A path or URL for the default value that Video component is going to take. Can also be a tuple consisting of (video filepath, subtitle filepath). If a subtitle file is provided, it should be of type .srt or .vtt. Or can be callable, in which case the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"format",annotation:"str | None",doc:"Format of video format to be returned by component, such as &#x27;avi&#x27; or &#x27;mp4&#x27;. Use &#x27;mp4&#x27; to ensure browser playability. If set to None, video will keep uploaded format.",default:"None"},{name:"sources",annotation:"list[Literal[('upload', 'webcam')]] | None",doc:"A list of sources permitted for video. &quot;upload&quot; creates a box where user can drop an video file, &quot;webcam&quot; allows user to record a video from their webcam. If None, defaults to [&quot;upload, &quot;webcam&quot;].",default:"None"},{name:"height",annotation:"int | str | None",doc:"The height of the displayed video, specified in pixels if a number is passed, or in CSS units if a string is passed.",default:"None"},{name:"width",annotation:"int | str | None",doc:"The width of the displayed video, specified in pixels if a number is passed, or in CSS units if a string is passed.",default:"None"},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"interactive",annotation:"bool | None",doc:"if True, will allow users to upload a video; if False, can only be used to display videos. If not provided, this is inferred based on whether the component is used as an input or output.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"},{name:"mirror_webcam",annotation:"bool",doc:"If True webcam will be mirrored. Default is True.",default:"True"},{name:"include_audio",annotation:"bool | None",doc:"Whether the component should record/retain the audio track for a video. By default, audio is excluded for webcam videos and included for uploaded videos.",default:"None"},{name:"autoplay",annotation:"bool",doc:"Whether to automatically play the video when the component is used as an output. Note: browsers will not autoplay video files if the user has not interacted with the page yet.",default:"False"},{name:"show_share_button",annotation:"bool | None",doc:"If True, will show a share icon in the corner of the component that allows user to share outputs to Hugging Face Spaces Discussions. If False, icon does not appear. If set to None (default behavior), then the icon appears if this Gradio app is launched on Spaces, but not otherwise.",default:"None"},{name:"show_download_button",annotation:"bool | None",doc:"If True, will show a download icon in the corner of the component that allows user to download the output. If False, icon does not appear. By default, it will be True for output components and False for input components.",default:"None"},{name:"min_length",annotation:"int | None",doc:"The minimum length of video (in seconds) that the user can pass into the prediction function. If None, there is no minimum length.",default:"None"},{name:"max_length",annotation:"int | None",doc:"The maximum length of video (in seconds) that the user can pass into the prediction function. If None, there is no maximum length.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"change",description:"Triggered when the value of the Video changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Video"},{fn:null,name:"clear",description:"This listener is triggered when the user clears the Video using the X button for the component.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Video"},{fn:null,name:"start_recording",description:"This listener is triggered when the user starts recording with the Video.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Video"},{fn:null,name:"stop_recording",description:"This listener is triggered when the user stops recording with the Video.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Video"},{fn:null,name:"stop",description:"This listener is triggered when the user reaches the end of the media playing in the Video.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Video"},{fn:null,name:"play",description:"This listener is triggered when the user plays the media in the Video.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Video"},{fn:null,name:"pause",description:"This listener is triggered when the media in the Video stops for any reason.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Video"},{fn:null,name:"end",description:"This listener is triggered when the user reaches the end of the media playing in the Video.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Video"},{fn:null,name:"upload",description:"This listener is triggered when the user uploads a file into the Video.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.Video"}],preprocess:{parameter_doc:[{name:"payload",annotation:"VideoData | None",doc:"An instance of VideoData containing the video and subtitle files."}],return_doc:{annotation:"str | None",doc:"Passes the uploaded video as a `str` filepath or URL whose extension can be modified by `format`."}},postprocess:{parameter_doc:[{name:"value",annotation:"str | Path | tuple[str | Path, str | Path | None] | None",doc:"Expects a {str} or {pathlib.Path} filepath to a video which is displayed, or a {Tuple[str | pathlib.Path, str | pathlib.Path | None]} where the first element is a filepath to a video and the second element is an optional filepath to a subtitle file."}],return_doc:{annotation:"VideoData | None",doc:"VideoData object containing the video and subtitle files."}},string_shortcuts:[["Video","video","Uses default values"],["PlayableVideo","playablevideo","Uses format=\"mp4\""]],demos:[["video_identity_2","import gradio as gr\n\ndef video_identity(video):\n    return video\n\n\ndemo = gr.Interface(video_identity, \n                    gr.Video(), \n                    \"playable_video\", \n                    )\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]],parent:"gradio"},simpleimage:{class:null,name:"SimpleImage",description:"Creates an image component that can be used to upload images (as an input) or display images (as an output).",tags:{},parameters:[{name:"value",annotation:"str | None",doc:"A path or URL for the default value that SimpleImage component is going to take. If callable, the function will be called whenever the app loads to set the initial value of the component.",default:"None"},{name:"label",annotation:"str | None",doc:"The label for this component. Appears above the component and is also used as the header if there are a table of examples for this component. If None and used in a `gr.Interface`, the label will be the name of the parameter this component is assigned to.",default:"None"},{name:"every",annotation:"float | None",doc:"If `value` is a callable, run the function &#x27;every&#x27; number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component&#x27;s .load_event attribute.",default:"None"},{name:"show_label",annotation:"bool | None",doc:"if True, will display label.",default:"None"},{name:"show_download_button",annotation:"bool",doc:"If True, will display button to download image.",default:"True"},{name:"container",annotation:"bool",doc:"If True, will place the component in a container - providing some extra padding around the border.",default:"True"},{name:"scale",annotation:"int | None",doc:"relative size compared to adjacent Components. For example if Components A and B are in a Row, and A has scale=2, and B has scale=1, A will be twice as wide as B. Should be an integer. scale applies in Rows, and to top-level Components in Blocks where fill_height=True.",default:"None"},{name:"min_width",annotation:"int",doc:"minimum pixel width, will wrap if not sufficient screen space to satisfy this value. If a certain scale value results in this Component being narrower than min_width, the min_width parameter will be respected first.",default:"160"},{name:"interactive",annotation:"bool | None",doc:"if True, will allow users to upload and edit an image; if False, can only be used to display images. If not provided, this is inferred based on whether the component is used as an input or output.",default:"None"},{name:"visible",annotation:"bool",doc:"If False, component will be hidden.",default:"True"},{name:"elem_id",annotation:"str | None",doc:"An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"elem_classes",annotation:"list[str] | str | None",doc:"An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.",default:"None"},{name:"render",annotation:"bool",doc:"If False, component will not render be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.",default:"True"},{name:"key",annotation:"int | str | None",doc:"if assigned, will be used to assume identity across a re-render. Components that have the same key across a re-render will have their value preserved.",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"clear",description:"This listener is triggered when the user clears the SimpleImage using the X button for the component.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.SimpleImage"},{fn:null,name:"change",description:"Triggered when the value of the SimpleImage changes either because of user input (e.g. a user types in a textbox) OR because of a function update (e.g. an image receives a value from the output of an event trigger). See `.input()` for a listener that is only triggered by user input.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.SimpleImage"},{fn:null,name:"upload",description:"This listener is triggered when the user uploads a file into the SimpleImage.",tags:{},parameters:[{name:"fn",annotation:"Callable | None | Literal['decorator']",doc:"the function to call when this event is triggered. Often a machine learning model&#x27;s prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component.",default:"\"decorator\""},{name:"inputs",annotation:"Component | list[Component] | set[Component] | None",doc:"List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list.",default:"None"},{name:"outputs",annotation:"Block | list[Block] | list[Component] | None",doc:"List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list.",default:"None"},{name:"api_name",annotation:"str | None | Literal[False]",doc:"defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event.",default:"None"},{name:"scroll_to_output",annotation:"bool",doc:"If True, will scroll to output component on completion",default:"False"},{name:"show_progress",annotation:"Literal[('full', 'minimal', 'hidden')]",doc:"If True, will show progress animation while pending",default:"\"full\""},{name:"queue",annotation:"bool",doc:"If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app.",default:"True"},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component.",default:"False"},{name:"max_batch_size",annotation:"int",doc:"Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True)",default:"4"},{name:"preprocess",annotation:"bool",doc:"If False, will not run preprocessing of component data before running &#x27;fn&#x27; (e.g. leaving it as a base64 string if this method is called with the `Image` component).",default:"True"},{name:"postprocess",annotation:"bool",doc:"If False, will not run postprocessing of component data before returning &#x27;fn&#x27; output to the browser.",default:"True"},{name:"cancels",annotation:"dict[str, Any] | list[dict[str, Any]] | None",doc:"A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish.",default:"None"},{name:"every",annotation:"float | None",doc:"Run this event &#x27;every&#x27; number of seconds while the client connection is open. Interpreted in seconds.",default:"None"},{name:"trigger_mode",annotation:"Literal[('once', 'multiple', 'always_last')] | None",doc:"If &quot;once&quot; (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to &quot;multiple&quot;, unlimited submissions are allowed while pending, and &quot;always_last&quot; (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete.",default:"None"},{name:"js",annotation:"str | None",doc:"Optional frontend js method to run before running &#x27;fn&#x27;. Input arguments for js method are values of &#x27;inputs&#x27; and &#x27;outputs&#x27;, return should be a list of values for output components.",default:"None"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default).",default:"\"default\""},{name:"concurrency_id",annotation:"str | None",doc:"If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit.",default:"None"},{name:"show_api",annotation:"bool",doc:"whether to show this event in the &quot;view API&quot; page of the Gradio app, or in the &quot;.view_api()&quot; method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False.",default:"True"}],returns:{},example:null,override_signature:null,parent:"gradio.SimpleImage"}],preprocess:{parameter_doc:[{name:"payload",annotation:"FileData | None",doc:"A FileData object containing the image data."}],return_doc:{annotation:"str | None",doc:"A `str` containing the path to the image."}},postprocess:{parameter_doc:[{name:"value",annotation:"str | Path | None",doc:"Expects a `str` or `pathlib.Path` object containing the path to the image."}],return_doc:{annotation:"FileData | None",doc:"A FileData object containing the image data."}},string_shortcuts:[["SimpleImage","simpleimage","Uses default values"]],parent:"gradio"}},helpers:{set_static_paths:{class:null,name:"set_static_paths",description:"Set the static paths to be served by the gradio app. \u003Cbr> Static files are not moved to the gradio cache and are served directly from the file system. This function is useful when you want to serve files that you know will not be modified during the lifetime of the gradio app (like files used in gr.Examples). By setting static paths, your app will launch faster and it will consume less disk space. Calling this function will set the static paths for all gradio applications defined in the same interpreter session until it is called again or the session ends. To clear out the static paths, call this function with an empty list. \u003Cbr>",tags:{parameters:"paths: List of filepaths or directory names to be served by the gradio app. If it is a directory name, ALL files located within that directory will be considered static and not moved to the gradio cache. This also means that ALL files in that directory will be accessible over the network."},parameters:[{name:"paths",annotation:"list[str | Path]",doc:"List of filepaths or directory names to be served by the gradio app. If it is a directory name, ALL files located within that directory will be considered static and not moved to the gradio cache. This also means that ALL files in that directory will be accessible over the network."}],returns:{annotation:null},example:"import gradio as gr\n\n# Paths can be a list of strings or pathlib.Path objects\n# corresponding to filenames or directories.\ngr.set_static_paths(paths=[\"test/test_files/\"])\n\n# The example files and the default value of the input\n# will not be copied to the gradio cache and will be served directly.\ndemo = gr.Interface(\n    lambda s: s.rotate(45),\n    gr.Image(value=\"test/test_files/cheetah1.jpg\", type=\"pil\"),\n    gr.Image(),\n    examples=[\"test/test_files/bus.png\"],\n)\n\ndemo.launch()",fns:[],parent:"gradio"},eventdata:{class:null,name:"EventData",description:"When a subclass of EventData is added as a type hint to an argument of an event listener method, this object will be passed as that argument. It contains information about the event that triggered the listener, such the target object, and other data related to the specific event that are attributes of the subclass. \u003Cbr>",tags:{demos:"gallery_selections, tictactoe"},parameters:[{name:"target",annotation:"Block | None",doc:"The target object that triggered the event. Can be used to distinguish if multiple components are bound to the same listener."}],returns:{annotation:null},example:"table = gr.Dataframe([[1, 2, 3], [4, 5, 6]])\ngallery = gr.Gallery([(\"cat.jpg\", \"Cat\"), (\"dog.jpg\", \"Dog\")])\ntextbox = gr.Textbox(\"Hello World!\")\n\nstatement = gr.Textbox()\n\ndef on_select(evt: gr.SelectData):  # SelectData is a subclass of EventData\n    return f\"You selected {evt.value} at {evt.index} from {evt.target}\"\n\ntable.select(on_select, None, statement)\ngallery.select(on_select, None, statement)\ntextbox.select(on_select, None, statement)",fns:[],demos:[["gallery_selections","import gradio as gr\nimport numpy as np\n\nwith gr.Blocks() as demo:\n    imgs = gr.State()\n    gallery = gr.Gallery(allow_preview=False)\n\n    def deselect_images():\n        return gr.Gallery(selected_index=None)\n\n    def generate_images():\n        images = []\n        for _ in range(9):\n            image = np.ones((100, 100, 3), dtype=np.uint8) * np.random.randint(\n                0, 255, 3\n            )  # image is a solid single color\n            images.append(image)\n        return images, images\n\n    demo.load(generate_images, None, [gallery, imgs])\n\n    with gr.Row():\n        selected = gr.Number(show_label=False)\n        darken_btn = gr.Button(\"Darken selected\")\n    deselect_button = gr.Button(\"Deselect\")\n\n    deselect_button.click(deselect_images, None, gallery)\n\n    def get_select_index(evt: gr.SelectData):\n        return evt.index\n\n    gallery.select(get_select_index, None, selected)\n\n    def darken_img(imgs, index):\n        index = int(index)\n        imgs[index] = np.round(imgs[index] * 0.8).astype(np.uint8)\n        return imgs, imgs\n\n    darken_btn.click(darken_img, [imgs, selected], [imgs, gallery])\n\nif __name__ == \"__main__\":\n    demo.launch()\n"],["tictactoe","import gradio as gr\n\nwith gr.Blocks() as demo:\n    turn = gr.Textbox(\"X\", interactive=False, label=\"Turn\")\n    board = gr.Dataframe(value=[[\"\", \"\", \"\"]] * 3, interactive=False, type=\"array\")\n\n    def place(board, turn, evt: gr.SelectData):\n        if evt.value:\n            return board, turn\n        board[evt.index[0]][evt.index[1]] = turn\n        turn = \"O\" if turn == \"X\" else \"X\"\n        return board, turn\n\n    board.select(place, [board, turn], [board, turn], show_progress=\"hidden\")\n\nif __name__ == \"__main__\":\n    demo.launch()"]],parent:"gradio"},examples:{class:null,name:"Examples",description:"This class is a wrapper over the Dataset component and can be used to create Examples for Blocks / Interfaces. Populates the Dataset component with examples and assigns event listener so that clicking on an example populates the input/output components. Optionally handles example caching for fast inference. \u003Cbr>",tags:{demos:"fake_gan",guides:"more-on-examples-and-flagging, using-hugging-face-integrations, image-classification-in-pytorch, image-classification-in-tensorflow, image-classification-with-vision-transformers, create-your-own-friends-with-a-gan"},parameters:[{name:"examples",annotation:"list[Any] | list[list[Any]] | str",doc:"example inputs that can be clicked to populate specific components. Should be nested list, in which the outer list consists of samples and each inner list consists of an input corresponding to each input component. A string path to a directory of examples can also be provided but it should be within the directory with the python file running the gradio app. If there are multiple input components and a directory is provided, a log.csv file must be present in the directory to link corresponding inputs."},{name:"inputs",annotation:"Component | list[Component]",doc:"the component or list of components corresponding to the examples"},{name:"outputs",annotation:"Component | list[Component] | None",doc:"optionally, provide the component or list of components corresponding to the output of the examples. Required if `cache_examples` is not False.",default:"None"},{name:"fn",annotation:"Callable | None",doc:"optionally, provide the function to run to generate the outputs corresponding to the examples. Required if `cache_examples` is not False. Also required if `run_on_click` is True.",default:"None"},{name:"cache_examples",annotation:"bool | Literal['lazy'] | None",doc:"If True, caches examples in the server for fast runtime in examples. If &quot;lazy&quot;, then examples are cached after their first use. Can also be set by the GRADIO_CACHE_EXAMPLES environment variable, which takes a case-insensitive value, one of: {&quot;true&quot;, &quot;lazy&quot;, or &quot;false&quot;} (for the first two to take effect, `fn` and `outputs` should also be provided). In HuggingFace Spaces, this is True (as long as `fn` and `outputs` are also provided). The default option otherwise is False.",default:"None"},{name:"examples_per_page",annotation:"int",doc:"how many examples to show per page.",default:"10"},{name:"label",annotation:"str | None",doc:"the label to use for the examples component (by default, &quot;Examples&quot;)",default:"\"Examples\""},{name:"elem_id",annotation:"str | None",doc:"an optional string that is assigned as the id of this component in the HTML DOM.",default:"None"},{name:"run_on_click",annotation:"bool",doc:"if cache_examples is False, clicking on an example does not run the function when an example is clicked. Set this to True to run the function when an example is clicked. Has no effect if cache_examples is True.",default:"False"},{name:"preprocess",annotation:"bool",doc:"if True, preprocesses the example input before running the prediction function and caching the output. Only applies if `cache_examples` is not False.",default:"True"},{name:"postprocess",annotation:"bool",doc:"if True, postprocesses the example output after running the prediction function and before caching. Only applies if `cache_examples` is not False.",default:"True"},{name:"api_name",annotation:"str | Literal[False]",doc:"Defines how the event associated with clicking on the examples appears in the API docs. Can be a string or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use the example function.",default:"\"load_example\""},{name:"batch",annotation:"bool",doc:"If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. Used only if cache_examples is not False.",default:"False"}],returns:{annotation:null},example:null,fns:[],demos:[["fake_gan","# This demo needs to be run from the repo folder.\n# python demo/fake_gan/run.py\nimport random\n\nimport gradio as gr\n\n\ndef fake_gan():\n    images = [\n        (random.choice(\n            [\n                \"http://www.marketingtool.online/en/face-generator/img/faces/avatar-1151ce9f4b2043de0d2e3b7826127998.jpg\",\n                \"http://www.marketingtool.online/en/face-generator/img/faces/avatar-116b5e92936b766b7fdfc242649337f7.jpg\",\n                \"http://www.marketingtool.online/en/face-generator/img/faces/avatar-1163530ca19b5cebe1b002b8ec67b6fc.jpg\",\n                \"http://www.marketingtool.online/en/face-generator/img/faces/avatar-1116395d6e6a6581eef8b8038f4c8e55.jpg\",\n                \"http://www.marketingtool.online/en/face-generator/img/faces/avatar-11319be65db395d0e8e6855d18ddcef0.jpg\",\n            ]\n        ), f\"label {i}\")\n        for i in range(3)\n    ]\n    return images\n\n\nwith gr.Blocks() as demo:\n    gallery = gr.Gallery(\n        label=\"Generated images\", show_label=False, elem_id=\"gallery\"\n    , columns=[3], rows=[1], object_fit=\"contain\", height=\"auto\")\n    btn = gr.Button(\"Generate images\", scale=0)\n\n    btn.click(fake_gan, None, gallery)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]],guides:[{name:"using-hugging-face-integrations",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:1,absolute_index:47,pretty_name:"Using Hugging Face Integrations",content:"# Using Hugging Face Integrations\n\n\n\n\n\n\n## Introduction\n\nThe Hugging Face Hub is a central platform that has hundreds of thousands of [models](https://huggingface.co/models), [datasets](https://huggingface.co/datasets) and [demos](https://huggingface.co/spaces) (also known as Spaces). \n\nGradio has multiple features that make it extremely easy to leverage existing models and Spaces on the Hub. This guide walks through these features.\n\n\n## Demos with the Hugging Face Inference Endpoints\n\nHugging Face has a service called [Serverless Inference Endpoints](https://huggingface.co/docs/api-inference/index), which allows you to send HTTP requests to models on the Hub. The API includes a generous free tier, and you can switch to [dedicated Inference Endpoints](https://huggingface.co/inference-endpoints/dedicated) when you want to use it in production. Gradio integrates directly with Serverless Inference Endpoints so that you can create a demo simply by specifying a model's name (e.g. `Helsinki-NLP/opus-mt-en-es`), like this:\n\n```python\nimport gradio as gr\n\ndemo = gr.load(\"Helsinki-NLP/opus-mt-en-es\", src=\"models\")\n\ndemo.launch()\n```\n\nFor any Hugging Face model supported in Inference Endpoints, Gradio automatically infers the expected input and output and make the underlying server calls, so you don't have to worry about defining the prediction function. \n\nNotice that we just put specify the model name and state that the `src` should be `models` (Hugging Face's Model Hub). There is no need to install any dependencies (except `gradio`) since you are not loading the model on your computer.\n\nYou might notice that the first inference takes a little bit longer. This happens since the Inference Endpoints is loading the model in the server. You get some benefits afterward:\n\n- The inference will be much faster.\n- The server caches your requests.\n- You get built-in automatic scaling.\n\n## Hosting your Gradio demos on Spaces\n\n[Hugging Face Spaces](https://hf.co/spaces) allows anyone to host their Gradio demos freely, and uploading your Gradio demos take a couple of minutes. You can head to [hf.co/new-space](https://huggingface.co/new-space), select the Gradio SDK, create an `app.py` file, and voila! You have a demo you can share with anyone else. To learn more, read [this guide how to host on Hugging Face Spaces using the website](https://huggingface.co/blog/gradio-spaces).\n\nAlternatively, you can create a Space programmatically, making use of the [huggingface_hub client library](https://huggingface.co/docs/huggingface_hub/index) library. Here's an example:\n\n```python\nfrom huggingface_hub import (\n    create_repo,\n    get_full_repo_name,\n    upload_file,\n)\ncreate_repo(name=target_space_name, token=hf_token, repo_type=\"space\", space_sdk=\"gradio\")\nrepo_name = get_full_repo_name(model_id=target_space_name, token=hf_token)\nfile_url = upload_file(\n    path_or_fileobj=\"file.txt\",\n    path_in_repo=\"app.py\",\n    repo_id=repo_name,\n    repo_type=\"space\",\n    token=hf_token,\n)\n```\n\nHere, `create_repo` creates a gradio repo with the target name under a specific account using that account's Write Token. `repo_name` gets the full repo name of the related repo. Finally `upload_file` uploads a file inside the repo with the name `app.py`.\n\n\n## Loading demos from Spaces\n\nYou can also use and remix existing Gradio demos on Hugging Face Spaces. For example, you could take two existing Gradio demos on Spaces and put them as separate tabs and create a new demo. You can run this new demo locally, or upload it to Spaces, allowing endless possibilities to remix and create new demos!\n\nHere's an example that does exactly that:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n  with gr.Tab(\"Translate to Spanish\"):\n    gr.load(\"gradio/en2es\", src=\"spaces\")\n  with gr.Tab(\"Translate to French\"):\n    gr.load(\"abidlabs/en2fr\", src=\"spaces\")\n\ndemo.launch()\n```\n\nNotice that we use `gr.load()`, the same method we used to load models using Inference Endpoints. However, here we specify that the `src` is `spaces` (Hugging Face Spaces). \n\nNote: loading a Space in this way may result in slight differences from the original Space. In particular, any attributes that apply to the entire Blocks, such as the theme or custom CSS/JS, will not be loaded. You can copy these properties from the Space you are loading into your own `Blocks` object. \n\n## Demos with the `Pipeline` in `transformers`\n\nHugging Face's popular `transformers` library has a very easy-to-use abstraction, [`pipeline()`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/pipelines#transformers.pipeline) that handles most of the complex code to offer a simple API for common tasks. By specifying the task and an (optional) model, you can build a demo around an existing model with few lines of Python:\n\n```python\nimport gradio as gr\n\nfrom transformers import pipeline\n\npipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n\ndef predict(text):\n  return pipe(text)[0][\"translation_text\"]\n\ndemo = gr.Interface(\n  fn=predict,\n  inputs='text',\n  outputs='text',\n)\n\ndemo.launch()\n```\n\nBut `gradio` actually makes it even easier to convert a `pipeline` to a demo, simply by using the `gradio.Interface.from_pipeline` methods, which skips the need to specify the input and output components:\n\n```python\nfrom transformers import pipeline\nimport gradio as gr\n\npipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n\ndemo = gr.Interface.from_pipeline(pipe)\ndemo.launch()\n```\n\nThe previous code produces the following interface, which you can try right here in your browser:\n\n\u003Cgradio-app space=\"gradio/en2es\">\u003C/gradio-app>\n\n\n## Recap\n\nThat's it! Let's recap the various ways Gradio and Hugging Face work together:\n\n1. You can build a demo around Inference Endpoints without having to load the model, by using `gr.load()`.\n2. You host your Gradio demo on Hugging Face Spaces, either using the GUI or entirely in Python.\n3. You can load demos from Hugging Face Spaces to remix and create new Gradio demos using `gr.load()`.\n4. You can convert a `transformers` pipeline into a Gradio demo using `from_pipeline()`.\n\nü§ó\n",tags:["HUB","SPACES","EMBED"],spaces:["https://huggingface.co/spaces/gradio/en2es"],url:"/guides/using-hugging-face-integrations/",contributor:"\u003Ca href=\"https://huggingface.co/osanseviero\">Omar Sanseviero\u003C/a> ü¶ô"},{name:"image-classification-in-pytorch",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:55,pretty_name:"Image Classification In Pytorch",content:"# Image Classification in PyTorch\n\n\n\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from autonomous vehicles to medical imaging.\n\nSuch models are perfect to use with Gradio's _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like the demo on the bottom of the page.\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). We will be using a pretrained image classification model, so you should also have `torch` installed.\n\n## Step 1 ‚Äî Setting up the Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a pretrained Resnet-18 model, as it is easily downloadable from [PyTorch Hub](https://pytorch.org/hub/pytorch_vision_resnet/). You can use a different pretrained model or train your own.\n\n```python\nimport torch\n\nmodel = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()\n```\n\nBecause we will be using the model for inference, we have called the `.eval()` method.\n\n## Step 2 ‚Äî Defining a `predict` function\n\nNext, we will need to define a function that takes in the _user input_, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this [text file](https://git.io/JJkYN).\n\nIn the case of our pretrained model, it will look like this:\n\n```python\nimport requests\nfrom PIL import Image\nfrom torchvision import transforms\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef predict(inp):\n  inp = transforms.ToTensor()(inp).unsqueeze(0)\n  with torch.no_grad():\n    prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n    confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n  return confidences\n```\n\nLet's break this down. The function takes one parameter:\n\n- `inp`: the input image as a `PIL` image\n\nThen, the function converts the image to a PIL Image and then eventually a PyTorch `tensor`, passes it through the model, and returns:\n\n- `confidences`: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities\n\n## Step 3 ‚Äî Creating a Gradio Interface\n\nNow that we have our predictive function set up, we can create a Gradio Interface around it.\n\nIn this case, the input component is a drag-and-drop image component. To create this input, we use `Image(type=\"pil\")` which creates the component and handles the preprocessing to convert that to a `PIL` image.\n\nThe output component will be a `Label`, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images by constructing it as `Label(num_top_classes=3)`.\n\nFinally, we'll add one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:\n\n```python\nimport gradio as gr\n\ngr.Interface(fn=predict,\n             inputs=gr.Image(type=\"pil\"),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"lion.jpg\", \"cheetah.jpg\"]).launch()\n```\n\nThis produces the following interface, which you can try right here in your browser (try uploading your own examples!):\n\n\u003Cgradio-app space=\"gradio/pytorch-image-classifier\">\n\n\n---\n\nAnd you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n",tags:["VISION","RESNET","PYTORCH"],spaces:["https://huggingface.co/spaces/abidlabs/pytorch-image-classifier","https://huggingface.co/spaces/pytorch/ResNet","https://huggingface.co/spaces/pytorch/ResNext","https://huggingface.co/spaces/pytorch/SqueezeNet"],url:"/guides/image-classification-in-pytorch/",contributor:null},{name:"image-classification-in-tensorflow",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:56,pretty_name:"Image Classification In Tensorflow",content:"# Image Classification in TensorFlow and Keras\n\n\n\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from traffic control systems to satellite imaging.\n\nSuch models are perfect to use with Gradio's _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like the demo on the bottom of the page.\n\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). We will be using a pretrained Keras image classification model, so you should also have `tensorflow` installed.\n\n## Step 1 ‚Äî Setting up the Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a pretrained Mobile Net model, as it is easily downloadable from [Keras](https://keras.io/api/applications/mobilenet/). You can use a different pretrained model or train your own.\n\n```python\nimport tensorflow as tf\n\ninception_net = tf.keras.applications.MobileNetV2()\n```\n\nThis line automatically downloads the MobileNet model and weights using the Keras library.\n\n## Step 2 ‚Äî Defining a `predict` function\n\nNext, we will need to define a function that takes in the _user input_, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this [text file](https://git.io/JJkYN).\n\nIn the case of our pretrained model, it will look like this:\n\n```python\nimport requests\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef classify_image(inp):\n  inp = inp.reshape((-1, 224, 224, 3))\n  inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n  prediction = inception_net.predict(inp).flatten()\n  confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n  return confidences\n```\n\nLet's break this down. The function takes one parameter:\n\n- `inp`: the input image as a `numpy` array\n\nThen, the function adds a batch dimension, passes it through the model, and returns:\n\n- `confidences`: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities\n\n## Step 3 ‚Äî Creating a Gradio Interface\n\nNow that we have our predictive function set up, we can create a Gradio Interface around it.\n\nIn this case, the input component is a drag-and-drop image component. To create this input, we can use the `\"gradio.inputs.Image\"` class, which creates the component and handles the preprocessing to convert that to a numpy array. We will instantiate the class with a parameter that automatically preprocesses the input image to be 224 pixels by 224 pixels, which is the size that MobileNet expects.\n\nThe output component will be a `\"label\"`, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images.\n\nFinally, we'll add one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:\n\n```python\nimport gradio as gr\n\ngr.Interface(fn=classify_image,\n             inputs=gr.Image(shape=(224, 224)),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"banana.jpg\", \"car.jpg\"]).launch()\n```\n\nThis produces the following interface, which you can try right here in your browser (try uploading your own examples!):\n\n\u003Cgradio-app space=\"gradio/keras-image-classifier\">\n\n---\n\nAnd you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n",tags:["VISION","MOBILENET","TENSORFLOW"],spaces:["https://huggingface.co/spaces/abidlabs/keras-image-classifier"],url:"/guides/image-classification-in-tensorflow/",contributor:null},{name:"image-classification-with-vision-transformers",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:57,pretty_name:"Image Classification With Vision Transformers",content:"# Image Classification with Vision Transformers\n\n\n\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from facial recognition to manufacturing quality control.\n\nState-of-the-art image classifiers are based on the _transformers_ architectures, originally popularized for NLP tasks. Such architectures are typically called vision transformers (ViT). Such models are perfect to use with Gradio's _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in a **single line of Python**, and it will look like the demo on the bottom of the page.\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started).\n\n## Step 1 ‚Äî Choosing a Vision Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a model from the [Hugging Face Model Hub](https://huggingface.co/models?pipeline_tag=image-classification). The Hub contains thousands of models covering dozens of different machine learning tasks.\n\nExpand the Tasks category on the left sidebar and select \"Image Classification\" as our task of interest. You will then see all of the models on the Hub that are designed to classify images.\n\nAt the time of writing, the most popular one is `google/vit-base-patch16-224`, which has been trained on ImageNet images at a resolution of 224x224 pixels. We will use this model for our demo.\n\n## Step 2 ‚Äî Loading the Vision Transformer Model with Gradio\n\nWhen using a model from the Hugging Face Hub, we do not need to define the input or output components for the demo. Similarly, we do not need to be concerned with the details of preprocessing or postprocessing.\nAll of these are automatically inferred from the model tags.\n\nBesides the import statement, it only takes a single line of Python to load and launch the demo.\n\nWe use the `gr.Interface.load()` method and pass in the path to the model including the `huggingface/` to designate that it is from the Hugging Face Hub.\n\n```python\nimport gradio as gr\n\ngr.Interface.load(\n             \"huggingface/google/vit-base-patch16-224\",\n             examples=[\"alligator.jpg\", \"laptop.jpg\"]).launch()\n```\n\nNotice that we have added one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples.\n\nThis produces the following interface, which you can try right here in your browser. When you input an image, it is automatically preprocessed and sent to the Hugging Face Hub API, where it is passed through the model and returned as a human-interpretable prediction. Try uploading your own image!\n\n\u003Cgradio-app space=\"gradio/vision-transformer\">\n\n---\n\nAnd you're done! In one line of code, you have built a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n",tags:["VISION","TRANSFORMERS","HUB"],spaces:["https://huggingface.co/spaces/abidlabs/vision-transformer"],url:"/guides/image-classification-with-vision-transformers/",contributor:null},{name:"create-your-own-friends-with-a-gan",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:51,pretty_name:"Create Your Own Friends With A Gan",content:"# Create Your Own Friends with a GAN\n\n\n\n\n\n\n## Introduction\n\nIt seems that cryptocurrencies, [NFTs](https://www.nytimes.com/interactive/2022/03/18/technology/nft-guide.html), and the web3 movement are all the rage these days! Digital assets are being listed on marketplaces for astounding amounts of money, and just about every celebrity is debuting their own NFT collection. While your crypto assets [may be taxable, such as in Canada](https://www.canada.ca/en/revenue-agency/programs/about-canada-revenue-agency-cra/compliance/digital-currency/cryptocurrency-guide.html), today we'll explore some fun and tax-free ways to generate your own assortment of procedurally generated [CryptoPunks](https://www.larvalabs.com/cryptopunks).\n\nGenerative Adversarial Networks, often known just as _GANs_, are a specific class of deep-learning models that are designed to learn from an input dataset to create (_generate!_) new material that is convincingly similar to elements of the original training set. Famously, the website [thispersondoesnotexist.com](https://thispersondoesnotexist.com/) went viral with lifelike, yet synthetic, images of people generated with a model called StyleGAN2. GANs have gained traction in the machine learning world, and are now being used to generate all sorts of images, text, and even [music](https://salu133445.github.io/musegan/)!\n\nToday we'll briefly look at the high-level intuition behind GANs, and then we'll build a small demo around a pre-trained GAN to see what all the fuss is about. Here's a [peek](https://nimaboscarino-cryptopunks.hf.space) at what we're going to be putting together.\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). To use the pretrained model, also install `torch` and `torchvision`.\n\n## GANs: a very brief introduction\n\nOriginally proposed in [Goodfellow et al. 2014](https://arxiv.org/abs/1406.2661), GANs are made up of neural networks which compete with the intention of outsmarting each other. One network, known as the _generator_, is responsible for generating images. The other network, the _discriminator_, receives an image at a time from the generator along with a **real** image from the training data set. The discriminator then has to guess: which image is the fake?\n\nThe generator is constantly training to create images which are trickier for the discriminator to identify, while the discriminator raises the bar for the generator every time it correctly detects a fake. As the networks engage in this competitive (_adversarial!_) relationship, the images that get generated improve to the point where they become indistinguishable to human eyes!\n\nFor a more in-depth look at GANs, you can take a look at [this excellent post on Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/06/a-detailed-explanation-of-gan-with-implementation-using-tensorflow-and-keras/) or this [PyTorch tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html). For now, though, we'll dive into a demo!\n\n## Step 1 ‚Äî Create the Generator model\n\nTo generate new images with a GAN, you only need the generator model. There are many different architectures that the generator could use, but for this demo we'll use a pretrained GAN generator model with the following architecture:\n\n```python\nfrom torch import nn\n\nclass Generator(nn.Module):\n    # Refer to the link below for explanations about nc, nz, and ngf\n    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs\n    def __init__(self, nc=4, nz=100, ngf=64):\n        super(Generator, self).__init__()\n        self.network = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, input):\n        output = self.network(input)\n        return output\n```\n\nWe're taking the generator from [this repo by @teddykoker](https://github.com/teddykoker/cryptopunks-gan/blob/main/train.py#L90), where you can also see the original discriminator model structure.\n\nAfter instantiating the model, we'll load in the weights from the Hugging Face Hub, stored at [nateraw/cryptopunks-gan](https://huggingface.co/nateraw/cryptopunks-gan):\n\n```python\nfrom huggingface_hub import hf_hub_download\nimport torch\n\nmodel = Generator()\nweights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')\nmodel.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available\n```\n\n## Step 2 ‚Äî Defining a `predict` function\n\nThe `predict` function is the key to making Gradio work! Whatever inputs we choose through the Gradio interface will get passed through our `predict` function, which should operate on the inputs and generate outputs that we can display with Gradio output components. For GANs it's common to pass random noise into our model as the input, so we'll generate a tensor of random numbers and pass that through the model. We can then use `torchvision`'s `save_image` function to save the output of the model as a `png` file, and return the file name:\n\n```python\nfrom torchvision.utils import save_image\n\ndef predict(seed):\n    num_punks = 4\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n```\n\nWe're giving our `predict` function a `seed` parameter, so that we can fix the random tensor generation with a seed. We'll then be able to reproduce punks if we want to see them again by passing in the same seed.\n\n_Note!_ Our model needs an input tensor of dimensions 100x1x1 to do a single inference, or (BatchSize)x100x1x1 for generating a batch of images. In this demo we'll start by generating 4 punks at a time.\n\n## Step 3 ‚Äî Creating a Gradio interface\n\nAt this point you can even run the code you have with `predict(\u003CSOME_NUMBER>)`, and you'll find your freshly generated punks in your file system at `./punks.png`. To make a truly interactive demo, though, we'll build out a simple interface with Gradio. Our goals here are to:\n\n- Set a slider input so users can choose the \"seed\" value\n- Use an image component for our output to showcase the generated punks\n- Use our `predict()` to take the seed and generate the images\n\nWith `gr.Interface()`, we can define all of that with a single function call:\n\n```python\nimport gradio as gr\n\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n    ],\n    outputs=\"image\",\n).launch()\n```\n\n\n## Step 4 ‚Äî Even more punks!\n\nGenerating 4 punks at a time is a good start, but maybe we'd like to control how many we want to make each time. Adding more inputs to our Gradio interface is as simple as adding another item to the `inputs` list that we pass to `gr.Interface`:\n\n```python\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n        gr.Slider(4, 64, label='Number of Punks', step=1, default=10), # Adding another slider!\n    ],\n    outputs=\"image\",\n).launch()\n```\n\nThe new input will be passed to our `predict()` function, so we have to make some changes to that function to accept a new parameter:\n\n```python\ndef predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n```\n\nWhen you relaunch your interface, you should see a second slider that'll let you control the number of punks!\n\n## Step 5 - Polishing it up\n\nYour Gradio app is pretty much good to go, but you can add a few extra things to really make it ready for the spotlight ‚ú®\n\nWe can add some examples that users can easily try out by adding this to the `gr.Interface`:\n\n```python\ngr.Interface(\n    # ...\n    # keep everything as it is, and then add\n    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],\n).launch(cache_examples=True) # cache_examples is optional\n```\n\nThe `examples` parameter takes a list of lists, where each item in the sublists is ordered in the same order that we've listed the `inputs`. So in our case, `[seed, num_punks]`. Give it a try!\n\nYou can also try adding a `title`, `description`, and `article` to the `gr.Interface`. Each of those parameters accepts a string, so try it out and see what happens üëÄ `article` will also accept HTML, as [explored in a previous guide](/guides/key-features/#descriptive-content)!\n\nWhen you're all done, you may end up with something like [this](https://nimaboscarino-cryptopunks.hf.space).\n\nFor reference, here is our full code:\n\n```python\nimport torch\nfrom torch import nn\nfrom huggingface_hub import hf_hub_download\nfrom torchvision.utils import save_image\nimport gradio as gr\n\nclass Generator(nn.Module):\n    # Refer to the link below for explanations about nc, nz, and ngf\n    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs\n    def __init__(self, nc=4, nz=100, ngf=64):\n        super(Generator, self).__init__()\n        self.network = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, input):\n        output = self.network(input)\n        return output\n\nmodel = Generator()\nweights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')\nmodel.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available\n\ndef predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n        gr.Slider(4, 64, label='Number of Punks', step=1, default=10),\n    ],\n    outputs=\"image\",\n    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],\n).launch(cache_examples=True)\n```\n\n---\n\nCongratulations! You've built out your very own GAN-powered CryptoPunks generator, with a fancy Gradio interface that makes it easy for anyone to use. Now you can [scour the Hub for more GANs](https://huggingface.co/models?other=gan) (or train your own) and continue making even more awesome demos ü§ó\n",tags:["GAN","IMAGE","HUB"],spaces:["https://huggingface.co/spaces/NimaBoscarino/cryptopunks","https://huggingface.co/spaces/nateraw/cryptopunks-generator"],url:"/guides/create-your-own-friends-with-a-gan/",contributor:"\u003Ca href=\"https://huggingface.co/NimaBoscarino\">Nima Boscarino\u003C/a> and \u003Ca href=\"https://huggingface.co/nateraw\">Nate Raw\u003C/a>"}],parent:"gradio"},progress:{class:null,name:"Progress",description:"The Progress class provides a custom progress tracker that is used in a function signature. To attach a Progress tracker to a function, simply add a parameter right after the input parameters that has a default value set to a `gradio.Progress()` instance. The Progress tracker can then be updated in the function by calling the Progress object or using the `tqdm` method on an Iterable. The Progress tracker is currently only available with `queue()`.",tags:{},parameters:[{name:"track_tqdm",annotation:"bool",doc:"If True, the Progress object will track any tqdm.tqdm iterations with the tqdm library in the function.",default:"False"}],returns:{annotation:null},example:"import gradio as gr\nimport time\ndef my_function(x, progress=gr.Progress()):\n    progress(0, desc=\"Starting...\")\n    time.sleep(1)\n    for i in progress.tqdm(range(100)):\n        time.sleep(0.1)\n    return x\ngr.Interface(my_function, gr.Textbox(), gr.Textbox()).queue().launch()",fns:[{fn:null,name:"__call__",description:"Updates progress tracker with progress and message text.",tags:{},parameters:[{name:"progress",annotation:"float | tuple[int, int | None] | None",doc:"If float, should be between 0 and 1 representing completion. If Tuple, first number represents steps completed, and second value represents total steps or None if unknown. If None, hides progress bar."},{name:"desc",annotation:"str | None",doc:"description to display.",default:"None"},{name:"total",annotation:"int | None",doc:"estimated total number of steps.",default:"None"},{name:"unit",annotation:"str",doc:"unit of iterations.",default:"\"steps\""}],returns:{},example:null,override_signature:null,parent:"gradio.Progress"},{fn:null,name:"tqdm",description:"Attaches progress tracker to iterable, like tqdm.",tags:{},parameters:[{name:"iterable",annotation:"Iterable | None",doc:"iterable to attach progress tracker to."},{name:"desc",annotation:"str | None",doc:"description to display.",default:"None"},{name:"total",annotation:"int | None",doc:"estimated total number of steps.",default:"None"},{name:"unit",annotation:"str",doc:"unit of iterations.",default:"\"steps\""}],returns:{},example:null,override_signature:null,parent:"gradio.Progress"}],parent:"gradio"},make_waveform:{class:null,name:"make_waveform",description:"Generates a waveform video from an audio file. Useful for creating an easy to share audio visualization. The output should be passed into a `gr.Video` component.",tags:{parameters:"audio: Audio file path or tuple of (sample_rate, audio_data)\u003Cbr>bg_color: Background color of waveform (ignored if bg_image is provided)\u003Cbr>bg_image: Background image of waveform\u003Cbr>fg_alpha: Opacity of foreground waveform\u003Cbr>bars_color: Color of waveform bars. Can be a single color or a tuple of (start_color, end_color) of gradient\u003Cbr>bar_count: Number of bars in waveform\u003Cbr>bar_width: Width of bars in waveform. 1 represents full width, 0.5 represents half width, etc.\u003Cbr>animate: If true, the audio waveform overlay will be animated, if false, it will be static.",returns:"A filepath to the output video in mp4 format."},parameters:[{name:"audio",annotation:"str | tuple[int, np.ndarray]",doc:"Audio file path or tuple of (sample_rate, audio_data)"},{name:"bg_color",annotation:"str",doc:"Background color of waveform (ignored if bg_image is provided)",default:"\"#f3f4f6\""},{name:"bg_image",annotation:"str | None",doc:"Background image of waveform",default:"None"},{name:"fg_alpha",annotation:"float",doc:"Opacity of foreground waveform",default:"0.75"},{name:"bars_color",annotation:"str | tuple[str, str]",doc:"Color of waveform bars. Can be a single color or a tuple of (start_color, end_color) of gradient",default:"('#fbbf24', '#ea580c')"},{name:"bar_count",annotation:"int",doc:"Number of bars in waveform",default:"50"},{name:"bar_width",annotation:"float",doc:"Width of bars in waveform. 1 represents full width, 0.5 represents half width, etc.",default:"0.6"},{name:"animate",annotation:"bool",doc:"If true, the audio waveform overlay will be animated, if false, it will be static.",default:"False"}],returns:{annotation:null,doc:"A filepath to the output video in mp4 format."},example:null,fns:[],parent:"gradio"},load:{class:null,name:"load",description:"Constructs a demo from a Hugging Face repo. Can accept model repos (if src is \"models\") or Space repos (if src is \"spaces\"). The input and output components are automatically loaded from the repo. Note that if a Space is loaded, certain high-level attributes of the Blocks (e.g. custom `css`, `js`, and `head` attributes) will not be loaded.",tags:{parameters:"name: the name of the model (e.g. \"gpt2\" or \"facebook/bart-base\") or space (e.g. \"flax-community/spanish-gpt2\"), can include the `src` as prefix (e.g. \"models/facebook/bart-base\")\u003Cbr>src: the source of the model: `models` or `spaces` (or leave empty if source is provided as a prefix in `name`)\u003Cbr>hf_token: optional access token for loading private Hugging Face Hub models or spaces. Find your token here: https://huggingface.co/settings/tokens.  Warning: only provide this if you are loading a trusted private Space as it can be read by the Space you are loading.\u003Cbr>alias: optional string used as the name of the loaded model instead of the default name (only applies if loading a Space running Gradio 2.x)",returns:"a Gradio Blocks object for the given model"},parameters:[{name:"name",annotation:"str",doc:"the name of the model (e.g. &quot;gpt2&quot; or &quot;facebook/bart-base&quot;) or space (e.g. &quot;flax-community/spanish-gpt2&quot;), can include the `src` as prefix (e.g. &quot;models/facebook/bart-base&quot;)"},{name:"src",annotation:"str | None",doc:"the source of the model: `models` or `spaces` (or leave empty if source is provided as a prefix in `name`)",default:"None"},{name:"hf_token",annotation:"str | None",doc:"optional access token for loading private Hugging Face Hub models or spaces. Find your token here: https://huggingface.co/settings/tokens.  Warning: only provide this if you are loading a trusted private Space as it can be read by the Space you are loading.",default:"None"},{name:"alias",annotation:"str | None",doc:"optional string used as the name of the loaded model instead of the default name (only applies if loading a Space running Gradio 2.x)",default:"None"}],returns:{annotation:null,doc:"a Gradio Blocks object for the given model"},example:"import gradio as gr\ndemo = gr.load(\"gradio/question-answering\", src=\"spaces\")\ndemo.launch()",fns:[],parent:"gradio"}},modals:{error:{class:null,name:"Error",description:"This class allows you to pass custom error messages to the user. You can do so by raising a gr.Error(\"custom message\") anywhere in the code, and when that line is executed the custom message will appear in a modal on the demo.",tags:{demos:"calculator, blocks_chained_events"},parameters:[{name:"message",annotation:"\u003Cclass 'str'>",doc:"The error message to be displayed to the user.",default:"\"Error raised.\""}],returns:{annotation:null},example:"import gradio as gr\ndef divide(numerator, denominator):\n    if denominator == 0:\n        raise gr.Error(\"Cannot divide by zero!\")\ngr.Interface(divide, [\"number\", \"number\"], \"number\").launch()",fns:[],demos:[["calculator","import gradio as gr\n#from foo import BAR\n#\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        if num2 == 0:\n            raise gr.Error(\"Cannot divide by zero!\")\n        return num1 / num2\n\ndemo = gr.Interface(\n    calculator,\n    [\n        \"number\", \n        gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]),\n        \"number\"\n    ],\n    \"number\",\n    examples=[\n        [45, \"add\", 3],\n        [3.14, \"divide\", 2],\n        [144, \"multiply\", 2.5],\n        [0, \"subtract\", 1.2],\n    ],\n    title=\"Toy Calculator\",\n    description=\"Here's a sample toy calculator. Allows you to calculate things like $2+2=4$\",\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"],["blocks_chained_events","import gradio as gr\nimport time\n\n\ndef failure():\n    raise gr.Error(\"This should fail!\")\n\ndef exception():\n    raise ValueError(\"Something went wrong\")\n\ndef success():\n    return True\n\ndef warning_fn():\n    gr.Warning(\"This is a warning!\")\n\ndef info_fn():\n    gr.Info(\"This is some info\")\n\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Used in E2E tests of success event trigger. The then event covered in chatbot E2E tests.\"\n                \" Also testing that the status modals show up.\")\n    with gr.Row():\n        result = gr.Textbox(label=\"Result\")\n        result_2 = gr.Textbox(label=\"Consecutive Event\")\n    with gr.Row():\n        success_btn = gr.Button(value=\"Trigger Success\")\n        success_btn_2 = gr.Button(value=\"Trigger Consecutive Success\")\n        failure_btn = gr.Button(value=\"Trigger Failure\")\n        failure_exception = gr.Button(value=\"Trigger Failure With ValueError\")\n    with gr.Row():\n        trigger_warning = gr.Button(value=\"Trigger Warning\")\n        trigger_info = gr.Button(value=\"Trigger Info\")\n\n        success_btn_2.click(success, None, None).success(lambda: \"First Event Trigered\", None, result).success(lambda: \"Consecutive Event Triggered\", None, result_2)\n        success_btn.click(success, None, None).success(lambda: \"Success event triggered\", inputs=None, outputs=result)\n        failure_btn.click(failure, None, None).success(lambda: \"Should not be triggered\", inputs=None, outputs=result)\n        failure_exception.click(exception, None, None)\n        trigger_warning.click(warning_fn, None, None)\n        trigger_info.click(info_fn, None, None)\n\ndemo.queue()\n\nif __name__ == \"__main__\":\n    demo.launch(show_error=True)"]],parent:"gradio"},warning:{class:null,name:"Warning",description:"This function allows you to pass custom warning messages to the user. You can do so simply by writing `gr.Warning('message here')` in your function, and when that line is executed the custom message will appear in a modal on the demo. The modal is yellow by default and has the heading: \"Warning.\" Queue must be enabled for this behavior; otherwise, the warning will be printed to the console using the `warnings` library.",tags:{demos:"blocks_chained_events",parameters:"message: The warning message to be displayed to the user."},parameters:[{name:"message",annotation:"str",doc:"The warning message to be displayed to the user.",default:"\"Warning issued.\""}],returns:{annotation:null},example:"import gradio as gr\ndef hello_world():\n    gr.Warning('This is a warning message.')\n    return \"hello world\"\nwith gr.Blocks() as demo:\n    md = gr.Markdown()\n    demo.load(hello_world, inputs=None, outputs=[md])\ndemo.queue().launch()",fns:[],demos:[["blocks_chained_events","import gradio as gr\nimport time\n\n\ndef failure():\n    raise gr.Error(\"This should fail!\")\n\ndef exception():\n    raise ValueError(\"Something went wrong\")\n\ndef success():\n    return True\n\ndef warning_fn():\n    gr.Warning(\"This is a warning!\")\n\ndef info_fn():\n    gr.Info(\"This is some info\")\n\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Used in E2E tests of success event trigger. The then event covered in chatbot E2E tests.\"\n                \" Also testing that the status modals show up.\")\n    with gr.Row():\n        result = gr.Textbox(label=\"Result\")\n        result_2 = gr.Textbox(label=\"Consecutive Event\")\n    with gr.Row():\n        success_btn = gr.Button(value=\"Trigger Success\")\n        success_btn_2 = gr.Button(value=\"Trigger Consecutive Success\")\n        failure_btn = gr.Button(value=\"Trigger Failure\")\n        failure_exception = gr.Button(value=\"Trigger Failure With ValueError\")\n    with gr.Row():\n        trigger_warning = gr.Button(value=\"Trigger Warning\")\n        trigger_info = gr.Button(value=\"Trigger Info\")\n\n        success_btn_2.click(success, None, None).success(lambda: \"First Event Trigered\", None, result).success(lambda: \"Consecutive Event Triggered\", None, result_2)\n        success_btn.click(success, None, None).success(lambda: \"Success event triggered\", inputs=None, outputs=result)\n        failure_btn.click(failure, None, None).success(lambda: \"Should not be triggered\", inputs=None, outputs=result)\n        failure_exception.click(exception, None, None)\n        trigger_warning.click(warning_fn, None, None)\n        trigger_info.click(info_fn, None, None)\n\ndemo.queue()\n\nif __name__ == \"__main__\":\n    demo.launch(show_error=True)"]],parent:"gradio"},info:{class:null,name:"Info",description:"This function allows you to pass custom info messages to the user. You can do so simply by writing `gr.Info('message here')` in your function, and when that line is executed the custom message will appear in a modal on the demo. The modal is gray by default and has the heading: \"Info.\" Queue must be enabled for this behavior; otherwise, the message will be printed to the console.",tags:{demos:"blocks_chained_events",parameters:"message: The info message to be displayed to the user."},parameters:[{name:"message",annotation:"str",doc:"The info message to be displayed to the user.",default:"\"Info issued.\""}],returns:{annotation:null},example:"import gradio as gr\ndef hello_world():\n    gr.Info('This is some info.')\n    return \"hello world\"\nwith gr.Blocks() as demo:\n    md = gr.Markdown()\n    demo.load(hello_world, inputs=None, outputs=[md])\ndemo.queue().launch()",fns:[],demos:[["blocks_chained_events","import gradio as gr\nimport time\n\n\ndef failure():\n    raise gr.Error(\"This should fail!\")\n\ndef exception():\n    raise ValueError(\"Something went wrong\")\n\ndef success():\n    return True\n\ndef warning_fn():\n    gr.Warning(\"This is a warning!\")\n\ndef info_fn():\n    gr.Info(\"This is some info\")\n\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Used in E2E tests of success event trigger. The then event covered in chatbot E2E tests.\"\n                \" Also testing that the status modals show up.\")\n    with gr.Row():\n        result = gr.Textbox(label=\"Result\")\n        result_2 = gr.Textbox(label=\"Consecutive Event\")\n    with gr.Row():\n        success_btn = gr.Button(value=\"Trigger Success\")\n        success_btn_2 = gr.Button(value=\"Trigger Consecutive Success\")\n        failure_btn = gr.Button(value=\"Trigger Failure\")\n        failure_exception = gr.Button(value=\"Trigger Failure With ValueError\")\n    with gr.Row():\n        trigger_warning = gr.Button(value=\"Trigger Warning\")\n        trigger_info = gr.Button(value=\"Trigger Info\")\n\n        success_btn_2.click(success, None, None).success(lambda: \"First Event Trigered\", None, result).success(lambda: \"Consecutive Event Triggered\", None, result_2)\n        success_btn.click(success, None, None).success(lambda: \"Success event triggered\", inputs=None, outputs=result)\n        failure_btn.click(failure, None, None).success(lambda: \"Should not be triggered\", inputs=None, outputs=result)\n        failure_exception.click(exception, None, None)\n        trigger_warning.click(warning_fn, None, None)\n        trigger_info.click(info_fn, None, None)\n\ndemo.queue()\n\nif __name__ == \"__main__\":\n    demo.launch(show_error=True)"]],parent:"gradio"}},routes:{request:{class:null,name:"Request",description:"A Gradio request object that can be used to access the request headers, cookies, query parameters and other information about the request from within the prediction function. The class is a thin wrapper around the fastapi.Request class. Attributes of this class include: `headers`, `client`, `query_params`, `session_hash`, and `path_params`. If auth is enabled, the `username` attribute can be used to get the logged in user.",tags:{demos:"request_ip_headers"},parameters:[{name:"request",annotation:"fastapi.Request | None",doc:"A fastapi.Request",default:"None"},{name:"username",annotation:"str | None",doc:"The username of the logged in user (if auth is enabled)",default:"None"},{name:"session_hash",annotation:"str | None",doc:"The session hash of the current session. It is unique for each page load.",default:"None"}],returns:{annotation:null},example:"import gradio as gr\ndef echo(text, request: gr.Request):\n    if request:\n        print(\"Request headers dictionary:\", request.headers)\n        print(\"IP address:\", request.client.host)\n        print(\"Query parameters:\", dict(request.query_params))\n        print(\"Session hash:\", request.session_hash)\n    return text\nio = gr.Interface(echo, \"textbox\", \"textbox\").launch()",fns:[],demos:[["request_ip_headers","import gradio as gr\n\n\ndef predict(text, request: gr.Request):\n    headers = request.headers\n    host = request.client.host\n    user_agent = request.headers[\"user-agent\"]\n    return {\n        \"ip\": host,\n        \"user_agent\": user_agent,\n        \"headers\": headers,\n    }\n\n\ngr.Interface(predict, \"text\", \"json\").queue().launch()\n"]],parent:"gradio"},mount_gradio_app:{class:null,name:"mount_gradio_app",description:"Mount a gradio.Blocks to an existing FastAPI application. \u003Cbr>",tags:{parameters:"app: The parent FastAPI application.\u003Cbr>blocks: The blocks object we want to mount to the parent app.\u003Cbr>path: The path at which the gradio application will be mounted.\u003Cbr>app_kwargs: Additional keyword arguments to pass to the underlying FastAPI app as a dictionary of parameter keys and argument values. For example, `{\"docs_url\": \"/docs\"}`\u003Cbr>auth: If provided, username and password (or list of username-password tuples) required to access the gradio app. Can also provide function that takes username and password and returns True if valid login.\u003Cbr>auth_message: If provided, HTML message provided on login page for this gradio app.\u003Cbr>auth_dependency: A function that takes a FastAPI request and returns a string user ID or None. If the function returns None for a specific request, that user is not authorized to access the gradio app (they will see a 401 Unauthorized response). To be used with external authentication systems like OAuth. Cannot be used with `auth`.\u003Cbr>root_path: The subpath corresponding to the public deployment of this FastAPI application. For example, if the application is served at \"https://example.com/myapp\", the `root_path` should be set to \"/myapp\". A full URL beginning with http:// or https:// can be provided, which will be used in its entirety. Normally, this does not need to provided (even if you are using a custom `path`). However, if you are serving the FastAPI app behind a proxy, the proxy may not provide the full path to the Gradio app in the request headers. In which case, you can provide the root path here.\u003Cbr>allowed_paths: List of complete filepaths or parent directories that this gradio app is allowed to serve. Must be absolute paths. Warning: if you provide directories, any files in these directories or their subdirectories are accessible to all users of your app.\u003Cbr>blocked_paths: List of complete filepaths or parent directories that this gradio app is not allowed to serve (i.e. users of your app are not allowed to access). Must be absolute paths. Warning: takes precedence over `allowed_paths` and all other directories exposed by Gradio by default.\u003Cbr>favicon_path: If a path to a file (.png, .gif, or .ico) is provided, it will be used as the favicon for this gradio app's page.\u003Cbr>show_error: If True, any errors in the gradio app will be displayed in an alert modal and printed in the browser console log. Otherwise, errors will only be visible in the terminal session running the Gradio app.\u003Cbr>max_file_size: The maximum file size in bytes that can be uploaded. Can be a string of the form \"\u003Cvalue>\u003Cunit>\", where value is any positive integer and unit is one of \"b\", \"kb\", \"mb\", \"gb\", \"tb\". If None, no limit is set."},parameters:[{name:"app",annotation:"fastapi.FastAPI",doc:"The parent FastAPI application."},{name:"blocks",annotation:"gradio.Blocks",doc:"The blocks object we want to mount to the parent app."},{name:"path",annotation:"str",doc:"The path at which the gradio application will be mounted."},{name:"app_kwargs",annotation:"dict[str, Any] | None",doc:"Additional keyword arguments to pass to the underlying FastAPI app as a dictionary of parameter keys and argument values. For example, `{&quot;docs_url&quot;: &quot;/docs&quot;}`",default:"None"},{name:"auth",annotation:"Callable | tuple[str, str] | list[tuple[str, str]] | None",doc:"If provided, username and password (or list of username-password tuples) required to access the gradio app. Can also provide function that takes username and password and returns True if valid login.",default:"None"},{name:"auth_message",annotation:"str | None",doc:"If provided, HTML message provided on login page for this gradio app.",default:"None"},{name:"auth_dependency",annotation:"Callable[[fastapi.Request], str | None] | None",doc:"A function that takes a FastAPI request and returns a string user ID or None. If the function returns None for a specific request, that user is not authorized to access the gradio app (they will see a 401 Unauthorized response). To be used with external authentication systems like OAuth. Cannot be used with `auth`.",default:"None"},{name:"root_path",annotation:"str | None",doc:"The subpath corresponding to the public deployment of this FastAPI application. For example, if the application is served at &quot;https://example.com/myapp&quot;, the `root_path` should be set to &quot;/myapp&quot;. A full URL beginning with http:// or https:// can be provided, which will be used in its entirety. Normally, this does not need to provided (even if you are using a custom `path`). However, if you are serving the FastAPI app behind a proxy, the proxy may not provide the full path to the Gradio app in the request headers. In which case, you can provide the root path here.",default:"None"},{name:"allowed_paths",annotation:"list[str] | None",doc:"List of complete filepaths or parent directories that this gradio app is allowed to serve. Must be absolute paths. Warning: if you provide directories, any files in these directories or their subdirectories are accessible to all users of your app.",default:"None"},{name:"blocked_paths",annotation:"list[str] | None",doc:"List of complete filepaths or parent directories that this gradio app is not allowed to serve (i.e. users of your app are not allowed to access). Must be absolute paths. Warning: takes precedence over `allowed_paths` and all other directories exposed by Gradio by default.",default:"None"},{name:"favicon_path",annotation:"str | None",doc:"If a path to a file (.png, .gif, or .ico) is provided, it will be used as the favicon for this gradio app&#x27;s page.",default:"None"},{name:"show_error",annotation:"bool",doc:"If True, any errors in the gradio app will be displayed in an alert modal and printed in the browser console log. Otherwise, errors will only be visible in the terminal session running the Gradio app.",default:"True"},{name:"max_file_size",annotation:"str | int | None",doc:"The maximum file size in bytes that can be uploaded. Can be a string of the form &quot;&lt;value&gt;&lt;unit&gt;&quot;, where value is any positive integer and unit is one of &quot;b&quot;, &quot;kb&quot;, &quot;mb&quot;, &quot;gb&quot;, &quot;tb&quot;. If None, no limit is set.",default:"None"}],returns:{annotation:null},example:"from fastapi import FastAPI\nimport gradio as gr\napp = FastAPI()\n@app.get(\"/\")\ndef read_main():\n    return {\"message\": \"This is your main app\"}\nio = gr.Interface(lambda x: \"Hello, \" + x + \"!\", \"textbox\", \"textbox\")\napp = gr.mount_gradio_app(app, io, path=\"/gradio\")\n# Then run `uvicorn run:app` from the terminal and navigate to http://localhost:8000/gradio.",fns:[],parent:"gradio"}},events:["play","click","like","stream","apply","select","start_recording","release","blur","input","focus","submit","delete","stop_recording","upload","pause","end","edit","key_up","clear","pause_recording","stop","change"],chatinterface:{chatinterface:{class:null,name:"ChatInterface",description:"ChatInterface is Gradio's high-level abstraction for creating chatbot UIs, and allows you to create a web-based demo around a chatbot model in a few lines of code. Only one parameter is required: fn, which takes a function that governs the response of the chatbot based on the user input and chat history. Additional parameters can be used to control the appearance and behavior of the demo. \u003Cbr>",tags:{demos:"chatinterface_multimodal, chatinterface_random_response, chatinterface_streaming_echo",guides:"creating-a-chatbot-fast, sharing-your-app"},parameters:[{name:"fn",annotation:"Callable",doc:"The function to wrap the chat interface around. Should accept two parameters: a string input message and list of two-element lists of the form [[user_message, bot_message], ...] representing the chat history, and return a string response. See the Chatbot documentation for more information on the chat history format."},{name:"multimodal",annotation:"bool",doc:"If True, the chat interface will use a gr.MultimodalTextbox component for the input, which allows for the uploading of multimedia files. If False, the chat interface will use a gr.Textbox component for the input.",default:"False"},{name:"chatbot",annotation:"Chatbot | None",doc:"An instance of the gr.Chatbot component to use for the chat interface, if you would like to customize the chatbot properties. If not provided, a default gr.Chatbot component will be created.",default:"None"},{name:"textbox",annotation:"Textbox | MultimodalTextbox | None",doc:"An instance of the gr.Textbox or gr.MultimodalTextbox component to use for the chat interface, if you would like to customize the textbox properties. If not provided, a default gr.Textbox or gr.MultimodalTextbox component will be created.",default:"None"},{name:"additional_inputs",annotation:"str | Component | list[str | Component] | None",doc:"An instance or list of instances of gradio components (or their string shortcuts) to use as additional inputs to the chatbot. If components are not already rendered in a surrounding Blocks, then the components will be displayed under the chatbot, in an accordion.",default:"None"},{name:"additional_inputs_accordion_name",annotation:"str | None",doc:"Deprecated. Will be removed in a future version of Gradio. Use the `additional_inputs_accordion` parameter instead.",default:"None"},{name:"additional_inputs_accordion",annotation:"str | Accordion | None",doc:"If a string is provided, this is the label of the `gr.Accordion` to use to contain additional inputs. A `gr.Accordion` object can be provided as well to configure other properties of the container holding the additional inputs. Defaults to a `gr.Accordion(label=&quot;Additional Inputs&quot;, open=False)`. This parameter is only used if `additional_inputs` is provided.",default:"None"},{name:"examples",annotation:"list[str] | list[dict[str, str | list]] | list[list] | None",doc:"Sample inputs for the function; if provided, appear below the chatbot and can be clicked to populate the chatbot input. Should be a list of strings if `multimodal` is False, and a list of dictionaries (with keys `text` and `files`) if `multimodal` is True.",default:"None"},{name:"cache_examples",annotation:"bool | Literal['lazy'] | None",doc:"If True, caches examples in the server for fast runtime in examples. The default option in HuggingFace Spaces is True. The default option elsewhere is False.",default:"None"},{name:"examples_per_page",annotation:"int",doc:"If examples are provided, how many to display per page.",default:"10"},{name:"title",annotation:"str | None",doc:"a title for the interface; if provided, appears above chatbot in large font. Also used as the tab title when opened in a browser window.",default:"None"},{name:"description",annotation:"str | None",doc:"a description for the interface; if provided, appears above the chatbot and beneath the title in regular font. Accepts Markdown and HTML content.",default:"None"},{name:"theme",annotation:"Theme | str | None",doc:"Theme to use, loaded from gradio.themes.",default:"None"},{name:"css",annotation:"str | None",doc:"Custom css as a string or path to a css file. This css will be included in the demo webpage.",default:"None"},{name:"js",annotation:"str | None",doc:"Custom js as a string or path to a js file. The custom js should be in the form of a single js function. This function will automatically be executed when the page loads. For more flexibility, use the head parameter to insert js inside &lt;script&gt; tags.",default:"None"},{name:"head",annotation:"str | None",doc:"Custom html to insert into the head of the demo webpage. This can be used to add custom meta tags, multiple scripts, stylesheets, etc. to the page.",default:"None"},{name:"analytics_enabled",annotation:"bool | None",doc:"Whether to allow basic telemetry. If None, will use GRADIO_ANALYTICS_ENABLED environment variable if defined, or default to True.",default:"None"},{name:"submit_btn",annotation:"str | None | Button",doc:"Text to display on the submit button. If None, no button will be displayed. If a Button object, that button will be used.",default:"\"Submit\""},{name:"stop_btn",annotation:"str | None | Button",doc:"Text to display on the stop button, which replaces the submit_btn when the submit_btn or retry_btn is clicked and response is streaming. Clicking on the stop_btn will halt the chatbot response. If set to None, stop button functionality does not appear in the chatbot. If a Button object, that button will be used as the stop button.",default:"\"Stop\""},{name:"retry_btn",annotation:"str | None | Button",doc:"Text to display on the retry button. If None, no button will be displayed. If a Button object, that button will be used.",default:"\"üîÑ  Retry\""},{name:"undo_btn",annotation:"str | None | Button",doc:"Text to display on the delete last button. If None, no button will be displayed. If a Button object, that button will be used.",default:"\"‚Ü©Ô∏è Undo\""},{name:"clear_btn",annotation:"str | None | Button",doc:"Text to display on the clear button. If None, no button will be displayed. If a Button object, that button will be used.",default:"\"üóëÔ∏è  Clear\""},{name:"autofocus",annotation:"bool",doc:"If True, autofocuses to the textbox when the page loads.",default:"True"},{name:"concurrency_limit",annotation:"int | None | Literal['default']",doc:"If set, this is the maximum number of chatbot submissions that can be running simultaneously. Can be set to None to mean no limit (any number of chatbot submissions can be running simultaneously). Set to &quot;default&quot; to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `.queue()`, which is 1 by default).",default:"\"default\""},{name:"fill_height",annotation:"bool",doc:"If True, the chat interface will expand to the height of window.",default:"True"},{name:"delete_cache",annotation:"tuple[int, int] | None",doc:"A tuple corresponding [frequency, age] both expressed in number of seconds. Every `frequency` seconds, the temporary files created by this Blocks instance will be deleted if more than `age` seconds have passed since the file was created. For example, setting this to (86400, 86400) will delete temporary files every day. The cache will be deleted entirely when the server restarts. If None, no cache deletion will occur.",default:"None"}],returns:{annotation:null},example:"import gradio as gr\n\ndef echo(message, history):\n    return message\n\ndemo = gr.ChatInterface(fn=echo, examples=[\"hello\", \"hola\", \"merhaba\"], title=\"Echo Bot\")\ndemo.launch()",fns:[],demos:[["chatinterface_multimodal","import gradio as gr\n\n\ndef echo(message, history):\n    return message[\"text\"]\n\n\ndemo = gr.ChatInterface(\n    fn=echo,\n    examples=[{\"text\": \"hello\"}, {\"text\": \"hola\"}, {\"text\": \"merhaba\"}],\n    title=\"Echo Bot\",\n    multimodal=True,\n)\ndemo.launch()\n"],["chatinterface_random_response","import random\nimport gradio as gr\n\ndef random_response(message, history):\n    return random.choice([\"Yes\", \"No\"])\n\ndemo = gr.ChatInterface(random_response)\n\nif __name__ == \"__main__\":\n    demo.launch()\n"],["chatinterface_streaming_echo","import time\nimport gradio as gr\n\n\ndef slow_echo(message, history):\n    for i in range(len(message)):\n        time.sleep(0.05)\n        yield \"You typed: \" + message[: i + 1]\n\n\ndemo = gr.ChatInterface(slow_echo).queue()\n\nif __name__ == \"__main__\":\n    demo.launch()\n"]],guides:[{name:"creating-a-chatbot-fast",category:"chatbots",pretty_category:"Chatbots",guide_index:1,absolute_index:21,pretty_name:"Creating A Chatbot Fast",content:"# How to Create a Chatbot with Gradio\n\n\n\n## Introduction\n\nChatbots are a popular application of large language models. Using `gradio`, you can easily build a demo of your chatbot model and share that with your users, or try it yourself using an intuitive chatbot UI.\n\nThis tutorial uses `gr.ChatInterface()`, which is a high-level abstraction that allows you to create your chatbot UI fast, often with a single line of code. The chatbot interface that we create will look something like this:\n\n\u003Cgradio-app space='gradio/chatinterface_streaming_echo'>\u003C/gradio-app>\n\nWe'll start with a couple of simple examples, and then show how to use `gr.ChatInterface()` with real language models from several popular APIs and libraries, including `langchain`, `openai`, and Hugging Face.\n\n**Prerequisites**: please make sure you are using the **latest version** version of Gradio:\n\n```bash\n$ pip install --upgrade gradio\n```\n\n## Defining a chat function\n\nWhen working with `gr.ChatInterface()`, the first thing you should do is define your chat function. Your chat function should take two arguments: `message` and then `history` (the arguments can be named anything, but must be in this order).\n\n- `message`: a `str` representing the user's input.\n- `history`: a `list` of `list` representing the conversations up until that point. Each inner list consists of two `str` representing a pair: `[user input, bot response]`.\n\nYour function should return a single string response, which is the bot's response to the particular user input `message`. Your function can take into account the `history` of messages, as well as the current message.\n\nLet's take a look at a few examples.\n\n## Example: a chatbot that responds yes or no\n\nLet's write a chat function that responds `Yes` or `No` randomly.\n\nHere's our chat function:\n\n```python\nimport random\n\ndef random_response(message, history):\n    return random.choice([\"Yes\", \"No\"])\n```\n\nNow, we can plug this into `gr.ChatInterface()` and call the `.launch()` method to create the web interface:\n\n```python\nimport gradio as gr\n\ngr.ChatInterface(random_response).launch()\n```\n\nThat's it! Here's our running demo, try it out:\n\n\u003Cgradio-app space='gradio/chatinterface_random_response'>\u003C/gradio-app>\n\n## Another example using the user's input and history\n\nOf course, the previous example was very simplistic, it didn't even take user input or the previous history into account! Here's another simple example showing how to incorporate a user's input as well as the history.\n\n```python\nimport random\nimport gradio as gr\n\ndef alternatingly_agree(message, history):\n    if len(history) % 2 == 0:\n        return f\"Yes, I do think that '{message}'\"\n    else:\n        return \"I don't think so\"\n\ngr.ChatInterface(alternatingly_agree).launch()\n```\n\n## Streaming chatbots\n\nIn your chat function, you can use `yield` to generate a sequence of partial responses, each replacing the previous ones. This way, you'll end up with a streaming chatbot. It's that simple!\n\n```python\nimport time\nimport gradio as gr\n\ndef slow_echo(message, history):\n    for i in range(len(message)):\n        time.sleep(0.3)\n        yield \"You typed: \" + message[: i+1]\n\ngr.ChatInterface(slow_echo).launch()\n```\n\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> While the response is streaming, the \"Submit\" button turns into a \"Stop\" button that can be used to stop the generator function. You can customize the appearance and behavior of the \"Stop\" button using the `stop_btn` parameter.\u003C/p>\n\n## Customizing your chatbot\n\nIf you're familiar with Gradio's `Interface` class, the `gr.ChatInterface` includes many of the same arguments that you can use to customize the look and feel of your Chatbot. For example, you can:\n\n- add a title and description above your chatbot using `title` and `description` arguments.\n- add a theme or custom css using `theme` and `css` arguments respectively.\n- add `examples` and even enable `cache_examples`, which make it easier for users to try it out .\n- You can change the text or disable each of the buttons that appear in the chatbot interface: `submit_btn`, `retry_btn`, `undo_btn`, `clear_btn`.\n\nIf you want to customize the `gr.Chatbot` or `gr.Textbox` that compose the `ChatInterface`, then you can pass in your own chatbot or textbox as well. Here's an example of how we can use these parameters:\n\n```python\nimport gradio as gr\n\ndef yes_man(message, history):\n    if message.endswith(\"?\"):\n        return \"Yes\"\n    else:\n        return \"Ask me anything!\"\n\ngr.ChatInterface(\n    yes_man,\n    chatbot=gr.Chatbot(height=300),\n    textbox=gr.Textbox(placeholder=\"Ask me a yes or no question\", container=False, scale=7),\n    title=\"Yes Man\",\n    description=\"Ask Yes Man any question\",\n    theme=\"soft\",\n    examples=[\"Hello\", \"Am I cool?\", \"Are tomatoes vegetables?\"],\n    cache_examples=True,\n    retry_btn=None,\n    undo_btn=\"Delete Previous\",\n    clear_btn=\"Clear\",\n).launch()\n```\n\nIn particular, if you'd like to add a \"placeholder\" for your chat interface, which appears before the user has started chatting, you can do so using the `placeholder` argument of `gr.Chatbot`, which accepts Markdown or HTML. \n\n```python\ngr.ChatInterface(\n    yes_man,\n    chatbot=gr.Chatbot(placeholder=\"\u003Cstrong>Your Personal Yes-Man\u003C/strong>\u003Cbr>Ask Me Anything\"),\n...\n```\n\nThe placeholder appears vertically and horizontally centered in the chatbot.\n\n## Add Multimodal Capability to your chatbot\n\nYou may want to add multimodal capability to your chatbot. For example, you may want users to be able to easily upload images or files to your chatbot and ask questions about it. You can make your chatbot \"multimodal\" by passing in a single parameter (`multimodal=True`) to the `gr.ChatInterface` class.\n\n\n```python\nimport gradio as gr\nimport time\n\ndef count_files(message, history):\n    num_files = len(message[\"files\"])\n    return f\"You uploaded {num_files} files\"\n\ndemo = gr.ChatInterface(fn=count_files, examples=[{\"text\": \"Hello\", \"files\": []}], title=\"Echo Bot\", multimodal=True)\n\ndemo.launch()\n```\n\nWhen `multimodal=True`, the signature of `fn` changes slightly. The first parameter of your function should accept a dictionary consisting of the submitted text and uploaded files that looks like this: `{\"text\": \"user input\", \"file\": [\"file_path1\", \"file_path2\", ...]}`. Similarly, any examples you provide should be in a dictionary of this form. Your function should still return a single `str` message. \u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> If you'd like to customize the UI/UX of the textbox for your multimodal chatbot, you should pass in an instance of `gr.MultimodalTextbox` to the `textbox` argument of `ChatInterface` instead of an instance of `gr.Textbox`.\u003C/p>\n\n## Additional Inputs\n\nYou may want to add additional parameters to your chatbot and expose them to your users through the Chatbot UI. For example, suppose you want to add a textbox for a system prompt, or a slider that sets the number of tokens in the chatbot's response. The `ChatInterface` class supports an `additional_inputs` parameter which can be used to add additional input components.\n\nThe `additional_inputs` parameters accepts a component or a list of components. You can pass the component instances directly, or use their string shortcuts (e.g. `\"textbox\"` instead of `gr.Textbox()`). If you pass in component instances, and they have _not_ already been rendered, then the components will appear underneath the chatbot (and any examples) within a `gr.Accordion()`. You can set the label of this accordion using the `additional_inputs_accordion_name` parameter.\n\nHere's a complete example:\n\n```python\nimport gradio as gr\nimport time\n\n\ndef echo(message, history, system_prompt, tokens):\n    response = f\"System prompt: {system_prompt}\\n Message: {message}.\"\n    for i in range(min(len(response), int(tokens))):\n        time.sleep(0.05)\n        yield response[: i + 1]\n\n\ndemo = gr.ChatInterface(\n    echo,\n    additional_inputs=[\n        gr.Textbox(\"You are helpful AI.\", label=\"System Prompt\"),\n        gr.Slider(10, 100),\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.queue().launch()\n\n```\n\nIf the components you pass into the `additional_inputs` have already been rendered in a parent `gr.Blocks()`, then they will _not_ be re-rendered in the accordion. This provides flexibility in deciding where to lay out the input components. In the example below, we position the `gr.Textbox()` on top of the Chatbot UI, while keeping the slider underneath.\n\n```python\nimport gradio as gr\nimport time\n\ndef echo(message, history, system_prompt, tokens):\n    response = f\"System prompt: {system_prompt}\\n Message: {message}.\"\n    for i in range(min(len(response), int(tokens))):\n        time.sleep(0.05)\n        yield response[: i+1]\n\nwith gr.Blocks() as demo:\n    system_prompt = gr.Textbox(\"You are helpful AI.\", label=\"System Prompt\")\n    slider = gr.Slider(10, 100, render=False)\n\n    gr.ChatInterface(\n        echo, additional_inputs=[system_prompt, slider]\n    )\n\ndemo.launch()\n```\n\nIf you need to create something even more custom, then its best to construct the chatbot UI using the low-level `gr.Blocks()` API. We have [a dedicated guide for that here](/guides/creating-a-custom-chatbot-with-blocks).\n\n## Using your chatbot via an API\n\nOnce you've built your Gradio chatbot and are hosting it on [Hugging Face Spaces](https://hf.space) or somewhere else, then you can query it with a simple API at the `/chat` endpoint. The endpoint just expects the user's message (and potentially additional inputs if you have set any using the `additional_inputs` parameter), and will return the response, internally keeping track of the messages sent so far.\n\n[](https://github.com/gradio-app/gradio/assets/1778297/7b10d6db-6476-4e2e-bebd-ecda802c3b8f)\n\nTo use the endpoint, you should use either the [Gradio Python Client](/guides/getting-started-with-the-python-client) or the [Gradio JS client](/guides/getting-started-with-the-js-client).\n\n## A `langchain` example\n\nNow, let's actually use the `gr.ChatInterface` with some real large language models. We'll start by using `langchain` on top of `openai` to build a general-purpose streaming chatbot application in 19 lines of code. You'll need to have an OpenAI key for this example (keep reading for the free, open-source equivalent!)\n\n```python\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import AIMessage, HumanMessage\nimport openai\nimport gradio as gr\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # Replace with your key\n\nllm = ChatOpenAI(temperature=1.0, model='gpt-3.5-turbo-0613')\n\ndef predict(message, history):\n    history_langchain_format = []\n    for human, ai in history:\n        history_langchain_format.append(HumanMessage(content=human))\n        history_langchain_format.append(AIMessage(content=ai))\n    history_langchain_format.append(HumanMessage(content=message))\n    gpt_response = llm(history_langchain_format)\n    return gpt_response.content\n\ngr.ChatInterface(predict).launch()\n```\n\n## A streaming example using `openai`\n\nOf course, we could also use the `openai` library directy. Here a similar example, but this time with streaming results as well:\n\n```python\nfrom openai import OpenAI\nimport gradio as gr\n\napi_key = \"sk-...\"  # Replace with your key\nclient = OpenAI(api_key=api_key)\n\ndef predict(message, history):\n    history_openai_format = []\n    for human, assistant in history:\n        history_openai_format.append({\"role\": \"user\", \"content\": human })\n        history_openai_format.append({\"role\": \"assistant\", \"content\":assistant})\n    history_openai_format.append({\"role\": \"user\", \"content\": message})\n  \n    response = client.chat.completions.create(model='gpt-3.5-turbo',\n    messages= history_openai_format,\n    temperature=1.0,\n    stream=True)\n\n    partial_message = \"\"\n    for chunk in response:\n        if chunk.choices[0].delta.content is not None:\n              partial_message = partial_message + chunk.choices[0].delta.content\n              yield partial_message\n\ngr.ChatInterface(predict).launch()\n```\n\n## Example using a local, open-source LLM with Hugging Face\n\nOf course, in many cases you want to run a chatbot locally. Here's the equivalent example using Together's RedePajama model, from Hugging Face (this requires you to have a GPU with CUDA).\n\n```python\nimport gradio as gr\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\nfrom threading import Thread\n\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\", torch_dtype=torch.float16)\nmodel = model.to('cuda:0')\n\nclass StopOnTokens(StoppingCriteria):\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        stop_ids = [29, 0]\n        for stop_id in stop_ids:\n            if input_ids[0][-1] == stop_id:\n                return True\n        return False\n\ndef predict(message, history):\n    history_transformer_format = history + [[message, \"\"]]\n    stop = StopOnTokens()\n\n    messages = \"\".join([\"\".join([\"\\n\u003Chuman>:\"+item[0], \"\\n\u003Cbot>:\"+item[1]])\n                for item in history_transformer_format])\n\n    model_inputs = tokenizer([messages], return_tensors=\"pt\").to(\"cuda\")\n    streamer = TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)\n    generate_kwargs = dict(\n        model_inputs,\n        streamer=streamer,\n        max_new_tokens=1024,\n        do_sample=True,\n        top_p=0.95,\n        top_k=1000,\n        temperature=1.0,\n        num_beams=1,\n        stopping_criteria=StoppingCriteriaList([stop])\n        )\n    t = Thread(target=model.generate, kwargs=generate_kwargs)\n    t.start()\n\n    partial_message = \"\"\n    for new_token in streamer:\n        if new_token != '\u003C':\n            partial_message += new_token\n            yield partial_message\n\ngr.ChatInterface(predict).launch()\n```\n\nWith those examples, you should be all set to create your own Gradio Chatbot demos soon! For building even more custom Chatbot applications, check out [a dedicated guide](/guides/creating-a-custom-chatbot-with-blocks) using the low-level `gr.Blocks()` API.\n",tags:["NLP","TEXT","CHAT"],spaces:[],url:"/guides/creating-a-chatbot-fast/",contributor:null},{name:"sharing-your-app",category:"additional-features",pretty_category:"Additional Features",guide_index:7,absolute_index:14,pretty_name:"Sharing Your App",content:"# Sharing Your App\n\nIn this Guide, we dive more deeply into the various aspects of sharing a Gradio app with others. We will cover:\n\n1. [Sharing demos with the share parameter](#sharing-demos)\n2. [Hosting on HF Spaces](#hosting-on-hf-spaces)\n3. [Embedding hosted spaces](#embedding-hosted-spaces)\n4. [Using the API page](#api-page)\n5. [Accessing network requests](#accessing-the-network-request-directly)\n6. [Mounting within FastAPI](#mounting-within-another-fast-api-app)\n7. [Authentication](#authentication)\n8. [Security and file access](#security-and-file-access)\n9. [Analytics](#analytics)\n\n## Sharing Demos\n\nGradio demos can be easily shared publicly by setting `share=True` in the `launch()` method. Like this:\n\n```python\nimport gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(fn=greet, inputs=\"textbox\", outputs=\"textbox\")\n    \ndemo.launch(share=True)  # Share your demo with just 1 extra parameter üöÄ\n```\n\nThis generates a public, shareable link that you can send to anybody! When you send this link, the user on the other side can try out the model in their browser. Because the processing happens on your device (as long as your device stays on), you don't have to worry about any packaging any dependencies. \n\n![sharing](https://github.com/gradio-app/gradio/blob/main/guides/assets/sharing.svg?raw=true)\n\n\nA share link usually looks something like this: **https://07ff8706ab.gradio.live**. Although the link is served through the Gradio Share Servers, these servers are only a proxy for your local server, and do not store any data sent through your app. Share links expire after 72 hours. (it is [also possible to set up your own Share Server](https://github.com/huggingface/frp/) on your own cloud server to overcome this restriction.)\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> Keep in mind that share links are publicly accessible, meaning that anyone can use your model for prediction! Therefore, make sure not to expose any sensitive information through the functions you write, or allow any critical changes to occur on your device. Or you can [add authentication to your Gradio app](#authentication) as discussed below.\u003C/p>\n\nNote that by default, `share=False`, which means that your server is only running locally. (This is the default, except in Google Colab notebooks, where share links are automatically created). As an alternative to using share links, you can use use [SSH port-forwarding](https://www.ssh.com/ssh/tunneling/example) to share your local server with specific users.\n\n\n## Hosting on HF Spaces\n\nIf you'd like to have a permanent link to your Gradio demo on the internet, use Hugging Face Spaces. [Hugging Face Spaces](http://huggingface.co/spaces/) provides the infrastructure to permanently host your machine learning model for free!\n\nAfter you have [created a free Hugging Face account](https://huggingface.co/join), you have two methods to deploy your Gradio app to Hugging Face Spaces:\n\n1. From terminal: run `gradio deploy` in your app directory. The CLI will gather some basic metadata and then launch your app. To update your space, you can re-run this command or enable the Github Actions option to automatically update the Spaces on `git push`.\n\n2. From your browser: Drag and drop a folder containing your Gradio model and all related files [here](https://huggingface.co/new-space). See [this guide how to host on Hugging Face Spaces](https://huggingface.co/blog/gradio-spaces) for more information, or watch the embedded video:\n\n\u003Cvideo autoplay muted loop>\n  \u003Csource src=\"https://github.com/gradio-app/gradio/blob/main/guides/assets/hf_demo.mp4?raw=true\" type=\"video/mp4\" />\n\u003C/video>\n\n\n## Embedding Hosted Spaces\n\nOnce you have hosted your app on Hugging Face Spaces (or on your own server), you may want to embed the demo on a different website, such as your blog or your portfolio. Embedding an interactive demo allows people to try out the machine learning model that you have built, without needing to download or install anything ‚Äî right in their browser! The best part is that you can embed interactive demos even in static websites, such as GitHub pages.\n\nThere are two ways to embed your Gradio demos. You can find quick links to both options directly on the Hugging Face Space page, in the \"Embed this Space\" dropdown option:\n\n![Embed this Space dropdown option](https://github.com/gradio-app/gradio/blob/main/guides/assets/embed_this_space.png?raw=true)\n\n### Embedding with Web Components\n\nWeb components typically offer a better experience to users than IFrames. Web components load lazily, meaning that they won't slow down the loading time of your website, and they automatically adjust their height based on the size of the Gradio app.\n\nTo embed with Web Components:\n\n1. Import the gradio JS library into into your site by adding the script below in your site (replace {GRADIO_VERSION} in the URL with the library version of Gradio you are using).\n\n```html\n\u003Cscript\n\ttype=\"module\"\n\tsrc=\"https://gradio.s3-us-west-2.amazonaws.com/{GRADIO_VERSION}/gradio.js\"\n>\u003C/script>\n```\n\n2. Add\n\n```html\n\u003Cgradio-app src=\"https://$your_space_host.hf.space\">\u003C/gradio-app>\n```\n\nelement where you want to place the app. Set the `src=` attribute to your Space's embed URL, which you can find in the \"Embed this Space\" button. For example:\n\n```html\n\u003Cgradio-app\n\tsrc=\"https://abidlabs-pytorch-image-classifier.hf.space\"\n>\u003C/gradio-app>\n```\n\n\u003Cscript>\nfetch(\"https://pypi.org/pypi/gradio/json\"\n).then(r => r.json()\n).then(obj => {\n    let v = obj.info.version;\n    content = document.querySelector('.prose');\n    content.innerHTML = content.innerHTML.replaceAll(\"{GRADIO_VERSION}\", v);\n});\n\u003C/script>\n\nYou can see examples of how web components look \u003Ca href=\"https://www.gradio.app\">on the Gradio landing page\u003C/a>.\n\nYou can also customize the appearance and behavior of your web component with attributes that you pass into the `\u003Cgradio-app>` tag:\n\n- `src`: as we've seen, the `src` attributes links to the URL of the hosted Gradio demo that you would like to embed\n- `space`: an optional shorthand if your Gradio demo is hosted on Hugging Face Space. Accepts a `username/space_name` instead of a full URL. Example: `gradio/Echocardiogram-Segmentation`. If this attribute attribute is provided, then `src` does not need to be provided.\n- `control_page_title`: a boolean designating whether the html title of the page should be set to the title of the Gradio app (by default `\"false\"`)\n- `initial_height`: the initial height of the web component while it is loading the Gradio app, (by default `\"300px\"`). Note that the final height is set based on the size of the Gradio app.\n- `container`: whether to show the border frame and information about where the Space is hosted (by default `\"true\"`)\n- `info`: whether to show just the information about where the Space is hosted underneath the embedded app (by default `\"true\"`)\n- `autoscroll`: whether to autoscroll to the output when prediction has finished (by default `\"false\"`)\n- `eager`: whether to load the Gradio app as soon as the page loads (by default `\"false\"`)\n- `theme_mode`: whether to use the `dark`, `light`, or default `system` theme mode (by default `\"system\"`)\n- `render`: an event that is triggered once the embedded space has finished rendering.\n\nHere's an example of how to use these attributes to create a Gradio app that does not lazy load and has an initial height of 0px.\n\n```html\n\u003Cgradio-app\n\tspace=\"gradio/Echocardiogram-Segmentation\"\n\teager=\"true\"\n\tinitial_height=\"0px\"\n>\u003C/gradio-app>\n```\n\nHere's another example of how to use the `render` event. An event listener is used to capture the `render` event and will call the `handleLoadComplete()` function once rendering is complete. \n\n```html\n\u003Cscript>\n\tfunction handleLoadComplete() {\n\t\tconsole.log(\"Embedded space has finished rendering\");\n\t}\n\n\tconst gradioApp = document.querySelector(\"gradio-app\");\n\tgradioApp.addEventListener(\"render\", handleLoadComplete);\n\u003C/script>\n```\n\n_Note: While Gradio's CSS will never impact the embedding page, the embedding page can affect the style of the embedded Gradio app. Make sure that any CSS in the parent page isn't so general that it could also apply to the embedded Gradio app and cause the styling to break. Element selectors such as `header { ... }` and `footer { ... }` will be the most likely to cause issues._\n\n### Embedding with IFrames\n\nTo embed with IFrames instead (if you cannot add javascript to your website, for example), add this element:\n\n```html\n\u003Ciframe src=\"https://$your_space_host.hf.space\">\u003C/iframe>\n```\n\nAgain, you can find the `src=` attribute to your Space's embed URL, which you can find in the \"Embed this Space\" button.\n\nNote: if you use IFrames, you'll probably want to add a fixed `height` attribute and set `style=\"border:0;\"` to remove the boreder. In addition, if your app requires permissions such as access to the webcam or the microphone, you'll need to provide that as well using the `allow` attribute.\n\n## API Page\n\nYou can use almost any Gradio app as an API! In the footer of a Gradio app [like this one](https://huggingface.co/spaces/gradio/hello_world), you'll see a \"Use via API\" link.\n\n![Use via API](https://github.com/gradio-app/gradio/blob/main/guides/assets/use_via_api.png?raw=true)\n\nThis is a page that lists the endpoints that can be used to query the Gradio app, via our supported clients: either [the Python client](https://gradio.app/guides/getting-started-with-the-python-client/), or [the JavaScript client](https://gradio.app/guides/getting-started-with-the-js-client/). For each endpoint, Gradio automatically generates the parameters and their types, as well as example inputs, like this.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/view-api.png)\n\nThe endpoints are automatically created when you launch a Gradio `Interface`. If you are using Gradio `Blocks`, you can also set up a Gradio API page, though we recommend that you explicitly name each event listener, such as\n\n```python\nbtn.click(add, [num1, num2], output, api_name=\"addition\")\n```\n\nThis will add and document the endpoint `/api/addition/` to the automatically generated API page. Otherwise, your API endpoints will appear as \"unnamed\" endpoints.\n\n## Accessing the Network Request Directly\n\nWhen a user makes a prediction to your app, you may need the underlying network request, in order to get the request headers (e.g. for advanced authentication), log the client's IP address, getting the query parameters, or for other reasons. Gradio supports this in a similar manner to FastAPI: simply add a function parameter whose type hint is `gr.Request` and Gradio will pass in the network request as that parameter. Here is an example:\n\n```python\nimport gradio as gr\n\ndef echo(text, request: gr.Request):\n    if request:\n        print(\"Request headers dictionary:\", request.headers)\n        print(\"IP address:\", request.client.host)\n        print(\"Query parameters:\", dict(request.query_params))\n    return text\n\nio = gr.Interface(echo, \"textbox\", \"textbox\").launch()\n```\n\nNote: if your function is called directly instead of through the UI (this happens, for\nexample, when examples are cached, or when the Gradio app is called via API), then `request` will be `None`. \nYou should handle this case explicitly to ensure that your app does not throw any errors. That is why\nwe have the explicit check `if request`.\n\n## Mounting Within Another FastAPI App\n\nIn some cases, you might have an existing FastAPI app, and you'd like to add a path for a Gradio demo.\nYou can easily do this with `gradio.mount_gradio_app()`.\n\nHere's a complete example:\n\n```python\nfrom fastapi import FastAPI\nimport gradio as gr\n\nCUSTOM_PATH = \"/gradio\"\n\napp = FastAPI()\n\n\n@app.get(\"/\")\ndef read_main():\n    return {\"message\": \"This is your main app\"}\n\n\nio = gr.Interface(lambda x: \"Hello, \" + x + \"!\", \"textbox\", \"textbox\")\napp = gr.mount_gradio_app(app, io, path=CUSTOM_PATH)\n\n\n# Run this from the terminal as you would normally start a FastAPI app: `uvicorn run:app`\n# and navigate to http://localhost:8000/gradio in your browser.\n\n```\n\nNote that this approach also allows you run your Gradio apps on custom paths (`http://localhost:8000/gradio` in the example above).\n\n\n## Authentication\n\n### Password-protected app\n\nYou may wish to put an authentication page in front of your app to limit who can open your app. With the `auth=` keyword argument in the `launch()` method, you can provide a tuple with a username and password, or a list of acceptable username/password tuples; Here's an example that provides password-based authentication for a single user named \"admin\":\n\n```python\ndemo.launch(auth=(\"admin\", \"pass1234\"))\n```\n\nFor more complex authentication handling, you can even pass a function that takes a username and password as arguments, and returns `True` to allow access, `False` otherwise.\n\nHere's an example of a function that accepts any login where the username and password are the same:\n\n```python\ndef same_auth(username, password):\n    return username == password\ndemo.launch(auth=same_auth)\n```\n\nIf you have multiple users, you may wish to customize the content that is shown depending on the user that is logged in. You can retrieve the logged in user by [accessing the network request directly](#accessing-the-network-request-directly) as discussed above, and then reading the `.username` attribute of the request. Here's an example:\n\n\n```python\nimport gradio as gr\n\ndef update_message(request: gr.Request):\n    return f\"Welcome, {request.username}\"\n\nwith gr.Blocks() as demo:\n    m = gr.Markdown()\n    demo.load(update_message, None, m)\n    \ndemo.launch(auth=[(\"Abubakar\", \"Abubakar\"), (\"Ali\", \"Ali\")])\n```\n\nNote: For authentication to work properly, third party cookies must be enabled in your browser. This is not the case by default for Safari or for Chrome Incognito Mode. \n\nIf users visit the `/logout` page of your Gradio app, they will automatically be logged out and session cookies deleted. This allows you to add logout functionality to your Gradio app as well. Let's update the previous example to include a log out button:\n\n```python\nimport gradio as gr\n\ndef update_message(request: gr.Request):\n    return f\"Welcome, {request.username}\"\n\nwith gr.Blocks() as demo:\n    m = gr.Markdown()\n    logout_button = gr.Button(\"Logout\", link=\"/logout\")\n    demo.load(update_message, None, m)\n    \ndemo.launch(auth=[(\"Pete\", \"Pete\"), (\"Dawood\", \"Dawood\")])\n```\n\nNote: Gradio's built-in authentication provides a straightforward and basic layer of access control but does not offer robust security features for applications that require stringent access controls (e.g.  multi-factor authentication, rate limiting, or automatic lockout policies).\n\n### OAuth (Login via Hugging Face)\n\nGradio natively supports OAuth login via Hugging Face. In other words, you can easily add a _\"Sign in with Hugging Face\"_ button to your demo, which allows you to get a user's HF username as well as other information from their HF profile. Check out [this Space](https://huggingface.co/spaces/Wauplin/gradio-oauth-demo) for a live demo.\n\nTo enable OAuth, you must set `hf_oauth: true` as a Space metadata in your README.md file. This will register your Space\nas an OAuth application on Hugging Face. Next, you can use `gr.LoginButton` to add a login button to\nyour Gradio app. Once a user is logged in with their HF account, you can retrieve their profile by adding a parameter of type\n`gr.OAuthProfile` to any Gradio function. The user profile will be automatically injected as a parameter value. If you want\nto perform actions on behalf of the user (e.g. list user's private repos, create repo, etc.), you can retrieve the user\ntoken by adding a parameter of type `gr.OAuthToken`. You must define which scopes you will use in your Space metadata\n(see [documentation](https://huggingface.co/docs/hub/spaces-oauth#scopes) for more details).\n\nHere is a short example:\n\n```py\nimport gradio as gr\nfrom huggingface_hub import whoami\n\ndef hello(profile: gr.OAuthProfile | None) -> str:\n    if profile is None:\n        return \"I don't know you.\"\n    return f\"Hello {profile.name}\"\n\ndef list_organizations(oauth_token: gr.OAuthToken | None) -> str:\n    if oauth_token is None:\n        return \"Please log in to list organizations.\"\n    org_names = [org[\"name\"] for org in whoami(oauth_token.token)[\"orgs\"]]\n    return f\"You belong to {', '.join(org_names)}.\"\n\nwith gr.Blocks() as demo:\n    gr.LoginButton()\n    m1 = gr.Markdown()\n    m2 = gr.Markdown()\n    demo.load(hello, inputs=None, outputs=m1)\n    demo.load(list_organizations, inputs=None, outputs=m2)\n\ndemo.launch()\n```\n\nWhen the user clicks on the login button, they get redirected in a new page to authorize your Space.\n\n\u003Ccenter>\n\u003Cimg src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/oauth_sign_in.png\" style=\"width:300px; max-width:80%\">\n\u003C/center>\n\nUsers can revoke access to their profile at any time in their [settings](https://huggingface.co/settings/connected-applications).\n\nAs seen above, OAuth features are available only when your app runs in a Space. However, you often need to test your app\nlocally before deploying it. To test OAuth features locally, your machine must be logged in to Hugging Face. Please run `huggingface-cli login` or set `HF_TOKEN` as environment variable with one of your access token. You can generate a new token in your settings page (https://huggingface.co/settings/tokens). Then, clicking on the `gr.LoginButton` will login your local Hugging Face profile, allowing you to debug your app with your Hugging Face account before deploying it to a Space.\n\n\n### OAuth (with external providers)\n\nIt is also possible to authenticate with external OAuth providers (e.g. Google OAuth) in your Gradio apps. To do this, first mount your Gradio app within a FastAPI app ([as discussed above](#mounting-within-another-fast-api-app)). Then, you must write an *authentication function*, which gets the user's username from the OAuth provider and returns it. This function should be passed to the `auth_dependency` parameter in `gr.mount_gradio_app`. \n\nSimilar to [FastAPI dependency functions](https://fastapi.tiangolo.com/tutorial/dependencies/), the function specified by `auth_dependency` will run before any Gradio-related route in your FastAPI app. The function should accept a single parameter: the FastAPI `Request` and return either a string (representing a user's username) or `None`. If a string is returned, the user will be able to access the Gradio-related routes in your FastAPI app. \n\nFirst, let's show a simplistic example to illustrate the `auth_dependency` parameter:\n\n```python\nfrom fastapi import FastAPI, Request\nimport gradio as gr\n\napp = FastAPI()\n\ndef get_user(request: Request):\n    return request.headers.get(\"user\")\n\ndemo = gr.Interface(lambda s: f\"Hello {s}!\", \"textbox\", \"textbox\")\n\napp = gr.mount_gradio_app(app, demo, path=\"/demo\", auth_dependency=get_user)\n\nif __name__ == '__main__':\n    uvicorn.run(app)\n```\n\nIn this example, only requests that include a \"user\" header will be allowed to access the Gradio app. Of course, this does not add much security, since any user can add this header in their request.\n\nHere's a more complete example showing how to add Google OAuth to a Gradio app (assuming you've already created OAuth Credentials on the [Google Developer Console](https://console.cloud.google.com/project)):\n\n```python\nimport os\nfrom authlib.integrations.starlette_client import OAuth, OAuthError\nfrom fastapi import FastAPI, Depends, Request\nfrom starlette.config import Config\nfrom starlette.responses import RedirectResponse\nfrom starlette.middleware.sessions import SessionMiddleware\nimport uvicorn\nimport gradio as gr\n\napp = FastAPI()\n\n# Replace these with your own OAuth settings\nGOOGLE_CLIENT_ID = \"...\"\nGOOGLE_CLIENT_SECRET = \"...\"\nSECRET_KEY = \"...\"\n\nconfig_data = {'GOOGLE_CLIENT_ID': GOOGLE_CLIENT_ID, 'GOOGLE_CLIENT_SECRET': GOOGLE_CLIENT_SECRET}\nstarlette_config = Config(environ=config_data)\noauth = OAuth(starlette_config)\noauth.register(\n    name='google',\n    server_metadata_url='https://accounts.google.com/.well-known/openid-configuration',\n    client_kwargs={'scope': 'openid email profile'},\n)\n\nSECRET_KEY = os.environ.get('SECRET_KEY') or \"a_very_secret_key\"\napp.add_middleware(SessionMiddleware, secret_key=SECRET_KEY)\n\n# Dependency to get the current user\ndef get_user(request: Request):\n    user = request.session.get('user')\n    if user:\n        return user['name']\n    return None\n\n@app.get('/')\ndef public(user: dict = Depends(get_user)):\n    if user:\n        return RedirectResponse(url='/gradio')\n    else:\n        return RedirectResponse(url='/login-demo')\n\n@app.route('/logout')\nasync def logout(request: Request):\n    request.session.pop('user', None)\n    return RedirectResponse(url='/')\n\n@app.route('/login')\nasync def login(request: Request):\n    redirect_uri = request.url_for('auth')\n    # If your app is running on https, you should ensure that the\n    # `redirect_uri` is https, e.g. uncomment the following lines:\n    # \n    # from urllib.parse import urlparse, urlunparse\n    # redirect_uri = urlunparse(urlparse(str(redirect_uri))._replace(scheme='https'))\n    return await oauth.google.authorize_redirect(request, redirect_uri)\n\n@app.route('/auth')\nasync def auth(request: Request):\n    try:\n        access_token = await oauth.google.authorize_access_token(request)\n    except OAuthError:\n        return RedirectResponse(url='/')\n    request.session['user'] = dict(access_token)[\"userinfo\"]\n    return RedirectResponse(url='/')\n\nwith gr.Blocks() as login_demo:\n    gr.Button(\"Login\", link=\"/login\")\n\napp = gr.mount_gradio_app(app, login_demo, path=\"/login-demo\")\n\ndef greet(request: gr.Request):\n    return f\"Welcome to Gradio, {request.username}\"\n\nwith gr.Blocks() as main_demo:\n    m = gr.Markdown(\"Welcome to Gradio!\")\n    gr.Button(\"Logout\", link=\"/logout\")\n    main_demo.load(greet, None, m)\n\napp = gr.mount_gradio_app(app, main_demo, path=\"/gradio\", auth_dependency=get_user)\n\nif __name__ == '__main__':\n    uvicorn.run(app)\n```\n\nThere are actually two separate Gradio apps in this example! One that simply displays a log in button (this demo is accessible to any user), while the other main demo is only accessible to users that are logged in. You can try this example out on [this Space](https://huggingface.co/spaces/gradio/oauth-example).\n\n\n\n## Security and File Access\n\nSharing your Gradio app with others (by hosting it on Spaces, on your own server, or through temporary share links) **exposes** certain files on the host machine to users of your Gradio app.\n\nIn particular, Gradio apps ALLOW users to access to four kinds of files:\n\n- **Temporary files created by Gradio.** These are files that are created by Gradio as part of running your prediction function. For example, if your prediction function returns a video file, then Gradio will save that video to a temporary cache on your device and then send the path to the file to the front end. You can customize the location of temporary cache files created by Gradio by setting the environment variable `GRADIO_TEMP_DIR` to an absolute path, such as `/home/usr/scripts/project/temp/`. You can delete the files created by your app when it shuts down with the `delete_cache` parameter of `gradio.Blocks`, `gradio.Interface`, and `gradio.ChatInterface`. This parameter is a tuple of integers of the form `[frequency, age]` where `frequency` is how often to delete files and `age` is the time in seconds since the file was last modified.\n\n\n- **Cached examples created by Gradio.** These are files that are created by Gradio as part of caching examples for faster runtimes, if you set `cache_examples=True` or `cache_examples=\"lazy\"` in `gr.Interface()`, `gr.ChatInterface()` or in `gr.Examples()`. By default, these files are saved in the `gradio_cached_examples/` subdirectory within your app's working directory. You can customize the location of cached example files created by Gradio by setting the environment variable `GRADIO_EXAMPLES_CACHE` to an absolute path or a path relative to your working directory.\n\n- **Files that you explicitly allow via the `allowed_paths` parameter in `launch()`**. This parameter allows you to pass in a list of additional directories or exact filepaths you'd like to allow users to have access to. (By default, this parameter is an empty list).\n\n- **Static files that you explicitly set via the `gr.set_static_paths` function**. This parameter allows you to pass in a list of directories or filenames that will be considered static. This means that they will not be copied to the cache and will be served directly from your computer. This can help save disk space and reduce the time your app takes to launch but be mindful of possible security implications.\n\nGradio DOES NOT ALLOW access to:\n\n- **Files that you explicitly block via the `blocked_paths` parameter in `launch()`**. You can pass in a list of additional directories or exact filepaths to the `blocked_paths` parameter in `launch()`. This parameter takes precedence over the files that Gradio exposes by default or by the `allowed_paths`.\n\n- **Any other paths on the host machine**. Users should NOT be able to access other arbitrary paths on the host.\n\nSharing your Gradio application will also allow users to upload files to your computer or server. You can set a maximum file size for uploads to prevent abuse and to preserve disk space. You can do this with the `max_file_size` parameter of `.launch`. For example, the following two code snippets limit file uploads to 5 megabytes per file.\n\n```python\nimport gradio as gr\n\ndemo = gr.Interface(lambda x: x, \"image\", \"image\")\n\ndemo.launch(max_file_size=\"5mb\")\n# or\ndemo.launch(max_file_size=5 * gr.FileSize.MB)\n```\n\nPlease make sure you are running the latest version of `gradio` for these security settings to apply.\n\n## Analytics\n\nBy default, Gradio collects certain analytics to help us better understand the usage of the `gradio` library. This includes the following information:\n\n* What environment the Gradio app is running on (e.g. Colab Notebook, Hugging Face Spaces)\n* What input/output components are being used in the Gradio app\n* Whether the Gradio app is utilizing certain advanced features, such as `auth` or `show_error` \n* The IP address which is used solely to measure the number of unique developers using Gradio \n* The version of Gradio that is running \n\nNo information is collected from _users_ of your Gradio app. If you'd like to diable analytics altogether, you can do so by setting the `analytics_enabled` parameter to `False` in `gr.Blocks`, `gr.Interface`, or `gr.ChatInterface`. Or, you can set the GRADIO_ANALYTICS_ENABLED environment variable to `\"False\"` to apply this to all Gradio apps created across your system.\n\n*Note*: this reflects the analytics policy as of `gradio>=4.32.0`. \n",tags:[],spaces:[],url:"/guides/sharing-your-app/",contributor:null}],parent:"gradio"}},events_matrix:{AnnotatedImage:["select"],Audio:["stream","change","clear","play","pause","pause","stop","pause","pause","start_recording","pause_recording","stop_recording","upload"],Plot:["change","clear"],BarPlot:["change","clear"],Button:["click"],Chatbot:["change","select","like"],Checkbox:["change","input","select"],CheckboxGroup:["change","input","select"],ClearButton:["click"],Code:["change","input","focus","blur"],ColorPicker:["change","input","submit","focus","blur"],Dataframe:["change","input","select"],Dataset:["click","select"],DownloadButton:["click"],Dropdown:["change","input","select","focus","blur","key_up"],DuplicateButton:["click"],File:["change","select","clear","upload","delete"],FileExplorer:["change"],Gallery:["select","upload","change"],HighlightedText:["change","select"],HTML:["change"],Image:["clear","change","stream","select","upload"],ImageEditor:["clear","change","input","select","upload","apply"],JSON:["change"],Label:["change","select"],LinePlot:["change","clear"],LoginButton:["click"],LogoutButton:["click"],Markdown:["change"],Model3D:["change","upload","edit","clear"],MultimodalTextbox:["change","input","select","submit","focus","blur"],Number:["change","input","submit","focus"],ParamViewer:["change","upload"],Radio:["select","change","input"],ScatterPlot:["change","clear"],Slider:["change","input","release"],State:["change"],Textbox:["change","input","select","submit","focus","blur"],UploadButton:["click","upload"],Video:["change","clear","start_recording","stop_recording","stop","play","pause","end","upload"],SimpleImage:["clear","change","upload"]}},"python-client":{client:{class:null,name:"Client",description:"The main Client class for the Python client. This class is used to connect to a remote Gradio app and call its API endpoints. \u003Cbr>",tags:{},parameters:[{name:"src",annotation:"str",doc:"Either the name of the Hugging Face Space to load, (e.g. &quot;abidlabs/whisper-large-v2&quot;) or the full URL (including &quot;http&quot; or &quot;https&quot;) of the hosted Gradio app to load (e.g. &quot;http://mydomain.com/app&quot; or &quot;https://bec81a83-5b5c-471e.gradio.live/&quot;)."},{name:"hf_token",annotation:"str | None",doc:"The Hugging Face token to use to access private Spaces. Automatically fetched if you are logged in via the Hugging Face Hub CLI. Obtain from: https://huggingface.co/settings/token",default:"None"},{name:"max_workers",annotation:"int",doc:"The maximum number of thread workers that can be used to make requests to the remote Gradio app simultaneously.",default:"40"},{name:"verbose",annotation:"bool",doc:"Whether the client should print statements to the console.",default:"True"},{name:"auth",annotation:"tuple[str, str] | None",doc:null,default:"None"},{name:"headers",annotation:"dict[str, str] | None",doc:"Additional headers to send to the remote Gradio app on every request. By default only the HF authorization and user-agent headers are sent. This parameter will override the default headers if they have the same keys.",default:"None"},{name:"download_files",annotation:"str | Path | Literal[False]",doc:"Directory where the client should download output files  on the local machine from the remote API. By default, uses the value of the GRADIO_TEMP_DIR environment variable which, if not set by the user, is a temporary directory on your machine. If False, the client does not download files and returns a FileData dataclass object with the filepath on the remote machine instead.",default:"\"/tmp/gradio\""},{name:"ssl_verify",annotation:"bool",doc:"If False, skips certificate validation which allows the client to connect to Gradio apps that are using self-signed certificates.",default:"True"}],returns:{annotation:null},example:"from gradio_client import Client\n\nclient = Client(\"abidlabs/whisper-large-v2\")  # connecting to a Hugging Face Space\nclient.predict(\"test.mp4\", api_name=\"/predict\")\n>> What a nice recording! # returns the result of the remote API call\n\nclient = Client(\"https://bec81a83-5b5c-471e.gradio.live\")  # connecting to a temporary Gradio share URL\njob = client.submit(\"hello\", api_name=\"/predict\")  # runs the prediction in a background thread\njob.result()\n>> 49 # returns the result of the remote API call (blocking call)",fns:[{fn:null,name:"predict",description:"Calls the Gradio API and returns the result (this is a blocking call). &lt;br&gt;",tags:{},parameters:[{name:"args",annotation:"\u003Cclass 'inspect._empty'>",doc:"The arguments to pass to the remote API. The order of the arguments must match the order of the inputs in the Gradio app."},{name:"api_name",annotation:"str | None",doc:"The name of the API endpoint to call starting with a leading slash, e.g. &quot;/predict&quot;. Does not need to be provided if the Gradio app has only one named API endpoint.",default:"None"},{name:"fn_index",annotation:"int | None",doc:"As an alternative to api_name, this parameter takes the index of the API endpoint to call, e.g. 0. Both api_name and fn_index can be provided, but if they conflict, api_name will take precedence.",default:"None"}],returns:{annotation:"Any",doc:"The result of the API call. Will be a Tuple if the API has multiple outputs."},example:"from gradio_client import Client\nclient = Client(src=\"gradio/calculator\")\nclient.predict(5, \"add\", 4, api_name=\"/predict\")\n>> 9.0",override_signature:null,parent:"gradio.Client"},{fn:null,name:"submit",description:"Creates and returns a Job object which calls the Gradio API in a background thread. The job can be used to retrieve the status and result of the remote API call. &lt;br&gt;",tags:{},parameters:[{name:"args",annotation:"\u003Cclass 'inspect._empty'>",doc:"The arguments to pass to the remote API. The order of the arguments must match the order of the inputs in the Gradio app."},{name:"api_name",annotation:"str | None",doc:"The name of the API endpoint to call starting with a leading slash, e.g. &quot;/predict&quot;. Does not need to be provided if the Gradio app has only one named API endpoint.",default:"None"},{name:"fn_index",annotation:"int | None",doc:"As an alternative to api_name, this parameter takes the index of the API endpoint to call, e.g. 0. Both api_name and fn_index can be provided, but if they conflict, api_name will take precedence.",default:"None"},{name:"result_callbacks",annotation:"Callable | list[Callable] | None",doc:"A callback function, or list of callback functions, to be called when the result is ready. If a list of functions is provided, they will be called in order. The return values from the remote API are provided as separate parameters into the callback. If None, no callback will be called.",default:"None"}],returns:{annotation:"Job",doc:"A Job object that can be used to retrieve the status and result of the remote API call."},example:"from gradio_client import Client\nclient = Client(src=\"gradio/calculator\")\njob = client.submit(5, \"add\", 4, api_name=\"/predict\")\njob.status()\n>> \u003CStatus.STARTING: 'STARTING'>\njob.result()  # blocking call\n>> 9.0",override_signature:null,parent:"gradio.Client"},{fn:null,name:"view_api",description:"Prints the usage info for the API. If the Gradio app has multiple API endpoints, the usage info for each endpoint will be printed separately. If return_format=&quot;dict&quot; the info is returned in dictionary format, as shown in the example below. &lt;br&gt;",tags:{},parameters:[{name:"all_endpoints",annotation:"bool | None",doc:"If True, prints information for both named and unnamed endpoints in the Gradio app. If False, will only print info about named endpoints. If None (default), will print info about named endpoints, unless there aren&#x27;t any -- in which it will print info about unnamed endpoints.",default:"None"},{name:"print_info",annotation:"bool",doc:"If True, prints the usage info to the console. If False, does not print the usage info.",default:"True"},{name:"return_format",annotation:"Literal[('dict', 'str')] | None",doc:"If None, nothing is returned. If &quot;str&quot;, returns the same string that would be printed to the console. If &quot;dict&quot;, returns the usage info as a dictionary that can be programmatically parsed, and *all endpoints are returned in the dictionary* regardless of the value of `all_endpoints`. The format of the dictionary is in the docstring of this method.",default:"None"}],returns:{},example:"from gradio_client import Client\nclient = Client(src=\"gradio/calculator\")\nclient.view_api(return_format=\"dict\")\n>> {\n    'named_endpoints': {\n        '/predict': {\n            'parameters': [\n                {\n                    'label': 'num1',\n                    'python_type': 'int | float',\n                    'type_description': 'numeric value',\n                    'component': 'Number',\n                    'example_input': '5'\n                },\n                {\n                    'label': 'operation',\n                    'python_type': 'str',\n                    'type_description': 'string value',\n                    'component': 'Radio',\n                    'example_input': 'add'\n                },\n                {\n                    'label': 'num2',\n                    'python_type': 'int | float',\n                    'type_description': 'numeric value',\n                    'component': 'Number',\n                    'example_input': '5'\n                },\n            ],\n            'returns': [\n                {\n                    'label': 'output',\n                    'python_type': 'int | float',\n                    'type_description': 'numeric value',\n                    'component': 'Number',\n                },\n            ]\n        },\n        '/flag': {\n            'parameters': [\n                ...\n                ],\n            'returns': [\n                ...\n                ]\n            }\n        }\n    'unnamed_endpoints': {\n        2: {\n            'parameters': [\n                ...\n                ],\n            'returns': [\n                ...\n                ]\n            }\n        }\n    }\n}",override_signature:null,parent:"gradio.Client"},{fn:null,name:"duplicate",description:"Duplicates a Hugging Face Space under your account and returns a Client object for the new Space. No duplication is created if the Space already exists in your account (to override this, provide a new name for the new Space using `to_id`). To use this method, you must provide an `hf_token` or be logged in via the Hugging Face Hub CLI. &lt;br&gt; The new Space will be private by default and use the same hardware as the original Space. This can be changed by using the `private` and `hardware` parameters. For hardware upgrades (beyond the basic CPU tier), you may be required to provide billing information on Hugging Face: https://huggingface.co/settings/billing &lt;br&gt;",tags:{},parameters:[{name:"from_id",annotation:"str",doc:"The name of the Hugging Face Space to duplicate in the format &quot;{username}/{space_id}&quot;, e.g. &quot;gradio/whisper&quot;."},{name:"to_id",annotation:"str | None",doc:"The name of the new Hugging Face Space to create, e.g. &quot;abidlabs/whisper-duplicate&quot;. If not provided, the new Space will be named &quot;{your_HF_username}/{space_id}&quot;.",default:"None"},{name:"hf_token",annotation:"str | None",doc:"The Hugging Face token to use to access private Spaces. Automatically fetched if you are logged in via the Hugging Face Hub CLI. Obtain from: https://huggingface.co/settings/token",default:"None"},{name:"private",annotation:"bool",doc:"Whether the new Space should be private (True) or public (False). Defaults to True.",default:"True"},{name:"hardware",annotation:"Literal[('cpu-basic', 'cpu-upgrade', 't4-small', 't4-medium', 'a10g-small', 'a10g-large', 'a100-large')] | SpaceHardware | None",doc:"The hardware tier to use for the new Space. Defaults to the same hardware tier as the original Space. Options include &quot;cpu-basic&quot;, &quot;cpu-upgrade&quot;, &quot;t4-small&quot;, &quot;t4-medium&quot;, &quot;a10g-small&quot;, &quot;a10g-large&quot;, &quot;a100-large&quot;, subject to availability.",default:"None"},{name:"secrets",annotation:"dict[str, str] | None",doc:"A dictionary of (secret key, secret value) to pass to the new Space. Defaults to None. Secrets are only used when the Space is duplicated for the first time, and are not updated if the duplicated Space already exists.",default:"None"},{name:"sleep_timeout",annotation:"int",doc:"The number of minutes after which the duplicate Space will be puased if no requests are made to it (to minimize billing charges). Defaults to 5 minutes.",default:"5"},{name:"max_workers",annotation:"int",doc:"The maximum number of thread workers that can be used to make requests to the remote Gradio app simultaneously.",default:"40"},{name:"verbose",annotation:"bool",doc:"Whether the client should print statements to the console.",default:"True"}],returns:{},example:"import os\nfrom gradio_client import Client\nHF_TOKEN = os.environ.get(\"HF_TOKEN\")\nclient = Client.duplicate(\"abidlabs/whisper\", hf_token=HF_TOKEN)\nclient.predict(\"audio_sample.wav\")\n>> \"This is a test of the whisper speech recognition model.\"",override_signature:null,parent:"gradio.Client"},{fn:null,name:"deploy_discord",description:"Deploy the upstream app as a discord bot. Currently only supports gr.ChatInterface.",tags:{},parameters:[{name:"discord_bot_token",annotation:"str | None",doc:"This is the &quot;password&quot; needed to be able to launch the bot. Users can get a token by creating a bot app on the discord website. If run the method without specifying a token, the space will explain how to get one. See here: https://huggingface.co/spaces/freddyaboulton/test-discord-bot-v1.",default:"None"},{name:"api_names",annotation:"list[str | tuple[str, str]] | None",doc:"The api_names of the app to turn into bot commands. This parameter currently has no effect as ChatInterface only has one api_name (&#x27;/chat&#x27;).",default:"None"},{name:"to_id",annotation:"str | None",doc:"The name of the space hosting the discord bot. If None, the name will be gradio-discord-bot-{random-substring}",default:"None"},{name:"hf_token",annotation:"str | None",doc:"HF api token with write priviledges in order to upload the files to HF space. Can be ommitted if logged in via the HuggingFace CLI, unless the upstream space is private. Obtain from: https://huggingface.co/settings/token",default:"None"},{name:"private",annotation:"bool",doc:"Whether the space hosting the discord bot is private. The visibility of the discord bot itself is set via the discord website. See https://huggingface.co/spaces/freddyaboulton/test-discord-bot-v1",default:"False"}],returns:{},example:null,override_signature:null,parent:"gradio.Client"}],parent:"gradio"},job:{class:null,name:"Job",description:"A Job is a wrapper over the Future class that represents a prediction call that has been submitted by the Gradio client. This class is not meant to be instantiated directly, but rather is created by the Client.submit() method. \u003Cbr> A Job object includes methods to get the status of the prediction call, as well to get the outputs of the prediction call. Job objects are also iterable, and can be used in a loop to get the outputs of prediction calls as they become available for generator endpoints.",tags:{},parameters:[{name:"future",annotation:"Future",doc:"The future object that represents the prediction call, created by the Client.submit() method"},{name:"communicator",annotation:"Communicator | None",doc:"The communicator object that is used to communicate between the client and the background thread running the job",default:"None"},{name:"verbose",annotation:"bool",doc:"Whether to print any status-related messages to the console",default:"True"},{name:"space_id",annotation:"str | None",doc:"The space ID corresponding to the Client object that created this Job object",default:"None"}],returns:{annotation:null},example:null,fns:[{fn:null,name:"result",description:"Return the result of the call that the future represents. Raises CancelledError: If the future was cancelled, TimeoutError: If the future didn&#x27;t finish executing before the given timeout, and Exception: If the call raised then that exception will be raised. &lt;br&gt;",tags:{},parameters:[{name:"timeout",annotation:"float | None",doc:"The number of seconds to wait for the result if the future isn&#x27;t done. If None, then there is no limit on the wait time.",default:"None"}],returns:{annotation:"Any",doc:"The result of the call that the future represents. For generator functions, it will return the final iteration."},example:"from gradio_client import Client\ncalculator = Client(src=\"gradio/calculator\")\njob = calculator.submit(\"foo\", \"add\", 4, fn_index=0)\njob.result(timeout=5)\n>> 9",override_signature:null,parent:"gradio.Job"},{fn:null,name:"outputs",description:"Returns a list containing the latest outputs from the Job. &lt;br&gt; If the endpoint has multiple output components, the list will contain a tuple of results. Otherwise, it will contain the results without storing them in tuples. &lt;br&gt; For endpoints that are queued, this list will contain the final job output even if that endpoint does not use a generator function. &lt;br&gt;",tags:{},parameters:[],returns:{},example:"from gradio_client import Client\nclient = Client(src=\"gradio/count_generator\")\njob = client.submit(3, api_name=\"/count\")\nwhile not job.done():\n    time.sleep(0.1)\njob.outputs()\n>> ['0', '1', '2']",override_signature:null,parent:"gradio.Job"},{fn:null,name:"status",description:"Returns the latest status update from the Job in the form of a StatusUpdate object, which contains the following fields: code, rank, queue_size, success, time, eta, and progress_data. &lt;br&gt; progress_data is a list of updates emitted by the gr.Progress() tracker of the event handler. Each element of the list has the following fields: index, length, unit, progress, desc. If the event handler does not have a gr.Progress() tracker, the progress_data field will be None. &lt;br&gt;",tags:{},parameters:[],returns:{},example:"from gradio_client import Client\nclient = Client(src=\"gradio/calculator\")\njob = client.submit(5, \"add\", 4, api_name=\"/predict\")\njob.status()\n>> \u003CStatus.STARTING: 'STARTING'>\njob.status().eta\n>> 43.241  # seconds",override_signature:null,parent:"gradio.Job"}],parent:"gradio"}}},js:{textbox:"# `@gradio/textbox`\n [v0.6.5](https://www.npmjs.com/package/@gradio/textbox)\n\n```html\n\u003Cscript>\n    import { BaseTextbox, BaseExample } from \"@gradio/textbox\";\n\u003C/script>\n```\n\nBaseTextbox\n```javascript\n\texport let value = \"\";\n\texport let value_is_output = false;\n\texport let lines = 1;\n\texport let placeholder = \"Type here...\";\n\texport let label: string;\n\texport let info: string | undefined = undefined;\n\texport let disabled = false;\n\texport let show_label = true;\n\texport let container = true;\n\texport let max_lines: number;\n\texport let type: \"text\" | \"password\" | \"email\" = \"text\";\n\texport let show_copy_button = false;\n\texport let rtl = false;\n\texport let autofocus = false;\n\texport let text_align: \"left\" | \"right\" | undefined = undefined;\n\texport let autoscroll = true;\n```\n\nBaseExample\n```javascript\n\texport let value: string;\n\texport let type: \"gallery\" | \"table\";\n\texport let selected = false;\n```",multimodaltextbox:"# `@gradio/multimodaltextbox`\n [v0.4.10](https://www.npmjs.com/package/@gradio/multimodaltextbox)\n\n```html\n\u003Cscript>\n    import { BaseMultimodalTextbox, BaseExample } from \"@gradio/multimodaltextbox\";\n\u003C/script>\n```\n\nBaseMultimodalTextbox\n```javascript\n\texport let value = \"\";\n\texport let value_is_output = false;\n\texport let lines = 1;\n\texport let placeholder = \"Type here...\";\n\texport let label: string;\n\texport let info: string | undefined = undefined;\n\texport let disabled = false;\n\texport let show_label = true;\n\texport let container = true;\n\texport let max_lines: number;\n\texport let type: \"text\" | \"password\" | \"email\" = \"text\";\n\texport let show_copy_button = false;\n\texport let rtl = false;\n\texport let autofocus = false;\n\texport let text_align: \"left\" | \"right\" | undefined = undefined;\n\texport let autoscroll = true;\n```\n\nBaseExample\n```javascript\n\texport let value: string;\n\texport let type: \"gallery\" | \"table\";\n\texport let selected = false;\n```",markdown:"# `@gradio/markdown`\n [v0.7.6](https://www.npmjs.com/package/@gradio/markdown)\n\n```html\n\u003Cscript>\n    import { BaseMarkdown, MarkdownCode, BaseExample } from `@gradio/markdown`;\n\u003C/script>\n```\n\nBaseMarkdown\n```javascript\n\texport let elem_id = \"\";\n\texport let elem_classes: string[] = [];\n\texport let visible = true;\n\texport let value: string;\n\texport let min_height = false;\n\texport let rtl = false;\n\texport let sanitize_html = true;\n\texport let line_breaks = false;\n\texport let latex_delimiters: {\n\t\tleft: string;\n\t\tright: string;\n\t\tdisplay: boolean;\n\t}[];\n```\n\nMarkdownCode\n```javascript\n\texport let chatbot = true;\n\texport let message: string;\n\texport let sanitize_html = true;\n\texport let latex_delimiters: {\n\t\tleft: string;\n\t\tright: string;\n\t\tdisplay: boolean;\n\t}[] = [];\n\texport let render_markdown = true;\n\texport let line_breaks = true;\n```\n\nBaseExample\n```javascript\n\texport let value: string;\n\texport let type: \"gallery\" | \"table\";\n\texport let selected = false;\n\texport let sanitize_html: boolean;\n\texport let line_breaks: boolean;\n\texport let latex_delimiters: {\n\t\tleft: string;\n\t\tright: string;\n\t\tdisplay: boolean;\n\t}[];\n```",upload:"# `@gradio/upload`\n [v0.11.1](https://www.npmjs.com/package/@gradio/upload)\n\n```html\n\u003Cscript>\n    import { Upload, ModifyUpload, normalise_file, get_fetchable_url_or_file, upload, prepare_files } from \"@gradio/upload\";\n\u003C/script>\n```\n\nUpload\n```javascript\n\texport let filetype: string | null = null;\n\texport let dragging = false;\n\texport let boundedheight = true;\n\texport let center = true;\n\texport let flex = true;\n\texport let file_count = \"single\";\n\texport let disable_click = false;\n\texport let root: string;\n\texport let hidden = false;\n```\n\nModifyUpload\n```javascript\n    export let editable = false;\n\texport let undoable = false;\n\texport let absolute = true;\n\texport let i18n: I18nFormatter;\n```\n\n```javascript\nexport function normalise_file(\n\tfile: FileData | null,\n\tserver_url: string,\n\tproxy_url: string | null\n): FileData | null;\n\nexport function normalise_file(\n\tfile: FileData[] | null,\n\tserver_url: string,\n\tproxy_url: string | null\n): FileData[] | null;\n\nexport function normalise_file(\n\tfile: FileData[] | FileData | null,\n\tserver_url: string, // root: string,\n\tproxy_url: string | null // root_url: string | null\n): FileData[] | FileData | null;\n\nexport function normalise_file(\n\tfile: FileData[] | FileData | null,\n\tserver_url: string, // root: string,\n\tproxy_url: string | null // root_url: string | null\n): FileData[] | FileData | null;\n\nexport function get_fetchable_url_or_file(\n\tpath: string | null,\n\tserver_url: string,\n\tproxy_url: string | null\n): string\n\nexport async function upload(\n\tfile_data: FileData[],\n\troot: string,\n\tupload_fn: typeof upload_files = upload_files\n): Promise\u003C(FileData | null)[] | null>\n\nexport async function prepare_files(\n\tfiles: File[],\n\tis_stream?: boolean\n): Promise\u003CFileData[]> {\n\treturn files.map(\n\t\t(f, i) =>\n\t\t\tnew FileData({\n\t\t\t\tpath: f.name,\n\t\t\t\torig_name: f.name,\n\t\t\t\tblob: f,\n\t\t\t\tsize: f.size,\n\t\t\t\tmime_type: f.type,\n\t\t\t\tis_stream\n\t\t\t})\n\t);\n}\n```",statustracker:"# `@gradio/statustracker`\n [v0.6.0](https://www.npmjs.com/package/@gradio/statustracker)\n\n```html\n\u003Cscript>\n    import {StatusTracker, Toast, Loader} from `@gradio/statustracker`;\n\u003C/script>\n```\n\nStatusTracker\n```javascript\n\texport let i18n: I18nFormatter;\n\texport let eta: number | null = null;\n\texport let queue = false;\n\texport let queue_position: number | null;\n\texport let queue_size: number | null;\n\texport let status: \"complete\" | \"pending\" | \"error\" | \"generating\";\n\texport let scroll_to_output = false;\n\texport let timer = true;\n\texport let show_progress: \"full\" | \"minimal\" | \"hidden\" = \"full\";\n\texport let message: string | null = null;\n\texport let progress: LoadingStatus[\"progress\"] | null | undefined = null;\n\texport let variant: \"default\" | \"center\" = \"default\";\n\texport let loading_text = \"Loading...\";\n\texport let absolute = true;\n\texport let translucent = false;\n\texport let border = false;\n\texport let autoscroll: boolean;\n```\n\nToast\n```javascript\n\texport let messages: ToastMessage[] = [];\n```\n\nLoader\n```javascript\n\texport let margin = true;\n```",checkbox:"# `@gradio/checkbox`\n [v0.3.6](https://www.npmjs.com/package/@gradio/checkbox)\n\n```html\n\u003Cscript>\n    import { BaseCheckbox } from \"@gradio/checkbox\";\n\u003C/script>\n```\n\nBaseCheckBox:\n```javascript\n\texport let value = false;\n\texport let label = \"Checkbox\";\n\texport let mode: \"static\" | \"interactive\";\n```",lite:"# Gradio-Lite: Serverless Gradio Running Entirely in Your Browser\n [v4.36.0](https://www.npmjs.com/package/@gradio/lite)\n\n\nGradio is a popular Python library for creating interactive machine learning apps. Traditionally, Gradio applications have relied on server-side infrastructure to run, which can be a hurdle for developers who need to host their applications. \n\nEnter Gradio-lite (`@gradio/lite`): a library that leverages [Pyodide](https://pyodide.org/en/stable/) to bring Gradio directly to your browser. \n\n## What is `@gradio/lite`?\n\n`@gradio/lite` is a JavaScript library that enables you to run Gradio applications directly within your web browser. It achieves this by utilizing Pyodide, a Python runtime for WebAssembly, which allows Python code to be executed in the browser environment. With `@gradio/lite`, you can **write regular Python code for your Gradio applications**, and they will **run seamlessly in the browser** without the need for server-side infrastructure.\n\n## Getting Started\n\nLet's build a \"Hello World\" Gradio app in `@gradio/lite`\n\n\n### 1. Import JS and CSS \n\nStart by creating a new HTML file, if you don't have one already. The best way to use @gradio/lite currently is via the CDN. Import the JavaScript and CSS corresponding to the `@gradio/lite` package by using the following code:\n\n\n```html\n\u003Chtml>\n\t\u003Chead>\n\t\t\u003Cscript type=\"module\" crossorigin src=\"https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.js\">\u003C/script>\n\t\t\u003Clink rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.css\" />\n\t\u003C/head>\n\u003C/html>\n```\n\nNote that you should generally use the latest version of `@gradio/lite` that is available. You can see the [versions available here](https://www.jsdelivr.com/package/npm/@gradio/lite?tab=files).\n\n### 2. Create the `\u003Cgradio-lite>` tags\n\nSomewhere in the body of your HTML page (wherever you'd like the Gradio app to be rendered), create opening and closing `\u003Cgradio-lite>` tags. \n\n```html\n\u003Chtml>\n\t\u003Chead>\n\t\t\u003Cscript type=\"module\" crossorigin src=\"https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.js\">\u003C/script>\n\t\t\u003Clink rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.css\" />\n\t\u003C/head>\n\t\u003Cbody>\n\t\t\u003Cgradio-lite>\n\t\t\u003C/gradio-lite>\n\t\u003C/body>\n\u003C/html>\n```\n\nNote: you can add the `theme` attribute to the `\u003Cgradio-lite>` tag to force the theme to be dark or light (by default, it respects the system theme). E.g.\n\n```html\n\u003Cgradio-lite theme=\"dark\">\n...\n\u003C/gradio-lite>\n```\n\n### 3. Write your Gradio app inside of the tags\n\nNow, write your Gradio app as you would normally, in Python! Keep in mind that since this is Python, whitespace and indentations matter. \n\n```html\n\u003Chtml>\n\t\u003Chead>\n\t\t\u003Cscript type=\"module\" crossorigin src=\"https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.js\">\u003C/script>\n\t\t\u003Clink rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.css\" />\n\t\u003C/head>\n\t\u003Cbody>\n\t\t\u003Cgradio-lite>\n\t\timport gradio as gr\n\n\t\tdef greet(name):\n\t\t\treturn \"Hello, \" + name + \"!\"\n\t\t\n\t\tgr.Interface(greet, \"textbox\", \"textbox\").launch()\n\t\t\u003C/gradio-lite>\n\t\u003C/body>\n\u003C/html>\n```\n\nAnd that's it! You should now be able to open your HTML page in the browser and see the Gradio app rendered! Note that it may take a little while for the Gradio app to load initially since Pyodide can take a while to install in your browser.\n\n**Note on debugging**: to see any errors in your Gradio-lite application, open the inspector in your web browser. All errors (including Python errors) will be printed there.\n\n## More Examples: Adding Additional Files and Requirements\n\nWhat if you want to create a Gradio app that spans multiple files? Or that has custom Python requirements? Both are possible with `@gradio/lite`!\n\n### Multiple Files\n\nAdding multiple files within a `@gradio/lite` app is very straightrward: use the `\u003Cgradio-file>` tag. You can have as many `\u003Cgradio-file>` tags as you want, but each one needs to have a `name` attribute and the entry point to your Gradio app should have the `entrypoint` attribute.\n\nHere's an example:\n\n```html\n\u003Cgradio-lite>\n\n\u003Cgradio-file name=\"app.py\" entrypoint>\nimport gradio as gr\nfrom utils import add\n\ndemo = gr.Interface(fn=add, inputs=[\"number\", \"number\"], outputs=\"number\")\n\ndemo.launch()\n\u003C/gradio-file>\n\n\u003Cgradio-file name=\"utils.py\" >\ndef add(a, b):\n\treturn a + b\n\u003C/gradio-file>\n\n\u003C/gradio-lite>\t\t\n\n```\n\n### Additional Requirements\n\nIf your Gradio app has additional requirements, it is usually possible to [install them in the browser using micropip](https://pyodide.org/en/stable/usage/loading-packages.html#loading-packages). We've created a wrapper to make this paticularly convenient: simply list your requirements in the same syntax as a `requirements.txt` and enclose them with `\u003Cgradio-requirements>` tags.\n\nHere, we install `transformers_js_py` to run a text classification model directly in the browser!\n\n```html\n\u003Cgradio-lite>\n\n\u003Cgradio-requirements>\ntransformers_js_py\n\u003C/gradio-requirements>\n\n\u003Cgradio-file name=\"app.py\" entrypoint>\nfrom transformers_js import import_transformers_js\nimport gradio as gr\n\ntransformers = await import_transformers_js()\npipeline = transformers.pipeline\npipe = await pipeline('sentiment-analysis')\n\nasync def classify(text):\n\treturn await pipe(text)\n\ndemo = gr.Interface(classify, \"textbox\", \"json\")\ndemo.launch()\n\u003C/gradio-file>\n\n\u003C/gradio-lite>\t\n\n```\n\n**Try it out**: You can see this example running in [this Hugging Face Static Space](https://huggingface.co/spaces/abidlabs/gradio-lite-classify), which lets you host static (serverless) web applications for free. Visit the page and you'll be able to run a machine learning model without internet access!\n\n## Benefits of Using `@gradio/lite`\n\n### 1. Serverless Deployment\nThe primary advantage of @gradio/lite is that it eliminates the need for server infrastructure. This simplifies deployment, reduces server-related costs, and makes it easier to share your Gradio applications with others.\n\n### 2. Low Latency\nBy running in the browser, @gradio/lite offers low-latency interactions for users. There's no need for data to travel to and from a server, resulting in faster responses and a smoother user experience.\n\n### 3. Privacy and Security\nSince all processing occurs within the user's browser, `@gradio/lite` enhances privacy and security. User data remains on their device, providing peace of mind regarding data handling.\n\n### Limitations\n\n* Currently, the biggest limitation in using `@gradio/lite` is that your Gradio apps will generally take more time (usually 5-15 seconds) to load initially in the browser. This is because the browser needs to load the Pyodide runtime before it can render Python code. \n\n* Not every Python package is supported by Pyodide. While `gradio` and many other popular packages (including `numpy`, `scikit-learn`, and `transformers-js`) can be installed in Pyodide, if your app has many dependencies, its worth checking whether whether the dependencies are included in Pyodide, or can be [installed with `micropip`](https://micropip.pyodide.org/en/v0.2.2/project/api.html#micropip.install).\n\n## Try it out!\n\nYou can immediately try out `@gradio/lite` by copying and pasting this code in a local `index.html` file and opening it with your browser:\n\n```html\n\u003Chtml>\n\t\u003Chead>\n\t\t\u003Cscript type=\"module\" crossorigin src=\"https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.js\">\u003C/script>\n\t\t\u003Clink rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.css\" />\n\t\u003C/head>\n\t\u003Cbody>\n\t\t\u003Cgradio-lite>\n\t\timport gradio as gr\n\n\t\tdef greet(name):\n\t\t\treturn \"Hello, \" + name + \"!\"\n\t\t\n\t\tgr.Interface(greet, \"textbox\", \"textbox\").launch()\n\t\t\u003C/gradio-lite>\n\t\u003C/body>\n\u003C/html>\n```\n\n\nWe've also created a playground on the Gradio website that allows you to interactively edit code and see the results immediately! \n\nPlayground: https://www.gradio.app/playground\n\n\n",colorpicker:"# `@gradio/colorpicker`\n [v0.3.6](https://www.npmjs.com/package/@gradio/colorpicker)\n\n```html\n\u003Cscript>\n    import { BaseColorPicker, BaseExample } from \"@gradio/colorpicker\";\n\u003C/script>\n```\n\nBaseColorPicker\n```javascript\n\texport let value = \"#000000\";\n\texport let value_is_output = false;\n\texport let label: string;\n\texport let info: string | undefined = undefined;\n\texport let disabled = false;\n\texport let show_label = true;\n```\n\nBaseExample\n```javascript\n\texport let value: string;\n\texport let type: \"gallery\" | \"table\";\n\texport let selected = false;\n```",tooltip:"# `@gradio/tooltip`\n [v0.1.0](https://www.npmjs.com/package/@gradio/tooltip)\n\n```javascript\nimport { Tooltip } from \"@gradio/tooltip\";\n```\n\n```javascript\n\texport let text: string;\n\texport let x: number;\n\texport let y: number;\n\texport let color: string;\n```",image:"# `@gradio/image`\n [v0.11.9](https://www.npmjs.com/package/@gradio/image)\n\n```html\n\u003Cscript>\n\timport { BaseImageUploader, BaseStaticImage, Webcam, BaseExample } from \"@gradio/image\";\n\u003C/script>\n```\n\nBaseImageUploader\n```javascript\n\texport let sources: (\"clipboard\" | \"webcam\" | \"upload\")[] = [\n\t\t\"upload\",\n\t\t\"clipboard\",\n\t\t\"webcam\"\n\t];\n\texport let streaming = false;\n\texport let pending = false;\n\texport let mirror_webcam: boolean;\n\texport let selectable = false;\n\texport let root: string;\n\texport let i18n: I18nFormatter;\n```\n\nBaseStaticImage\n```javascript\n\texport let value: null | FileData;\n\texport let label: string | undefined = undefined;\n\texport let show_label: boolean;\n\texport let show_download_button = true;\n\texport let selectable = false;\n\texport let show_share_button = false;\n\texport let root: string;\n\texport let i18n: I18nFormatter;\n```\n\nWebcam\n```javascript\n\texport let streaming = false;\n\texport let pending = false;\n\n\texport let mode: \"image\" | \"video\" = \"image\";\n\texport let mirror_webcam: boolean;\n\texport let include_audio: boolean;\n\texport let i18n: I18nFormatter;\n```\n\nBaseExample\n```javascript\n\texport let value: string;\n\texport let samples_dir: string;\n\texport let type: \"gallery\" | \"table\";\n\texport let selected = false;\n```",chatbot:"# `@gradio/button`\n [v0.10.10](https://www.npmjs.com/package/@gradio/chatbot)\n\n```html\n\u003Cscript>\n\timport { BaseChatBot } from \"@gradio/chatbot\";\n\u003C/script>\n```\n\n\nBaseChatBot\n```javascript\n\texport let value:\n\t\t| [\n\t\t\t\tstring | { file: FileData; alt_text: string | null } | null,\n\t\t\t\tstring | { file: FileData; alt_text: string | null } | null\n\t\t  ][]\n\t\t| null;\n\tlet old_value:\n\t\t| [\n\t\t\t\tstring | { file: FileData; alt_text: string | null } | null,\n\t\t\t\tstring | { file: FileData; alt_text: string | null } | null\n\t\t  ][]\n\t\t| null = null;\n\texport let latex_delimiters: {\n\t\tleft: string;\n\t\tright: string;\n\t\tdisplay: boolean;\n\t}[];\n\texport let pending_message = false;\n\texport let selectable = false;\n\texport let likeable = false;\n\texport let show_share_button = false;\n\texport let rtl = false;\n\texport let show_copy_button = false;\n\texport let avatar_images: [string | null, string | null] = [null, null];\n\texport let sanitize_html = true;\n\texport let bubble_full_width = true;\n\texport let render_markdown = true;\n\texport let line_breaks = true;\n\texport let root: string;\n\texport let root_url: null | string;\n\texport let i18n: I18nFormatter;\n\texport let layout: \"bubble\" | \"panel\" = \"bubble\";\n```",utils:"# `@gradio/utils`\n [v0.4.2](https://www.npmjs.com/package/@gradio/utils)\n\nGeneral functions for handling events in Gradio Svelte components\n\n\n```javascript\nexport async function uploadToHuggingFace(\n\t\tdata: string,\n\t\ttype: \"base64\" | \"url\"\n\t): Promise\u003Cstring>\n\nexport function copy(node: HTMLDivElement): ActionReturn\n\n\n```",gallery:"# `@gradio/gallery`\n [v0.10.9](https://www.npmjs.com/package/@gradio/gallery)\n\n```html\n\u003Cscript>\n\timport { BaseGallery } from \"@gradio/gallery\";\n\u003C/script>\n```\n\nBaseGallery\n```javascript\n\texport let show_label = true;\n\texport let label: string;\n\texport let root = \"\";\n\texport let root_url: null | string = null;\n\texport let value: { image: FileData; caption: string | null }[] | null = null;\n\texport let columns: number | number[] | undefined = [2];\n\texport let rows: number | number[] | undefined = undefined;\n\texport let height: number | \"auto\" = \"auto\";\n\texport let preview: boolean;\n\texport let allow_preview = true;\n\texport let object_fit: \"contain\" | \"cover\" | \"fill\" | \"none\" | \"scale-down\" =\n\t\t\"cover\";\n\texport let show_share_button = false;\n\texport let show_download_button = false;\n\texport let i18n: I18nFormatter;\n\texport let selected_index: number | null = null;\n```",radio:"# `@gradio/radio`\n [v0.5.6](https://www.npmjs.com/package/@gradio/radio)\n\n```html\n\u003Cscript>\n    import { BaseRadio, BaseExample } from \"@gradio/radio\"; \n\u003C/script>\n```\n\nBaseRadio\n```javascript\n\texport let display_value: string;\n\texport let internal_value: string | number;\n\texport let disabled = false;\n\texport let elem_id = \"\";\n\texport let selected: string | number | null = null;\n```\n\nBaseExample\n```javascript\n\texport let value: string;\n\texport let type: \"gallery\" | \"table\";\n\texport let selected = false;\n```",json:"# `@gradio/json`\n [v0.2.6](https://www.npmjs.com/package/@gradio/json)\n\n```html\n\u003Cscript>\n\timport { BaseJSON } from \"@gradio/json\";\n\u003C/script>\n```\n\nBaseJSON\n```html\n\texport let value: any = {};\n```",html:"# `@gradio/html`\n [v0.2.6](https://www.npmjs.com/package/@gradio/html)\n\n```javascript\nimport { BaseHTML } from \"@gradio/html\";\n```\n\nBaseHTML\n```javascript\n\texport let elem_id = \"\";\n\texport let elem_classes: string[] = [];\n\texport let value: string;\n\texport let visible = true;\n\texport let min_height = false;\n```",dataframe:"# `@gradio/dataframe`\n [v0.8.9](https://www.npmjs.com/package/@gradio/dataframe)\n\n```html\n\u003Cscript>\n    import { BaseDataFrame, BaseExample } from \"@gradio/dataframe\";\n\u003C/script>\n```\n\nBaseDataFrame\n```javascript\n\texport let datatype: Datatype | Datatype[];\n\texport let label: string | null = null;\n\texport let headers: Headers = [];\n\tlet values: (string | number)[][];\n\texport let value: { data: Data; headers: Headers; metadata: Metadata } | null;\n\texport let col_count: [number, \"fixed\" | \"dynamic\"];\n\texport let row_count: [number, \"fixed\" | \"dynamic\"];\n\texport let latex_delimiters: {\n\t\tleft: string;\n\t\tright: string;\n\t\tdisplay: boolean;\n\t}[];\n\n\texport let editable = true;\n\texport let wrap = false;\n\texport let root: string;\n\texport let i18n: I18nFormatter;\n\n\texport let height = 500;\n\texport let line_breaks = true;\n\texport let column_widths: string[] = [];\n```\n\nBaseExample\n```javascript\n\texport let gradio: Gradio;\n\texport let value: (string | number)[][] | string;\n\texport let type: \"gallery\" | \"table\";\n\texport let selected = false;\n\texport let index: number;\n```",button:"# `@gradio/button`\n [v0.2.42](https://www.npmjs.com/package/@gradio/button)\n\n```javascript\n\u003Cscript>\n\timport { BaseButton } from \"@gradio/button\";\n\timport { createEventDispatcher, tick, getContext } from \"svelte\";\n\tconst dispatch = createEventDispatcher();\n\u003C/script>\n\n\u003CBaseButton\n\t{value}\n\t{variant}\n\t{elem_id}\n\t{elem_classes}\n\t{size}\n\t{scale}\n\t{link}\n\t{icon}\n\t{min_width}\n\t{visible}\n\t{root}\n\t{root_url}\n\ton:click={() => dispatch(\"click\")}\n>\n\t{\"My Button\"}\n\u003C/Button>\n```\n",label:"# `@gradio/label`\n [v0.3.6](https://www.npmjs.com/package/@gradio/label)\n\n```html\n\u003Cscript>\n\timport { BaseLabel } from \"@gradio/label\";\n\u003C/script>\n```\n\nBaseLabel\n```javascript\n\texport let value: {\n\t\tlabel?: string;\n\t\tconfidences?: { label: string; confidence: number }[];\n\t};\n\texport let color: string | undefined = undefined;\n\texport let selectable = false;\n```\n",audio:"# `@gradio/audio`\n [v0.11.9](https://www.npmjs.com/package/@gradio/audio)\n\n```html\n\u003Cscript>\n\timport { BaseStaticAudio, BaseInteractiveAudio, BasePlayer, BaseExample } from \"@gradio/audio\";\n\u003C/script>\n```\n\n\nBaseExample:\n```javascript\n\texport let value: string;\n\texport let type: \"gallery\" | \"table\";\n\texport let selected = false;\n```\n\nBaseStaticAudio:\n```javascript\n\texport let value: null | { name: string; data: string } = null;\n\texport let label: string;\n\texport let name: string;\n\texport let show_label = true;\n\texport let autoplay: boolean;\n\texport let show_download_button = true;\n\texport let show_share_button = false;\n\texport let i18n: I18nFormatter;\n\texport let waveform_settings = {};\n```\n\nBaseInteractiveAudio:\n```javascript\n\texport let value: null | { name: string; data: string } = null;\n\texport let label: string;\n\texport let root: string;\n\texport let show_label = true;\n\texport let sources:\n\t\t| [\"microphone\"]\n\t\t| [\"upload\"]\n\t\t| [\"microphone\", \"upload\"]\n\t\t| [\"upload\", \"microphone\"] = [\"microphone\", \"upload\"];\n\texport let pending = false;\n\texport let streaming = false;\n\texport let autoplay = false;\n\texport let i18n: I18nFormatter;\n\texport let waveform_settings = {};\n\texport let dragging: boolean;\n\texport let active_source: \"microphone\" | \"upload\";\n\texport let handle_reset_value: () => void = () => {};\n```\n\nBasePlayer:\n```javascript\n\texport let value: null | { name: string; data: string } = null;\n\texport let label: string;\n\texport let autoplay: boolean;\n\texport let i18n: I18nFormatter;\n\texport let dispatch: (event: any, detail?: any) => void;\n\texport let dispatch_blob: (\n\t\tblobs: Uint8Array[] | Blob[],\n\t\tevent: \"stream\" | \"change\" | \"stop_recording\"\n\t) => Promise\u003Cvoid> = () => Promise.resolve();\n\texport let interactive = false;\n\texport let waveform_settings = {};\n\texport let mode = \"\";\n\texport let handle_reset_value: () => void = () => {};\n```",imageeditor:"# `@gradio/imageeditor`\n [v0.7.9](https://www.npmjs.com/package/@gradio/imageeditor)\n",dropdown:"# `@gradio/dropdown`\n [v0.7.6](https://www.npmjs.com/package/@gradio/dropdown)\n\n```html\n\u003Cscript>\n    import {BaseDropdown, BaseMultiselect, BaseExample } from \"@gradio/dropdown\";\n\u003C/script>\n```\n\nBaseDropdown\n```javascript\n\texport let label: string;\n\texport let info: string | undefined = undefined;\n\texport let value: string | number | (string | number)[] | undefined = [];\n\texport let value_is_output = false;\n\texport let choices: [string, string | number][];\n\texport let disabled = false;\n\texport let show_label: boolean;\n\texport let container = true;\n\texport let allow_custom_value = false;\n\texport let filterable = true;\n```\n\nBaseMultiselect\n```javascript\n\texport let label: string;\n\texport let info: string | undefined = undefined;\n\texport let value: string | number | (string | number)[] | undefined = [];\n\texport let value_is_output = false;\n\texport let max_choices: number | null = null;\n\texport let choices: [string, string | number][];\n\texport let disabled = false;\n\texport let show_label: boolean;\n\texport let container = true;\n\texport let allow_custom_value = false;\n\texport let filterable = true;\n\texport let i18n: I18nFormatter;\n```\n\nBaseExample\n```javascript\n\texport let value: string;\n\texport let type: \"gallery\" | \"table\";\n\texport let selected = false;    \n```",file:"# `@gradio/file`\n [v0.8.1](https://www.npmjs.com/package/@gradio/file)\n\n```html\n\u003Cscript>\n\timport { BaseFile, BaseFileUpload, FilePreview, BaseExample } from \"@gradio/file\";\n\u003C/script>\n```\n\nBaseFile\n```javascript\n\texport let value: FileData | FileData[] | null = null;\n\texport let label: string;\n\texport let show_label = true;\n\texport let selectable = false;\n\texport let height: number | undefined = undefined;\n\texport let i18n: I18nFormatter;\n```\n\nBaseFileUpload\n```javascript\n\texport let value: null | FileData | FileData[];\n\texport let label: string;\n\texport let show_label = true;\n\texport let file_count = \"single\";\n\texport let file_types: string[] | null = null;\n\texport let selectable = false;\n\texport let root: string;\n\texport let height: number | undefined = undefined;\n\texport let i18n: I18nFormatter;\n```\n\nFilePreview\n```javascript\n\texport let value: FileData | FileData[];\n\texport let selectable = false;\n\texport let height: number | undefined = undefined;\n\texport let i18n: I18nFormatter;\n```\n\nBaseExample\n```javascript\n\texport let value: FileData;\n\texport let type: \"gallery\" | \"table\";\n\texport let selected = false;\n```",code:"# `@gradio/code`\n [v0.6.10](https://www.npmjs.com/package/@gradio/code)\n\n```html\n\u003Cscript>\n    import { BaseCode, BaseCopy, BaseDownload, BaseWidget, BaseExample} from \"gradio/code\";\n\u003C/script>\n```\n\nBaseCode\n```javascript\n\texport let class_names = \"\";\n\texport let value = \"\";\n\texport let dark_mode: boolean;\n\texport let basic = true;\n\texport let language: string;\n\texport let lines = 5;\n\texport let extensions: Extension[] = [];\n\texport let use_tab = true;\n\texport let readonly = false;\n\texport let placeholder: string | HTMLElement | null | undefined = undefined;\n```\n\nBaseCopy\n```javascript\n\texport let value: string;\n```\n\nBaseDownload\n```javascript\n\texport let value: string;\n\texport let language: string;\n```\n\nBaseWidget\n```javascript\n\texport let value: string;\n\texport let language: string;\n```\n\nBaseExample\n```\n\texport let value: string;\n\texport let type: \"gallery\" | \"table\";\n\texport let selected = false;\n```",atoms:"# `@gradio/atoms`\n [v0.7.4](https://www.npmjs.com/package/@gradio/atoms)\n\n```html\n\u003Cscript lang=\"ts\">\n\timport { Block, BlockTitle, BlockLabel, IconButton, Empty, Info, ShareButton, UploadText} from \"@gradio/atoms\";\n\u003C/script>\n```\n\nBlock:\n```javascript\n\texport let height: number | undefined = undefined;\n\texport let width: number | undefined = undefined;\n\texport let elem_id = \"\";\n\texport let elem_classes: string[] = [];\n\texport let variant: \"solid\" | \"dashed\" | \"none\" = \"solid\";\n\texport let border_mode: \"base\" | \"focus\" = \"base\";\n\texport let padding = true;\n\texport let type: \"normal\" | \"fieldset\" = \"normal\";\n\texport let test_id: string | undefined = undefined;\n\texport let explicit_call = false;\n\texport let container = true;\n\texport let visible = true;\n\texport let allow_overflow = true;\n\texport let scale: number | null = null;\n\texport let min_width = 0;\n```\n\nBlockTitle:\n```javascript\n\texport let show_label = true;\n\texport let info: string | undefined = undefined;\n```\n\nBlockLabel:\n```javascript\n\texport let label: string | null = null;\n\texport let Icon: any;\n\texport let show_label = true;\n\texport let disable = false;\n\texport let float = true;\n```\n\nIconButton:\n```javascript\n\texport let Icon: any;\n\texport let label = \"\";\n\texport let show_label = false;\n\texport let pending = false;\n```\n\nEmpty:\n```javascript\n\texport let size: \"small\" | \"large\" = \"small\";\n\texport let unpadded_box = false;\n```\n\nShareButton:\n```javascript\n\texport let formatter: (arg0: any) => Promise\u003Cstring>;\n\texport let value: any;\n\texport let i18n: I18nFormatter;\n```\n\nUploadText:\n```javascript\n\texport let type: \"video\" | \"image\" | \"audio\" | \"file\" | \"csv\" = \"file\";\n\texport let i18n: I18nFormatter;\n```",video:"# `@gradio/video`\n [v0.8.9](https://www.npmjs.com/package/@gradio/video)\n\n```javascript\n\u003Cscript>\n\timport { BaseInteractiveVideo, BaseStaticVideo, BasePlayer } from \"@gradio/button\";\n\timport type { FileData } from \"@gradio/upload\";\n\timport type { Gradio } from \"@gradio/utils\";\n\texport let _video: FileData;\n\u003C/script>\n\n\u003CStaticVideo\n\tvalue={_video}\n\t{label}\n\t{show_label}\n\t{autoplay}\n\t{show_share_button}\n\ti18n={gradio.i18n}\n/>\n\n\u003CVideo\n\tvalue={_video}\n\t{label}\n\t{show_label}\n\tsource={\"upload\"}\n\t{mirror_webcam}\n\t{include_audio}\n\t{autoplay}\n\ti18n={gradio.i18n}\n>\n\t\u003Cp>Upload Video Here\u003C/p>\n\u003C/Video>\n\n\u003CBasePlayer\n\tsrc={value.data}\n\t{autoplay}\n\ton:play\n\ton:pause\n\ton:stop\n\ton:end\n\tmirror={false}\n\t{label}\n/>\n```\n",form:"# `@gradio/form`\n [v0.1.18](https://www.npmjs.com/package/@gradio/form)\n\n```html\n\u003Cscript>\n\timport { Form } from \"@gradio/form\";\n\u003C/script>\n```\n\nForm\n```javascript\n\texport let visible = true;\n\texport let scale: number | null = null;\n\texport let min_width = 0;\n```\n",highlightedtext:"# `@gradio/highlightedtext`\n [v0.7.0](https://www.npmjs.com/package/@gradio/highlightedtext)\n\n```html\n\u003Cscript>\n    import { BaseStaticHighlightedText, BaseInteractiveHighlightedText } from `@gradio/highlightedtext`;\n\u003C/script>\n```\n\n\nBaseStaticHighlightedText\n```javascript\n\texport let value: {\n\t\ttoken: string;\n\t\tclass_or_confidence: string | number | null;\n\t}[] = [];\n\texport let show_legend = false;\n\texport let show_inline_category = true;\n\texport let color_map: Record\u003Cstring, string> = {};\n\texport let selectable = false;\n```\n\nBaseInteractiveHighlightedText\n```javascript\n\texport let value: {\n\t\ttoken: string;\n\t\tclass_or_confidence: string | number | null;\n\t}[] = [];\n\texport let show_legend = false;\n\texport let color_map: Record\u003Cstring, string> = {};\n\texport let selectable = false;\n```\n",model3D:"# `gradio/model3d`\n\n```html\n\u003Cscript>\n    import {BaseModel3D, BaseModel3DUpload, BaseExample } from `@gradio/model3d`;\n\u003C/script>\n```\n\nBaseModel3D\n```javascript\n\texport let value: FileData | null;\n\texport let clear_color: [number, number, number, number] = [0, 0, 0, 0];\n\texport let label = \"\";\n\texport let show_label: boolean;\n\texport let i18n: I18nFormatter;\n\texport let zoom_speed = 1;\n\n\t// alpha, beta, radius\n\texport let camera_position: [number | null, number | null, number | null] = [\n\t\tnull,\n\t\tnull,\n\t\tnull\n\t];\n```\n\nBaseModel3DUpload\n```javascript\n\texport let value: null | FileData;\n\texport let clear_color: [number, number, number, number] = [0, 0, 0, 0];\n\texport let label = \"\";\n\texport let show_label: boolean;\n\texport let root: string;\n\texport let i18n: I18nFormatter;\n\texport let zoom_speed = 1;\n\n\t// alpha, beta, radius\n\texport let camera_position: [number | null, number | null, number | null] = [\n\t\tnull,\n\t\tnull,\n\t\tnull\n\t];\n```\n\nBaseExample\n```javascript\n\texport let value: string;\n\texport let type: \"gallery\" | \"table\";\n\texport let selected = false;\n```",uploadbutton:"# `@gradio/uploadbutton`\n [v0.6.10](https://www.npmjs.com/package/@gradio/uploadbutton)\n\n```html\n\u003Cscript>\n    import { BaseUploadButton } from \"@gradio/uploadbutton\";\n\u003C/script>\n```\n\nBaseUploadButton\n```javascript\n\texport let elem_id = \"\";\n\texport let elem_classes: string[] = [];\n\texport let visible = true;\n\texport let label: string;\n\texport let value: null | FileData | FileData[];\n\texport let file_count: string;\n\texport let file_types: string[] = [];\n\texport let root: string;\n\texport let size: \"sm\" | \"lg\" = \"lg\";\n\texport let scale: number | null = null;\n\texport let min_width: number | undefined = undefined;\n\texport let variant: \"primary\" | \"secondary\" | \"stop\" = \"secondary\";\n\texport let disabled = false;\n```","js-client":"## JavaScript Client Library\n\nInteract with Gradio APIs using our JavaScript (and TypeScript) client.\n\n\n## Installation\n\nThe Gradio JavaScript Client is available on npm as `@gradio/client`. You can install it as below:\n\n```shell\nnpm i @gradio/client\n```\n\nOr, you can include it directly in your HTML via the jsDelivr CDN:\n\n```shell\n\u003Cscript src=\"https://cdn.jsdelivr.net/npm/@gradio/client/dist/index.min.js\">\u003C/script>\n```\n\n## Usage\n\nThe JavaScript Gradio Client exposes the Client class, `Client`, along with various other utility functions. `Client` is used to initialise and establish a connection to, or duplicate, a Gradio app. \n\n### `Client`\n\nThe Client function connects to the API of a hosted Gradio space and returns an object that allows you to make calls to that API.\n\nThe simplest example looks like this:\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\");\nconst result = await app.predict(\"/predict\");\n```\n\nThis function accepts two arguments: `source` and `options`:\n\n#### `source`\n\nThis is the url or name of the gradio app whose API you wish to connect to. This parameter is required and should always be a string. For example:\n\n```ts\nClient.connect(\"user/space-name\");  \n```\n\n#### `options`\n\nThe options object can optionally be passed a second parameter. This object has two properties, `hf_token` and `status_callback`.\n\n##### `hf_token`\n\nThis should be a Hugging Face personal access token and is required if you wish to make calls to a private gradio api. This option is optional and should be a string starting with `\"hf_\"`.\n\nExample:\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\", { hf_token: \"hf_...\" });\n```\n\n##### `status_callback`\n\nThis should be a function which will notify you of the status of a space if it is not running. If the gradio API you are connecting to is not awake and running or is not hosted on Hugging Face space then this function will do nothing.\n\n**Additional context**\n\nApplications hosted on Hugging Face spaces can be in a number of different states. As spaces are a GitOps tool and will rebuild when new changes are pushed to the repository, they have various building, running and error states. If a space is not 'running' then the function passed as the `status_callback` will notify you of the current state of the space and the status of the space as it changes. Spaces that are building or sleeping can take longer than usual to respond, so you can use this information to give users feedback about the progress of their action.\n\n```ts\nimport { Client, type SpaceStatus } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\", {\n\t// The space_status parameter does not need to be manually annotated, this is just for illustration.\n\tspace_status: (space_status: SpaceStatus) => console.log(space_status)\n});\n```\n\n```ts\ninterface SpaceStatusNormal {\n\tstatus: \"sleeping\" | \"running\" | \"building\" | \"error\" | \"stopped\";\n\tdetail:\n\t\t| \"SLEEPING\"\n\t\t| \"RUNNING\"\n\t\t| \"RUNNING_BUILDING\"\n\t\t| \"BUILDING\"\n\t\t| \"NOT_FOUND\";\n\tload_status: \"pending\" | \"error\" | \"complete\" | \"generating\";\n\tmessage: string;\n}\n\ninterface SpaceStatusError {\n\tstatus: \"space_error\";\n\tdetail: \"NO_APP_FILE\" | \"CONFIG_ERROR\" | \"BUILD_ERROR\" | \"RUNTIME_ERROR\";\n\tload_status: \"error\";\n\tmessage: string;\n\tdiscussions_enabled: boolean;\n\ntype SpaceStatus = SpaceStatusNormal | SpaceStatusError;\n```\n\nThe gradio client returns an object with a number of methods and properties:\n\n#### `predict`\n\nThe `predict` method allows you to call an api endpoint and get a prediction result:\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\");\nconst result = await app.predict(\"/predict\");\n```\n\n`predict` accepts two parameters, `endpoint` and `payload`. It returns a promise that resolves to the prediction result.\n\n##### `endpoint`\n\nThis is the endpoint for an api request and is required. The default endpoint for a `gradio.Interface` is `\"/predict\"`. Explicitly named endpoints have a custom name. The endpoint names can be found on the \"View API\" page of a space.\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\");\nconst result = await app.predict(\"/predict\");\n```\n\n##### `payload`\n\nThe `payload` argument is generally required but this depends on the API itself. If the API endpoint depends on values being passed in then the argument is required for the API request to succeed. The data that should be passed in is detailed on the \"View API\" page of a space, or accessible via the `view_api()` method of the client.\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\");\nconst result = await app.predict(\"/predict\", {\n\tinput: 1,\n\tword_1: \"Hello\",\n\tword_2: \"friends\"\n});\n```\n\n#### `submit`\n\nThe `submit` method provides a more flexible way to call an API endpoint, providing you with status updates about the current progress of the prediction as well as supporting more complex endpoint types.\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\");\nconst submission = app.submit(\"/predict\", { name: \"Chewbacca\" });\n```\n\nThe `submit` method accepts the same [`endpoint`](#endpoint) and [`payload`](#payload) arguments as `predict`.\n\nThe `submit` method does not return a promise and should not be awaited, instead it returns an async iterator with a  `cancel` method.\n\n##### Accessing values\n\nIterating the submission allows you to access the events related to the submitted API request. There are two types of events that can be listened for: `\"data\"` updates and `\"status\"` updates. By default only the `\"data\"` event is reported, but you can listen for the `\"status\"` event by manually passing the events you care about when instantiating the client:\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\", {\n\tevents: [\"data\", \"status\"]\n});\n```\n\n`\"data\"` updates are issued when the API computes a value, the callback provided as the second argument will be called when such a value is sent to the client. The shape of the data depends on the way the API itself is constructed. This event may fire more than once if that endpoint supports emmitting new values over time.\n\n`\"status` updates are issued when the status of a request changes. This information allows you to offer feedback to users when the queue position of the request changes, or when the request changes from queued to processing.\n\nThe status payload look like this:\n\n```ts\ninterface Status {\n\tqueue: boolean;\n\tcode?: string;\n\tsuccess?: boolean;\n\tstage: \"pending\" | \"error\" | \"complete\" | \"generating\";\n\tsize?: number;\n\tposition?: number;\n\teta?: number;\n\tmessage?: string;\n\tprogress_data?: Array\u003C{\n\t\tprogress: number | null;\n\t\tindex: number | null;\n\t\tlength: number | null;\n\t\tunit: string | null;\n\t\tdesc: string | null;\n\t}>;\n\ttime?: Date;\n}\n```\n\nUsage looks like this:\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\");\nconst submission = app\n\t.submit(\"/predict\", { name: \"Chewbacca\" })\n\n\tfor await (const msg of submission) {\n\t\tif (msg.type === \"data\") {\n\t\t\tconsole.log(msg.data);\n\t\t}\n\n\t\tif (msg.type === \"status\") {\n\t\t\tconsole.log(msg);\n\t\t}\n\t}\n```\n\n\n##### `cancel`\n\nCertain types of gradio function can run repeatedly and in some cases indefinitely. the `cancel` method will stop such an endpoints and prevent the API from issuing additional updates.\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\");\nconst submission = app\n\t.submit(\"/predict\", { name: \"Chewbacca\" })\n\n\n// later\n\nsubmission.cancel();\n```\n\n#### `view_api`\n\nThe `view_api` method provides details about the API you are connected to. It returns a JavaScript object of all named endpoints, unnamed endpoints and what values they accept and return. This method does not accept arguments.\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\");\nconst api_info = await app.view_api();\n\nconsole.log(api_info);\n```\n\n#### `config`\n\nThe `config` property contains the configuration for the gradio application you are connected to. This object may contain useful meta information about the application.\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\");\nconsole.log(app.config);\n```\n\n### `duplicate`\n\nThe duplicate function will attempt to duplicate the space that is referenced and return an instance of `client` connected to that space. If the space has already been duplicated then it will not create a new duplicate and will instead connect to the existing duplicated space. The huggingface token that is passed in will dictate the user under which the space is created.\n\n`duplicate` accepts the same arguments as `client` with the addition of a `private` options property dictating whether the duplicated space should be private or public. A huggingface token is required for duplication to work.\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.duplicate(\"user/space-name\", {\n\thf_token: \"hf_...\"\n});\n```\n\nThis function accepts two arguments: `source` and `options`:\n\n#### `source`\n\nThe space to duplicate and connect to. [See `client`'s `source` parameter](#source).\n\n#### `options`\n\nAccepts all options that `client` accepts, except `hf_token` is required. [See `client`'s `options` parameter](#source).\n\n`duplicate` also accepts one additional `options` property.\n\n##### `private`\n\nThis is an optional property specific to `duplicate`'s options object and will determine whether the space should be public or private. Spaces duplicated via the `duplicate` method are public by default.\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.duplicate(\"user/space-name\", {\n\thf_token: \"hf_...\",\n\tprivate: true\n});\n```\n\n##### `timeout`\n\nThis is an optional property specific to `duplicate`'s options object and will set the timeout in minutes before the duplicated space will go to sleep.\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.duplicate(\"user/space-name\", {\n\thf_token: \"hf_...\",\n\tprivate: true,\n\ttimeout: 5\n});\n```\n\n##### `hardware`\n\nThis is an optional property specific to `duplicate`'s options object and will set the hardware for the duplicated space. By default the hardware used will match that of the original space. If this cannot be obtained it will default to `\"cpu-basic\"`. For hardware upgrades (beyond the basic CPU tier), you may be required to provide [billing information on Hugging Face](https://huggingface.co/settings/billing).\n\nPossible hardware options are:\n\n- `\"cpu-basic\"`\n- `\"cpu-upgrade\"`\n- `\"cpu-xl\"`\n- `\"t4-small\"`\n- `\"t4-medium\"`\n- `\"a10g-small\"`\n- `\"a10g-large\"`\n- `\"a10g-largex2\"`\n- `\"a10g-largex4\"`\n- `\"a100-large\"`\n- `\"zero-a10g\"`\n- `\"h100\"`\n- `\"h100x8\"`\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.duplicate(\"user/space-name\", {\n\thf_token: \"hf_...\",\n\tprivate: true,\n\thardware: \"a10g-small\"\n});\n```\n"},js_pages:["atoms","audio","button","chatbot","checkbox","code","colorpicker","dataframe","dropdown","file","form","gallery","highlightedtext","html","image","imageeditor","js-client","json","label","lite","markdown","model3D","multimodaltextbox","radio","statustracker","textbox","tooltip","upload","uploadbutton","utils","video"],on_main:false,wheel:{gradio_install:"pip install https://gradio-builds.s3.amazonaws.com/d35c290aadcb85113ee7ceea96a7ed7dc894b1d2/gradio-4.36.1-py3-none-any.whl",gradio_py_client_install:"pip install 'gradio-client @ git+https://github.com/gradio-app/gradio@d35c290aadcb85113ee7ceea96a7ed7dc894b1d2#subdirectory=client/python'",gradio_js_client_install:"npm install https://gradio-builds.s3.amazonaws.com/d35c290aadcb85113ee7ceea96a7ed7dc894b1d2/gradio-client-1.1.1.tgz"},pages:{gradio:[{category:"Building Demos",pages:[{name:"interface",pretty_name:"Interface",path:"gradio/01_building-demos/01_interface.svx",page_index:1,abolute_index:0},{name:"chatinterface",pretty_name:"Chatinterface",path:"gradio/01_building-demos/02_chatinterface.svx",page_index:2,abolute_index:0},{name:"tabbedinterface",pretty_name:"Tabbedinterface",path:"gradio/01_building-demos/03_tabbedinterface.svx",page_index:3,abolute_index:0},{name:"blocks",pretty_name:"Blocks",path:"gradio/01_building-demos/04_blocks.svx",page_index:4,abolute_index:0}]},{category:"Blocks Layout",pages:[{name:"render",pretty_name:"Render",path:"gradio/02_blocks-layout/01_render.svx",page_index:1,abolute_index:0},{name:"accordion",pretty_name:"Accordion",path:"gradio/02_blocks-layout/accordion.svx",page_index:null,abolute_index:0},{name:"column",pretty_name:"Column",path:"gradio/02_blocks-layout/column.svx",page_index:null,abolute_index:0},{name:"group",pretty_name:"Group",path:"gradio/02_blocks-layout/group.svx",page_index:null,abolute_index:0},{name:"row",pretty_name:"Row",path:"gradio/02_blocks-layout/row.svx",page_index:null,abolute_index:0},{name:"tab",pretty_name:"Tab",path:"gradio/02_blocks-layout/tab.svx",page_index:null,abolute_index:0}]},{category:"Components",pages:[{name:"introduction",pretty_name:"Introduction",path:"gradio/03_components/01_introduction.svx",page_index:1,abolute_index:0},{name:"annotatedimage",pretty_name:"Annotatedimage",path:"gradio/03_components/annotatedimage.svx",page_index:null,abolute_index:0},{name:"audio",pretty_name:"Audio",path:"gradio/03_components/audio.svx",page_index:null,abolute_index:0},{name:"barplot",pretty_name:"Barplot",path:"gradio/03_components/barplot.svx",page_index:null,abolute_index:0},{name:"button",pretty_name:"Button",path:"gradio/03_components/button.svx",page_index:null,abolute_index:0},{name:"chatbot",pretty_name:"Chatbot",path:"gradio/03_components/chatbot.svx",page_index:null,abolute_index:0},{name:"checkbox",pretty_name:"Checkbox",path:"gradio/03_components/checkbox.svx",page_index:null,abolute_index:0},{name:"checkboxgroup",pretty_name:"Checkboxgroup",path:"gradio/03_components/checkboxgroup.svx",page_index:null,abolute_index:0},{name:"clearbutton",pretty_name:"Clearbutton",path:"gradio/03_components/clearbutton.svx",page_index:null,abolute_index:0},{name:"code",pretty_name:"Code",path:"gradio/03_components/code.svx",page_index:null,abolute_index:0},{name:"colorpicker",pretty_name:"Colorpicker",path:"gradio/03_components/colorpicker.svx",page_index:null,abolute_index:0},{name:"dataframe",pretty_name:"Dataframe",path:"gradio/03_components/dataframe.svx",page_index:null,abolute_index:0},{name:"dataset",pretty_name:"Dataset",path:"gradio/03_components/dataset.svx",page_index:null,abolute_index:0},{name:"downloadbutton",pretty_name:"Downloadbutton",path:"gradio/03_components/downloadbutton.svx",page_index:null,abolute_index:0},{name:"dropdown",pretty_name:"Dropdown",path:"gradio/03_components/dropdown.svx",page_index:null,abolute_index:0},{name:"duplicatebutton",pretty_name:"Duplicatebutton",path:"gradio/03_components/duplicatebutton.svx",page_index:null,abolute_index:0},{name:"file",pretty_name:"File",path:"gradio/03_components/file.svx",page_index:null,abolute_index:0},{name:"fileexplorer",pretty_name:"Fileexplorer",path:"gradio/03_components/fileexplorer.svx",page_index:null,abolute_index:0},{name:"gallery",pretty_name:"Gallery",path:"gradio/03_components/gallery.svx",page_index:null,abolute_index:0},{name:"highlightedtext",pretty_name:"Highlightedtext",path:"gradio/03_components/highlightedtext.svx",page_index:null,abolute_index:0},{name:"html",pretty_name:"Html",path:"gradio/03_components/html.svx",page_index:null,abolute_index:0},{name:"image",pretty_name:"Image",path:"gradio/03_components/image.svx",page_index:null,abolute_index:0},{name:"imageeditor",pretty_name:"Imageeditor",path:"gradio/03_components/imageeditor.svx",page_index:null,abolute_index:0},{name:"json",pretty_name:"Json",path:"gradio/03_components/json.svx",page_index:null,abolute_index:0},{name:"label",pretty_name:"Label",path:"gradio/03_components/label.svx",page_index:null,abolute_index:0},{name:"lineplot",pretty_name:"Lineplot",path:"gradio/03_components/lineplot.svx",page_index:null,abolute_index:0},{name:"loginbutton",pretty_name:"Loginbutton",path:"gradio/03_components/loginbutton.svx",page_index:null,abolute_index:0},{name:"logoutbutton",pretty_name:"Logoutbutton",path:"gradio/03_components/logoutbutton.svx",page_index:null,abolute_index:0},{name:"markdown",pretty_name:"Markdown",path:"gradio/03_components/markdown.svx",page_index:null,abolute_index:0},{name:"model3d",pretty_name:"Model3d",path:"gradio/03_components/model3d.svx",page_index:null,abolute_index:0},{name:"multimodaltextbox",pretty_name:"Multimodaltextbox",path:"gradio/03_components/multimodaltextbox.svx",page_index:null,abolute_index:0},{name:"number",pretty_name:"Number",path:"gradio/03_components/number.svx",page_index:null,abolute_index:0},{name:"paramviewer",pretty_name:"Paramviewer",path:"gradio/03_components/paramviewer.svx",page_index:null,abolute_index:0},{name:"plot",pretty_name:"Plot",path:"gradio/03_components/plot.svx",page_index:null,abolute_index:0},{name:"radio",pretty_name:"Radio",path:"gradio/03_components/radio.svx",page_index:null,abolute_index:0},{name:"scatterplot",pretty_name:"Scatterplot",path:"gradio/03_components/scatterplot.svx",page_index:null,abolute_index:0},{name:"simpleimage",pretty_name:"Simpleimage",path:"gradio/03_components/simpleimage.svx",page_index:null,abolute_index:0},{name:"slider",pretty_name:"Slider",path:"gradio/03_components/slider.svx",page_index:null,abolute_index:0},{name:"state",pretty_name:"State",path:"gradio/03_components/state.svx",page_index:null,abolute_index:0},{name:"textbox",pretty_name:"Textbox",path:"gradio/03_components/textbox.svx",page_index:null,abolute_index:0},{name:"uploadbutton",pretty_name:"Uploadbutton",path:"gradio/03_components/uploadbutton.svx",page_index:null,abolute_index:0},{name:"video",pretty_name:"Video",path:"gradio/03_components/video.svx",page_index:null,abolute_index:0}]},{category:"Helpers",pages:[{name:"eventdata",pretty_name:"Eventdata",path:"gradio/04_helpers/eventdata.svx",page_index:null,abolute_index:0},{name:"examples",pretty_name:"Examples",path:"gradio/04_helpers/examples.svx",page_index:null,abolute_index:0},{name:"load",pretty_name:"Load",path:"gradio/04_helpers/load.svx",page_index:null,abolute_index:0},{name:"make_waveform",pretty_name:"Make_waveform",path:"gradio/04_helpers/make_waveform.svx",page_index:null,abolute_index:0},{name:"progress",pretty_name:"Progress",path:"gradio/04_helpers/progress.svx",page_index:null,abolute_index:0},{name:"set_static_paths",pretty_name:"Set_static_paths",path:"gradio/04_helpers/set_static_paths.svx",page_index:null,abolute_index:0}]},{category:"Modals",pages:[{name:"error",pretty_name:"Error",path:"gradio/05_modals/error.svx",page_index:null,abolute_index:0},{name:"info",pretty_name:"Info",path:"gradio/05_modals/info.svx",page_index:null,abolute_index:0},{name:"warning",pretty_name:"Warning",path:"gradio/05_modals/warning.svx",page_index:null,abolute_index:0}]},{category:"Routes",pages:[{name:"mount_gradio_app",pretty_name:"Mount_gradio_app",path:"gradio/06_routes/mount_gradio_app.svx",page_index:null,abolute_index:0},{name:"request",pretty_name:"Request",path:"gradio/06_routes/request.svx",page_index:null,abolute_index:0}]},{category:"Other",pages:[{name:"flagging",pretty_name:"Flagging",path:"gradio/other/01_flagging.svx",page_index:1,abolute_index:0},{name:"themes",pretty_name:"Themes",path:"gradio/other/02_themes.svx",page_index:2,abolute_index:0},{name:"NO_RELOAD",pretty_name:"NO_RELOAD",path:"gradio/other/NO_RELOAD.svx",page_index:null,abolute_index:0}]}],"python-client":[{category:"Gradio_client",pages:[{name:"introduction",pretty_name:"Introduction",path:"python-client/gradio_client/01_introduction.svx",page_index:1,abolute_index:0},{name:"version-1-release",pretty_name:"Version 1 Release",path:"python-client/gradio_client/02_version-1-release.svx",page_index:2,abolute_index:0},{name:"client",pretty_name:"Client",path:"python-client/gradio_client/client.svx",page_index:null,abolute_index:0},{name:"job",pretty_name:"Job",path:"python-client/gradio_client/job.svx",page_index:null,abolute_index:0}]}]},js_client:"## JavaScript Client Library\n\nInteract with Gradio APIs using our JavaScript (and TypeScript) client.\n\n\n## Installation\n\nThe Gradio JavaScript Client is available on npm as `@gradio/client`. You can install it as below:\n\n```shell\nnpm i @gradio/client\n```\n\nOr, you can include it directly in your HTML via the jsDelivr CDN:\n\n```shell\n\u003Cscript src=\"https://cdn.jsdelivr.net/npm/@gradio/client/dist/index.min.js\">\u003C/script>\n```\n\n## Usage\n\nThe JavaScript Gradio Client exposes the Client class, `Client`, along with various other utility functions. `Client` is used to initialise and establish a connection to, or duplicate, a Gradio app. \n\n### `Client`\n\nThe Client function connects to the API of a hosted Gradio space and returns an object that allows you to make calls to that API.\n\nThe simplest example looks like this:\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\");\nconst result = await app.predict(\"/predict\");\n```\n\nThis function accepts two arguments: `source` and `options`:\n\n#### `source`\n\nThis is the url or name of the gradio app whose API you wish to connect to. This parameter is required and should always be a string. For example:\n\n```ts\nClient.connect(\"user/space-name\");  \n```\n\n#### `options`\n\nThe options object can optionally be passed a second parameter. This object has two properties, `hf_token` and `status_callback`.\n\n##### `hf_token`\n\nThis should be a Hugging Face personal access token and is required if you wish to make calls to a private gradio api. This option is optional and should be a string starting with `\"hf_\"`.\n\nExample:\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\", { hf_token: \"hf_...\" });\n```\n\n##### `status_callback`\n\nThis should be a function which will notify you of the status of a space if it is not running. If the gradio API you are connecting to is not awake and running or is not hosted on Hugging Face space then this function will do nothing.\n\n**Additional context**\n\nApplications hosted on Hugging Face spaces can be in a number of different states. As spaces are a GitOps tool and will rebuild when new changes are pushed to the repository, they have various building, running and error states. If a space is not 'running' then the function passed as the `status_callback` will notify you of the current state of the space and the status of the space as it changes. Spaces that are building or sleeping can take longer than usual to respond, so you can use this information to give users feedback about the progress of their action.\n\n```ts\nimport { Client, type SpaceStatus } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\", {\n\t// The space_status parameter does not need to be manually annotated, this is just for illustration.\n\tspace_status: (space_status: SpaceStatus) => console.log(space_status)\n});\n```\n\n```ts\ninterface SpaceStatusNormal {\n\tstatus: \"sleeping\" | \"running\" | \"building\" | \"error\" | \"stopped\";\n\tdetail:\n\t\t| \"SLEEPING\"\n\t\t| \"RUNNING\"\n\t\t| \"RUNNING_BUILDING\"\n\t\t| \"BUILDING\"\n\t\t| \"NOT_FOUND\";\n\tload_status: \"pending\" | \"error\" | \"complete\" | \"generating\";\n\tmessage: string;\n}\n\ninterface SpaceStatusError {\n\tstatus: \"space_error\";\n\tdetail: \"NO_APP_FILE\" | \"CONFIG_ERROR\" | \"BUILD_ERROR\" | \"RUNTIME_ERROR\";\n\tload_status: \"error\";\n\tmessage: string;\n\tdiscussions_enabled: boolean;\n\ntype SpaceStatus = SpaceStatusNormal | SpaceStatusError;\n```\n\nThe gradio client returns an object with a number of methods and properties:\n\n#### `predict`\n\nThe `predict` method allows you to call an api endpoint and get a prediction result:\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\");\nconst result = await app.predict(\"/predict\");\n```\n\n`predict` accepts two parameters, `endpoint` and `payload`. It returns a promise that resolves to the prediction result.\n\n##### `endpoint`\n\nThis is the endpoint for an api request and is required. The default endpoint for a `gradio.Interface` is `\"/predict\"`. Explicitly named endpoints have a custom name. The endpoint names can be found on the \"View API\" page of a space.\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\");\nconst result = await app.predict(\"/predict\");\n```\n\n##### `payload`\n\nThe `payload` argument is generally required but this depends on the API itself. If the API endpoint depends on values being passed in then the argument is required for the API request to succeed. The data that should be passed in is detailed on the \"View API\" page of a space, or accessible via the `view_api()` method of the client.\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\");\nconst result = await app.predict(\"/predict\", {\n\tinput: 1,\n\tword_1: \"Hello\",\n\tword_2: \"friends\"\n});\n```\n\n#### `submit`\n\nThe `submit` method provides a more flexible way to call an API endpoint, providing you with status updates about the current progress of the prediction as well as supporting more complex endpoint types.\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\");\nconst submission = app.submit(\"/predict\", { name: \"Chewbacca\" });\n```\n\nThe `submit` method accepts the same [`endpoint`](#endpoint) and [`payload`](#payload) arguments as `predict`.\n\nThe `submit` method does not return a promise and should not be awaited, instead it returns an async iterator with a  `cancel` method.\n\n##### Accessing values\n\nIterating the submission allows you to access the events related to the submitted API request. There are two types of events that can be listened for: `\"data\"` updates and `\"status\"` updates. By default only the `\"data\"` event is reported, but you can listen for the `\"status\"` event by manually passing the events you care about when instantiating the client:\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\", {\n\tevents: [\"data\", \"status\"]\n});\n```\n\n`\"data\"` updates are issued when the API computes a value, the callback provided as the second argument will be called when such a value is sent to the client. The shape of the data depends on the way the API itself is constructed. This event may fire more than once if that endpoint supports emmitting new values over time.\n\n`\"status` updates are issued when the status of a request changes. This information allows you to offer feedback to users when the queue position of the request changes, or when the request changes from queued to processing.\n\nThe status payload look like this:\n\n```ts\ninterface Status {\n\tqueue: boolean;\n\tcode?: string;\n\tsuccess?: boolean;\n\tstage: \"pending\" | \"error\" | \"complete\" | \"generating\";\n\tsize?: number;\n\tposition?: number;\n\teta?: number;\n\tmessage?: string;\n\tprogress_data?: Array\u003C{\n\t\tprogress: number | null;\n\t\tindex: number | null;\n\t\tlength: number | null;\n\t\tunit: string | null;\n\t\tdesc: string | null;\n\t}>;\n\ttime?: Date;\n}\n```\n\nUsage looks like this:\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\");\nconst submission = app\n\t.submit(\"/predict\", { name: \"Chewbacca\" })\n\n\tfor await (const msg of submission) {\n\t\tif (msg.type === \"data\") {\n\t\t\tconsole.log(msg.data);\n\t\t}\n\n\t\tif (msg.type === \"status\") {\n\t\t\tconsole.log(msg);\n\t\t}\n\t}\n```\n\n\n##### `cancel`\n\nCertain types of gradio function can run repeatedly and in some cases indefinitely. the `cancel` method will stop such an endpoints and prevent the API from issuing additional updates.\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\");\nconst submission = app\n\t.submit(\"/predict\", { name: \"Chewbacca\" })\n\n\n// later\n\nsubmission.cancel();\n```\n\n#### `view_api`\n\nThe `view_api` method provides details about the API you are connected to. It returns a JavaScript object of all named endpoints, unnamed endpoints and what values they accept and return. This method does not accept arguments.\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\");\nconst api_info = await app.view_api();\n\nconsole.log(api_info);\n```\n\n#### `config`\n\nThe `config` property contains the configuration for the gradio application you are connected to. This object may contain useful meta information about the application.\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"user/space-name\");\nconsole.log(app.config);\n```\n\n### `duplicate`\n\nThe duplicate function will attempt to duplicate the space that is referenced and return an instance of `client` connected to that space. If the space has already been duplicated then it will not create a new duplicate and will instead connect to the existing duplicated space. The huggingface token that is passed in will dictate the user under which the space is created.\n\n`duplicate` accepts the same arguments as `client` with the addition of a `private` options property dictating whether the duplicated space should be private or public. A huggingface token is required for duplication to work.\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.duplicate(\"user/space-name\", {\n\thf_token: \"hf_...\"\n});\n```\n\nThis function accepts two arguments: `source` and `options`:\n\n#### `source`\n\nThe space to duplicate and connect to. [See `client`'s `source` parameter](#source).\n\n#### `options`\n\nAccepts all options that `client` accepts, except `hf_token` is required. [See `client`'s `options` parameter](#source).\n\n`duplicate` also accepts one additional `options` property.\n\n##### `private`\n\nThis is an optional property specific to `duplicate`'s options object and will determine whether the space should be public or private. Spaces duplicated via the `duplicate` method are public by default.\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.duplicate(\"user/space-name\", {\n\thf_token: \"hf_...\",\n\tprivate: true\n});\n```\n\n##### `timeout`\n\nThis is an optional property specific to `duplicate`'s options object and will set the timeout in minutes before the duplicated space will go to sleep.\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.duplicate(\"user/space-name\", {\n\thf_token: \"hf_...\",\n\tprivate: true,\n\ttimeout: 5\n});\n```\n\n##### `hardware`\n\nThis is an optional property specific to `duplicate`'s options object and will set the hardware for the duplicated space. By default the hardware used will match that of the original space. If this cannot be obtained it will default to `\"cpu-basic\"`. For hardware upgrades (beyond the basic CPU tier), you may be required to provide [billing information on Hugging Face](https://huggingface.co/settings/billing).\n\nPossible hardware options are:\n\n- `\"cpu-basic\"`\n- `\"cpu-upgrade\"`\n- `\"cpu-xl\"`\n- `\"t4-small\"`\n- `\"t4-medium\"`\n- `\"a10g-small\"`\n- `\"a10g-large\"`\n- `\"a10g-largex2\"`\n- `\"a10g-largex4\"`\n- `\"a100-large\"`\n- `\"zero-a10g\"`\n- `\"h100\"`\n- `\"h100x8\"`\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.duplicate(\"user/space-name\", {\n\thf_token: \"hf_...\",\n\tprivate: true,\n\thardware: \"a10g-small\"\n});\n```\n",url_version:"4.36.1",VERSION:"4.36.1"},"uses":{"params":["version"]}},{"type":"data","data":{guides_by_category:[{category:"Getting Started",guides:[{name:"quickstart",category:"getting-started",pretty_category:"Getting Started",guide_index:1,absolute_index:0,pretty_name:"Quickstart",content:"# Quickstart\n\nGradio is an open-source Python package that allows you to quickly **build** a demo or web application for your machine learning model, API, or any arbitary Python function. You can then **share** a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. *No JavaScript, CSS, or web hosting experience needed!*\n\n\u003Cimg src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/lcm-screenshot-3.gif\" style=\"padding-bottom: 10px\">\n\nIt just takes a few lines of Python to create a demo like the one above, so let's get started üí´\n\n## Installation\n\n**Prerequisite**: Gradio requires [Python 3.8 or higher](https://www.python.org/downloads/)\n\n\nWe recommend installing Gradio using `pip`, which is included by default in Python. Run this in your terminal or command prompt:\n\n```bash\npip install gradio\n```\n\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> it is best to install Gradio in a virtual environment. Detailed installation instructions for all common operating systems \u003Ca href=\"https://www.gradio.app/main/guides/installing-gradio-in-a-virtual-environment\">are provided here\u003C/a>. \u003C/p>\n\n## Building Your First Demo\n\nYou can run Gradio in your favorite code editor, Jupyter notebook, Google Colab, or anywhere else you write Python. Let's write your first Gradio app:\n\n\n```python\nimport gradio as gr\n\ndef greet(name, intensity):\n    return \"Hello, \" + name + \"!\" * int(intensity)\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=[\"text\", \"slider\"],\n    outputs=[\"text\"],\n)\n\ndemo.launch()\n\n```\n\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> We shorten the imported name from \u003Ccode>gradio\u003C/code> to \u003Ccode>gr\u003C/code> for better readability of code. This is a widely adopted convention that you should follow so that anyone working with your code can easily understand it. \u003C/p>\n\nNow, run your code. If you've written the Python code in a file named, for example, `app.py`, then you would run `python app.py` from the terminal.\n\nThe demo below will open in a browser on [http://localhost:7860](http://localhost:7860) if running from a file. If you are running within a notebook, the demo will appear embedded within the notebook.\n\n\u003Cgradio-app space='gradio/hello_world_4'>\u003C/gradio-app>\n\nType your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> When developing locally, you can run your Gradio app in \u003Cstrong>hot reload mode\u003C/strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in \u003Ccode>gradio\u003C/code> before the name of the file instead of \u003Ccode>python\u003C/code>. In the example above, you would type: `gradio app.py` in your terminal. Learn more about hot reloading in the \u003Ca href=\"https://www.gradio.app/guides/developing-faster-with-reload-mode\">Hot Reloading Guide\u003C/a>.\u003C/p>\n\n\n**Understanding the `Interface` Class**\n\nYou'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs. \n\nThe `Interface` class has three core arguments:\n\n- `fn`: the function to wrap a user interface (UI) around\n- `inputs`: the Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.\n- `outputs`: the Gradio component(s) to use for the output. The number of components should match the number of return values from your function.\n\nThe `fn` argument is very flexible -- you can pass *any* Python function that you want to wrap with a UI. In the example above, we saw a relatively simple function, but the function could be anything from a music generator to a tax calculator to the prediction function of a pretrained machine learning model.\n\nThe `inputs` and `outputs` arguments take one or more Gradio components. As we'll see, Gradio includes more than [30 built-in components](https://www.gradio.app/docs/gradio/components) (such as the `gr.Textbox()`, `gr.Image()`, and `gr.HTML()` components) that are designed for machine learning applications. \u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> For the `inputs` and `outputs` arguments, you can pass in the name of these components as a string (`\"textbox\"`) or an instance of the class (`gr.Textbox()`).\u003C/p>\n\nIf your function accepts more than one argument, as is the case above, pass a list of input components to `inputs`, with each input component corresponding to one of the arguments of the function, in order. The same holds true if your function returns more than one value: simply pass in a list of components to `outputs`. This flexibility makes the `Interface` class a very powerful way to create demos.\n\nWe'll dive deeper into the `gr.Interface` on our series on [building Interfaces](https://www.gradio.app/main/guides/the-interface-class).\n\n## Sharing Your Demo\n\nWhat good is a beautiful demo if you can't share it? Gradio lets you easily share a machine learning demo without having to worry about the hassle of hosting on a web server. Simply set `share=True` in `launch()`, and a publicly accessible URL will be created for your demo. Let's revisit our example demo,  but change the last line as follows:\n\n```python\nimport gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(fn=greet, inputs=\"textbox\", outputs=\"textbox\")\n    \ndemo.launch(share=True)  # Share your demo with just 1 extra parameter üöÄ\n```\n\nWhen you run this code, a public URL will be generated for your demo in a matter of seconds, something like:\n\nüëâ &nbsp; `https://a23dsf231adb.gradio.live`\n\nNow, anyone around the world can try your Gradio demo from their browser, while the machine learning model and all computation continues to run locally on your computer.\n\nTo learn more about sharing your demo, read our dedicated guide on [sharing your Gradio application](https://www.gradio.app/guides/sharing-your-app).\n\n\n## Core Gradio Classes\n\nSo far, we've been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio include?aaa\n\n### Chatbots with `gr.ChatInterface`\n\nGradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight to [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast).\n\n### Custom Demos with `gr.Blocks`\n\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction ‚Äî still all in Python. \n\nYou can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\n\n### The Gradio Python & JavaScript Ecosystem\n\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\n* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio_client`): query any Gradio app programmatically in Python.\n* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript.\n* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \n* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications ‚Äî for free!\n\n## What's Next?\n\nKeep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [let's dive deeper into the Interface class](https://www.gradio.app/guides/the-interface-class).\n\nOr, if you already know the basics and are looking for something specific, you can search the more [technical API documentation](https://www.gradio.app/docs/).\n\n\n",tags:[],spaces:[],url:"/guides/quickstart/",contributor:null},{name:"key-features",category:"getting-started",pretty_category:"Getting Started",guide_index:2,absolute_index:1,pretty_name:"Key Features",content:"# Key Features\n\nLet's go through some of the key features of Gradio. This guide is intended to be a high-level overview of various things that you should be aware of as you build your demo. Where appropriate, we link to more detailed guides on specific topics.\n\n1. [Components](#components)\n2. [Queuing](#queuing)\n3. [Streaming outputs](#streaming-outputs)\n4. [Streaming inputs](#streaming-inputs)\n5. [Alert modals](#alert-modals)\n6. [Styling](#styling)\n7. [Progress bars](#progress-bars)\n8. [Batch functions](#batch-functions)\n\n## Components\n\nGradio includes more than 30 pre-built components (as well as many user-built _custom components_) that can be used as inputs or outputs in your demo with a single line of code. These components correspond to common data types in machine learning and data science, e.g. the `gr.Image` component is designed to handle input or output images, the `gr.Label` component displays classification labels and probabilities, the `gr.Plot` component displays various kinds of plots, and so on.\n\nEach component includes various constructor attributes that control the properties of the component. For example, you can control the number of lines in a `gr.Textbox` using the `lines` argument (which takes a positive integer) in its constructor. Or you can control the way that a user can provide an image in the `gr.Image` component using the `sources` parameter (which takes a list like `[\"webcam\", \"upload\"]`).\n\n**Static and Interactive Components**\n\nEvery component has a _static_ version that is designed to *display* data, and most components also have an _interactive_ version designed to let users input or modify the data. Typically, you don't need to think about this distinction, because when you build a Gradio demo, Gradio automatically figures out whether the component should be static or interactive based on whether it is being used as an input or output. However, you can set this manually using the `interactive` argument that every component supports.\n\n**Preprocessing and Postprocessing**\n\nWhen a component is used as an input, Gradio automatically handles the _preprocessing_ needed to convert the data from a type sent by the user's browser (such as an uploaded image) to a form that can be accepted by your function (such as a `numpy` array).\n\n\nSimilarly, when a component is used as an output, Gradio automatically handles the _postprocessing_ needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user's browser (a gallery of images).\n\nConsider an example demo with three input components (`gr.Textbox`, `gr.Number`, and `gr.Image`) and two outputs (`gr.Number` and `gr.Gallery`) that serve as a UI for your image-to-image generation model. Below is a diagram of what our preprocessing will send to the model and what our postprocessing will require from it.\n\n![](https://github.com/gradio-app/gradio/blob/main/guides/assets/dataflow.svg?raw=true)\n\nIn this image, the following preprocessing steps happen to send the data from the browser to your function:\n\n* The text in the textbox is converted to a Python `str` (essentially no preprocessing)\n* The number in the number input in converted to a Python `float` (essentially no preprocessing)\n* Most importantly, ihe image supplied by the user is converted to a `numpy.array` representation of the RGB values in the image\n\nImages are converted to NumPy arrays because they are a common format for machine learning workflows. You can control the _preprocessing_ using the component's parameters when constructing the component. For example, if you instantiate the `Image` component with the following parameters, it will preprocess the image to the `PIL` format instead:\n\n```py\nimg = gr.Image(type=\"pil\")\n```\n\nPostprocessing is even simpler! Gradio automatically recognizes the format of the returned data (e.g. does the user's function return a `numpy` array or a `str` filepath for the `gr.Image` component?) and postprocesses it appropriately into a format that can be displayed by the browser.\n\nSo in the image above, the following postprocessing steps happen to send the data returned from a user's function to the browser:\n\n* The `float` is displayed as a number and displayed directly to the user\n* The list of string filepaths (`list[str]`) is interpreted as a list of image filepaths and displayed as a gallery in the browser\n\nTake a look at the [Docs](https://gradio.app/docs) to see all the parameters for each Gradio component.\n\n## Queuing\n\nEvery Gradio app comes with a built-in queuing system that can scale to thousands of concurrent users. You can configure the queue by using `queue()` method which is supported by the `gr.Interface`, `gr.Blocks`, and `gr.ChatInterface` classes. \n\nFor example, you can control the number of requests processed at a single time by setting the `default_concurrency_limit` parameter of `queue()`, e.g.\n\n```python\ndemo = gr.Interface(...).queue(default_concurrency_limit=5)\ndemo.launch()\n```\n\nThis limits the number of requests processed for this event listener at a single time to 5. By default, the `default_concurrency_limit` is actually set to `1`, which means that when many users are using your app, only a single user's request will be processed at a time. This is because many machine learning functions consume a significant amount of memory and so it is only suitable to have a single user using the demo at a time. However, you can change this parameter in your demo easily.\n\nSee the [docs on queueing](https://gradio.app/docs/gradio/interface#interface-queue) for more details on configuring the queuing parameters.\n\n## Streaming outputs\n\nIn some cases, you may want to stream a sequence of outputs rather than show a single output at once. For example, you might have an image generation model and you want to show the image that is generated at each step, leading up to the final image. Or you might have a chatbot which streams its response one token at a time instead of returning it all at once.\n\nIn such cases, you can supply a **generator** function into Gradio instead of a regular function. Creating generators in Python is very simple: instead of a single `return` value, a function should `yield` a series of values instead. Usually the `yield` statement is put in some kind of loop. Here's an example of an generator that simply counts up to a given number:\n\n```python\ndef my_generator(x):\n    for i in range(x):\n        yield i\n```\n\nYou supply a generator into Gradio the same way as you would a regular function. For example, here's a a (fake) image generation model that generates noise for several steps before outputting an image using the `gr.Interface` class:\n\n```python\nimport gradio as gr\nimport numpy as np\nimport time\n\ndef fake_diffusion(steps):\n    rng = np.random.default_rng()\n    for i in range(steps):\n        time.sleep(1)\n        image = rng.random(size=(600, 600, 3))\n        yield image\n    image = np.ones((1000,1000,3), np.uint8)\n    image[:] = [255, 124, 0]\n    yield image\n\n\ndemo = gr.Interface(fake_diffusion,\n                    inputs=gr.Slider(1, 10, 3, step=1),\n                    outputs=\"image\")\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/fake_diffusion'>\u003C/gradio-app>\n\nNote that we've added a `time.sleep(1)` in the iterator to create an artificial pause between steps so that you are able to observe the steps of the iterator (in a real image generation model, this probably wouldn't be necessary).\n\n## Streaming inputs\n\nSimilarly, Gradio can handle streaming inputs, e.g. a live audio stream that can gets transcribed to text in real time, or an image generation model that reruns every time a user types a letter in a textbox. This is covered in more details in our guide on building [reactive Interfaces](/guides/reactive-interfaces). \n\n## Alert modals\n\nYou may wish to raise alerts to the user. To do so, raise a `gr.Error(\"custom message\")` to display an error message. You can also issue `gr.Warning(\"message\")` and `gr.Info(\"message\")` by having them as standalone lines in your function, which will immediately display modals while continuing the execution of your function. Queueing needs to be enabled for this to work.\n\nNote below how the `gr.Error` has to be raised, while the `gr.Warning` and `gr.Info` are single lines.\n\n```python\ndef start_process(name):\n    gr.Info(\"Starting process\")\n    if name is None:\n        gr.Warning(\"Name is empty\")\n    ...\n    if success == False:\n        raise gr.Error(\"Process failed\")\n```\n\n\n\n## Styling\n\nGradio themes are the easiest way to customize the look and feel of your app. You can choose from a variety of themes, or create your own. To do so, pass the `theme=` kwarg to the `Interface` constructor. For example:\n\n```python\ndemo = gr.Interface(..., theme=gr.themes.Monochrome())\n```\n\nGradio comes with a set of prebuilt themes which you can load from `gr.themes.*`. You can extend these themes or create your own themes from scratch - see the [theming guide](https://gradio.app/guides/theming-guide) for more details.\n\nFor additional styling ability, you can pass any CSS (as well as custom JavaScript) to your Gradio application. This is discussed in more detail in our [custom JS and CSS guide](/guides/custom-CSS-and-JS).\n\n\n## Progress bars\n\nGradio supports the ability to create custom Progress Bars so that you have customizability and control over the progress update that you show to the user. In order to enable this, simply add an argument to your method that has a default value of a `gr.Progress` instance. Then you can update the progress levels by calling this instance directly with a float between 0 and 1, or using the `tqdm()` method of the `Progress` instance to track progress over an iterable, as shown below.\n\n```python\nimport gradio as gr\nimport time\n\ndef slowly_reverse(word, progress=gr.Progress()):\n    progress(0, desc=\"Starting\")\n    time.sleep(1)\n    progress(0.05)\n    new_string = \"\"\n    for letter in progress.tqdm(word, desc=\"Reversing\"):\n        time.sleep(0.25)\n        new_string = letter + new_string\n    return new_string\n\ndemo = gr.Interface(slowly_reverse, gr.Text(), gr.Text())\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/progress_simple'>\u003C/gradio-app>\n\nIf you use the `tqdm` library, you can even report progress updates automatically from any `tqdm.tqdm` that already exists within your function by setting the default argument as `gr.Progress(track_tqdm=True)`!\n\n## Batch functions\n\nGradio supports the ability to pass _batch_ functions. Batch functions are just\nfunctions which take in a list of inputs and return a list of predictions.\n\nFor example, here is a batched function that takes in two lists of inputs (a list of\nwords and a list of ints), and returns a list of trimmed words as output:\n\n```py\nimport time\n\ndef trim_words(words, lens):\n    trimmed_words = []\n    time.sleep(5)\n    for w, l in zip(words, lens):\n        trimmed_words.append(w[:int(l)])\n    return [trimmed_words]\n```\n\nThe advantage of using batched functions is that if you enable queuing, the Gradio server can automatically _batch_ incoming requests and process them in parallel,\npotentially speeding up your demo. Here's what the Gradio code looks like (notice the `batch=True` and `max_batch_size=16`)\n\nWith the `gr.Interface` class:\n\n```python\ndemo = gr.Interface(\n    fn=trim_words, \n    inputs=[\"textbox\", \"number\"], \n    outputs=[\"output\"],\n    batch=True, \n    max_batch_size=16\n)\n\ndemo.launch()\n```\n\nWith the `gr.Blocks` class:\n\n```py\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        word = gr.Textbox(label=\"word\")\n        leng = gr.Number(label=\"leng\")\n        output = gr.Textbox(label=\"Output\")\n    with gr.Row():\n        run = gr.Button()\n\n    event = run.click(trim_words, [word, leng], output, batch=True, max_batch_size=16)\n\ndemo.launch()\n```\n\nIn the example above, 16 requests could be processed in parallel (for a total inference time of 5 seconds), instead of each request being processed separately (for a total\ninference time of 80 seconds). Many Hugging Face `transformers` and `diffusers` models work very naturally with Gradio's batch mode: here's [an example demo using diffusers to\ngenerate images in batches](https://github.com/gradio-app/gradio/blob/main/demo/diffusers_with_batching/run.py)\n\n\n\n",tags:[],spaces:[],url:"/guides/key-features/",contributor:null}]},{category:"Building Interfaces",guides:[{name:"the-interface-class",category:"building-interfaces",pretty_category:"Building Interfaces",guide_index:0,absolute_index:2,pretty_name:"The Interface Class",content:"# The `Interface` class\n\nAs mentioned in the [Quickstart](/main/guides/quickstart), the `gr.Interface` class is a high-level abstraction in Gradio that allows you to quickly create a demo for any Python function simply by specifying the input types and the output types. Revisiting our first demo:\n\n```python\nimport gradio as gr\n\ndef greet(name, intensity):\n    return \"Hello, \" + name + \"!\" * int(intensity)\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=[\"text\", \"slider\"],\n    outputs=[\"text\"],\n)\n\ndemo.launch()\n\n```\n\n\nWe see that the `Interface` class is initialized with three required parameters:\n\n- `fn`: the function to wrap a user interface (UI) around\n- `inputs`: which Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.\n- `outputs`: which Gradio component(s) to use for the output. The number of components should match the number of return values from your function.\n\nIn this Guide, we'll dive into `gr.Interface` and the various ways it can be customized, but before we do that, let's get a better understanding of Gradio components.\n\n## Gradio Components\n\nGradio includes more than 30 pre-built components (as well as many [community-built _custom components_](https://www.gradio.app/custom-components/gallery)) that can be used as inputs or outputs in your demo. These components correspond to common data types in machine learning and data science, e.g. the `gr.Image` component is designed to handle input or output images, the `gr.Label` component displays classification labels and probabilities, the `gr.Plot` component displays various kinds of plots, and so on. \n\n**Static and Interactive Components**\n\nEvery component has a _static_ version that is designed to *display* data, and most components also have an _interactive_ version designed to let users input or modify the data. Typically, you don't need to think about this distinction, because when you build a Gradio demo, Gradio automatically figures out whether the component should be static or interactive based on whether it is being used as an input or output. However, you can set this manually using the `interactive` argument that every component supports.\n\n**Preprocessing and Postprocessing**\n\nWhen a component is used as an input, Gradio automatically handles the _preprocessing_ needed to convert the data from a type sent by the user's browser (such as an uploaded image) to a form that can be accepted by your function (such as a `numpy` array).\n\n\nSimilarly, when a component is used as an output, Gradio automatically handles the _postprocessing_ needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user's browser (a gallery of images).\n\n## Components Attributes\n\nWe used the default versions of the `gr.Textbox` and `gr.Slider`, but what if you want to change how the UI components look or behave?\n\nLet's say you want to customize the slider to have values from 1 to 10, with a default of 2. And you wanted to customize the output text field ‚Äî you want it to be larger and have a label.\n\nIf you use the actual classes for `gr.Textbox` and `gr.Slider` instead of the string shortcuts, you have access to much more customizability through component attributes.\n\n```python\nimport gradio as gr\n\ndef greet(name, intensity):\n    return \"Hello, \" + name + \"!\" * intensity\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=[\"text\", gr.Slider(value=2, minimum=1, maximum=10, step=1)],\n    outputs=[gr.Textbox(label=\"greeting\", lines=3)],\n)\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/hello_world_2'>\u003C/gradio-app>\n\n## Multiple Input and Output Components\n\nSuppose you had a more complex function, with multiple outputs as well. In the example below, we define a function that takes a string, boolean, and number, and returns a string and number. \n\n```python\nimport gradio as gr\n\ndef greet(name, is_morning, temperature):\n    salutation = \"Good morning\" if is_morning else \"Good evening\"\n    greeting = f\"{salutation} {name}. It is {temperature} degrees today\"\n    celsius = (temperature - 32) * 5 / 9\n    return greeting, round(celsius, 2)\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=[\"text\", \"checkbox\", gr.Slider(0, 100)],\n    outputs=[\"text\", \"number\"],\n)\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/hello_world_3'>\u003C/gradio-app>\n\nJust as each component in the `inputs` list corresponds to one of the parameters of the function, in order, each component in the `outputs` list corresponds to one of the values returned by the function, in order.\n\n## An Image Example\n\nGradio supports many types of components, such as `Image`, `DataFrame`, `Video`, or `Label`. Let's try an image-to-image function to get a feel for these!\n\n```python\nimport numpy as np\nimport gradio as gr\n\ndef sepia(input_img):\n    sepia_filter = np.array([\n        [0.393, 0.769, 0.189], \n        [0.349, 0.686, 0.168], \n        [0.272, 0.534, 0.131]\n    ])\n    sepia_img = input_img.dot(sepia_filter.T)\n    sepia_img /= sepia_img.max()\n    return sepia_img\n\ndemo = gr.Interface(sepia, gr.Image(), \"image\")\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/sepia_filter'>\u003C/gradio-app>\n\nWhen using the `Image` component as input, your function will receive a NumPy array with the shape `(height, width, 3)`, where the last dimension represents the RGB values. We'll return an image as well in the form of a NumPy array. \n\nAs mentioned above, Gradio handles the preprocessing and postprocessing to convert images to NumPy arrays and vice versa. You can also control the preprocessing performed with the `type=` keyword argument. For example, if you wanted your function to take a file path to an image instead of a NumPy array, the input `Image` component could be written as:\n\n```python\ngr.Image(type=\"filepath\", shape=...)\n```\n\nYou can read more about the built-in Gradio components and how to customize them in the [Gradio docs](https://gradio.app/docs).\n\n## Example Inputs\n\nYou can provide example data that a user can easily load into `Interface`. This can be helpful to demonstrate the types of inputs the model expects, as well as to provide a way to explore your dataset in conjunction with your model. To load example data, you can provide a **nested list** to the `examples=` keyword argument of the Interface constructor. Each sublist within the outer list represents a data sample, and each element within the sublist represents an input for each input component. The format of example data for each component is specified in the [Docs](https://gradio.app/docs#components).\n\n```python\nimport gradio as gr\n#from foo import BAR\n#\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        if num2 == 0:\n            raise gr.Error(\"Cannot divide by zero!\")\n        return num1 / num2\n\ndemo = gr.Interface(\n    calculator,\n    [\n        \"number\", \n        gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]),\n        \"number\"\n    ],\n    \"number\",\n    examples=[\n        [45, \"add\", 3],\n        [3.14, \"divide\", 2],\n        [144, \"multiply\", 2.5],\n        [0, \"subtract\", 1.2],\n    ],\n    title=\"Toy Calculator\",\n    description=\"Here's a sample toy calculator. Allows you to calculate things like $2+2=4$\",\n)\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/calculator'>\u003C/gradio-app>\n\nYou can load a large dataset into the examples to browse and interact with the dataset through Gradio. The examples will be automatically paginated (you can configure this through the `examples_per_page` argument of `Interface`).\n\nContinue learning about examples in the [More On Examples](https://gradio.app/guides/more-on-examples) guide.\n\n## Descriptive Content\n\nIn the previous example, you may have noticed the `title=` and `description=` keyword arguments in the `Interface` constructor that helps users understand your app.\n\nThere are three arguments in the `Interface` constructor to specify where this content should go:\n\n- `title`: which accepts text and can display it at the very top of interface, and also becomes the page title.\n- `description`: which accepts text, markdown or HTML and places it right under the title.\n- `article`: which also accepts text, markdown or HTML and places it below the interface.\n\n![annotated](https://github.com/gradio-app/gradio/blob/main/guides/assets/annotated.png?raw=true)\n\nNote: if you're using the `Blocks` class, you can insert text, markdown, or HTML anywhere in your application using the `gr.Markdown(...)` or `gr.HTML(...)` components.\n\nAnother useful keyword argument is `label=`, which is present in every `Component`. This modifies the label text at the top of each `Component`. You can also add the `info=` keyword argument to form elements like `Textbox` or `Radio` to provide further information on their usage.\n\n```python\ngr.Number(label='Age', info='In years, must be greater than 0')\n```\n\n## Additional Inputs within an Accordion\n\nIf your prediction function takes many inputs, you may want to hide some of them within a collapsed accordion to avoid cluttering the UI. The `Interface` class takes an `additional_inputs` argument which is similar to `inputs` but any input components included here are not visible by default. The user must click on the accordion to show these components. The additional inputs are passed into the prediction function, in order, after the standard inputs.\n\nYou can customize the appearance of the accordion by using the optional `additional_inputs_accordion` argument, which accepts a string (in which case, it becomes the label of the accordion), or an instance of the `gr.Accordion()` class (e.g. this lets you control whether the accordion is open or closed by default).\n\nHere's an example:\n\n```python\nimport gradio as gr\n\ndef generate_fake_image(prompt, seed, initial_image=None):\n    return f\"Used seed: {seed}\", \"https://dummyimage.com/300/09f.png\"\n\n\ndemo = gr.Interface(\n    generate_fake_image,\n    inputs=[\"textbox\"],\n    outputs=[\"textbox\", \"image\"],\n    additional_inputs=[\n        gr.Slider(0, 1000),\n        \"image\"\n    ]\n)\n\ndemo.launch()\n\n\n```\n\u003Cgradio-app space='gradio/interface_with_additional_inputs'>\u003C/gradio-app>\n\n",tags:[],spaces:[],url:"/guides/the-interface-class/",contributor:null},{name:"more-on-examples",category:"building-interfaces",pretty_category:"Building Interfaces",guide_index:1,absolute_index:3,pretty_name:"More On Examples",content:"# More on Examples\n\nIn the [previous Guide](/main/guides/the-interface-class), we discussed how to provide example inputs for your demo to make it easier for users to try it out. Here, we dive into more details.\n\n## Providing Examples\n\nAdding examples to an Interface is as easy as providing a list of lists to the `examples`\nkeyword argument.\nEach sublist is a data sample, where each element corresponds to an input of the prediction function.\nThe inputs must be ordered in the same order as the prediction function expects them.\n\nIf your interface only has one input component, then you can provide your examples as a regular list instead of a list of lists.\n\n### Loading Examples from a Directory\n\nYou can also specify a path to a directory containing your examples. If your Interface takes only a single file-type input, e.g. an image classifier, you can simply pass a directory filepath to the `examples=` argument, and the `Interface` will load the images in the directory as examples.\nIn the case of multiple inputs, this directory must\ncontain a log.csv file with the example values.\nIn the context of the calculator demo, we can set `examples='/demo/calculator/examples'` and in that directory we include the following `log.csv` file:\n\n```csv\nnum,operation,num2\n5,\"add\",3\n4,\"divide\",2\n5,\"multiply\",3\n```\n\nThis can be helpful when browsing flagged data. Simply point to the flagged directory and the `Interface` will load the examples from the flagged data.\n\n### Providing Partial Examples\n\nSometimes your app has many input components, but you would only like to provide examples for a subset of them. In order to exclude some inputs from the examples, pass `None` for all data samples corresponding to those particular components.\n\n## Caching examples\n\nYou may wish to provide some cached examples of your model for users to quickly try out, in case your model takes a while to run normally.\nIf `cache_examples=True`, your Gradio app will run all of the examples and save the outputs when you call the `launch()` method. This data will be saved in a directory called `gradio_cached_examples` in your working directory by default. You can also set this directory with the `GRADIO_EXAMPLES_CACHE` environment variable, which can be either an absolute path or a relative path to your working directory.\n\nWhenever a user clicks on an example, the output will automatically be populated in the app now, using data from this cached directory instead of actually running the function. This is useful so users can quickly try out your model without adding any load!\n\nAlternatively, you can set `cache_examples=\"lazy\"`. This means that each particular example will only get cached after it is first used (by any user) in the Gradio app. This is helpful if your prediction function is long-running and you do not want to wait a long time for your Gradio app to start.\n\nKeep in mind once the cache is generated, it will not be updated automatically in future launches. If the examples or function logic change, delete the cache folder to clear the cache and rebuild it with another `launch()`.\n",tags:[],spaces:[],url:"/guides/more-on-examples/",contributor:null},{name:"flagging",category:"building-interfaces",pretty_category:"Building Interfaces",guide_index:2,absolute_index:4,pretty_name:"Flagging",content:"\n# Flagging\n\nYou may have noticed the \"Flag\" button that appears by default in your `Interface`. When a user using your demo sees input with interesting output, such as erroneous or unexpected model behaviour, they can flag the input for you to review. Within the directory provided by the `flagging_dir=` argument to the `Interface` constructor, a CSV file will log the flagged inputs. If the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well.\n\nFor example, with the calculator interface shown above, we would have the flagged data stored in the flagged directory shown below:\n\n```directory\n+-- calculator.py\n+-- flagged/\n|   +-- logs.csv\n```\n\n_flagged/logs.csv_\n\n```csv\nnum1,operation,num2,Output\n5,add,7,12\n6,subtract,1.5,4.5\n```\n\nWith the sepia interface shown earlier, we would have the flagged data stored in the flagged directory shown below:\n\n```directory\n+-- sepia.py\n+-- flagged/\n|   +-- logs.csv\n|   +-- im/\n|   |   +-- 0.png\n|   |   +-- 1.png\n|   +-- Output/\n|   |   +-- 0.png\n|   |   +-- 1.png\n```\n\n_flagged/logs.csv_\n\n```csv\nim,Output\nim/0.png,Output/0.png\nim/1.png,Output/1.png\n```\n\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging_options` argument of Interface. Users will have to select one of the strings when flagging, which will be saved as an additional column to the CSV.\n\n\n",tags:[],spaces:[],url:"/guides/flagging/",contributor:null},{name:"interface-state",category:"building-interfaces",pretty_category:"Building Interfaces",guide_index:3,absolute_index:5,pretty_name:"Interface State",content:"# Interface State\n\nSo far, we've assumed that your demos are *stateless*: that they do not persist information beyond a single function call. What if you want to modify the behavior of your demo based on previous interactions with the demo? There are two approaches in Gradio: *global state* and *session state*.\n\n## Global State\n\nIf the state is something that should be accessible to all function calls and all users, you can create a variable outside the function call and access it inside the function. For example, you may load a large model outside the function and use it inside the function so that every function call does not need to reload the model.\n\n```python\nimport gradio as gr\n\nscores = []\n\ndef track_score(score):\n    scores.append(score)\n    top_scores = sorted(scores, reverse=True)[:3]\n    return top_scores\n\ndemo = gr.Interface(\n    track_score, \n    gr.Number(label=\"Score\"), \n    gr.JSON(label=\"Top Scores\")\n)\ndemo.launch()\n```\n\nIn the code above, the `scores` array is shared between all users. If multiple users are accessing this demo, their scores will all be added to the same list, and the returned top 3 scores will be collected from this shared reference.\n\n## Session State\n\nAnother type of data persistence Gradio supports is session state, where data persists across multiple submits within a page session. However, data is _not_ shared between different users of your model. To store data in a session state, you need to do three things:\n\n1. Pass in an extra parameter into your function, which represents the state of the interface.\n2. At the end of the function, return the updated value of the state as an extra return value.\n3. Add the `'state'` input and `'state'` output components when creating your `Interface`\n\nHere's a simple app to illustrate session state - this app simply stores users previous submissions and displays them back to the user:\n\n\n```python\nimport gradio as gr\n\ndef store_message(message: str, history: list[str]):\n    output = {\n        \"Current messages\": message,\n        \"Previous messages\": history[::-1]\n    }\n    history.append(message)\n    return output, history\n\ndemo = gr.Interface(fn=store_message, \n                    inputs=[\"textbox\", gr.State(value=[])], \n                    outputs=[\"json\", gr.State()])\n\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/interface_state'>\u003C/gradio-app>\n\n\nNotice how the state persists across submits within each page, but if you load this demo in another tab (or refresh the page), the demos will not share chat history. Here, we could not store the submission history in a global variable, otherwise the submission history would then get jumbled between different users.\n\nThe initial value of the `State` is `None` by default. If you pass a parameter to the `value` argument of `gr.State()`, it is used as the default value of the state instead. \n\nNote: the `Interface` class only supports a single session state variable (though it can be a list with multiple elements). For more complex use cases, you can use Blocks, [which supports multiple `State` variables](/guides/state-in-blocks/). Alternatively, if you are building a chatbot that maintains user state, consider using the `ChatInterface` abstraction, [which manages state automatically](/guides/creating-a-chatbot-fast).\n",tags:[],spaces:[],url:"/guides/interface-state/",contributor:null},{name:"reactive-interfaces",category:"building-interfaces",pretty_category:"Building Interfaces",guide_index:4,absolute_index:6,pretty_name:"Reactive Interfaces",content:"# Reactive Interfaces\n\nFinally, we cover how to get Gradio demos to refresh automatically or continuously stream data.\n\n## Live Interfaces\n\nYou can make interfaces automatically refresh by setting `live=True` in the interface. Now the interface will recalculate as soon as the user input changes.\n\n```python\nimport gradio as gr\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        return num1 / num2\n\ndemo = gr.Interface(\n    calculator,\n    [\n        \"number\",\n        gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]),\n        \"number\"\n    ],\n    \"number\",\n    live=True,\n)\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/calculator_live'>\u003C/gradio-app>\n\nNote there is no submit button, because the interface resubmits automatically on change.\n\n## Streaming Components\n\nSome components have a \"streaming\" mode, such as `Audio` component in microphone mode, or the `Image` component in webcam mode. Streaming means data is sent continuously to the backend and the `Interface` function is continuously being rerun.\n\nThe difference between `gr.Audio(source='microphone')` and `gr.Audio(source='microphone', streaming=True)`, when both are used in `gr.Interface(live=True)`, is that the first `Component` will automatically submit data and run the `Interface` function when the user stops recording, whereas the second `Component` will continuously send data and run the `Interface` function _during_ recording.\n\nHere is example code of streaming images from the webcam.\n\n```python\nimport gradio as gr\nimport numpy as np\n\ndef flip(im):\n    return np.flipud(im)\n\ndemo = gr.Interface(\n    flip, \n    gr.Image(sources=[\"webcam\"], streaming=True), \n    \"image\",\n    live=True\n)\ndemo.launch()\n    \n```\n\nStreaming can also be done in an output component. A `gr.Audio(streaming=True)` output component can take a stream of audio data yielded piece-wise by a generator function and combines them into a single audio file.\n\n```python\nimport gradio as gr\nfrom pydub import AudioSegment\nfrom time import sleep\n\nwith gr.Blocks() as demo:\n    input_audio = gr.Audio(label=\"Input Audio\", type=\"filepath\", format=\"mp3\")\n    with gr.Row():\n        with gr.Column():\n            stream_as_file_btn = gr.Button(\"Stream as File\")\n            format = gr.Radio([\"wav\", \"mp3\"], value=\"wav\", label=\"Format\")\n            stream_as_file_output = gr.Audio(streaming=True)\n\n            def stream_file(audio_file, format):\n                audio = AudioSegment.from_file(audio_file)\n                i = 0\n                chunk_size = 1000\n                while chunk_size * i \u003C len(audio):\n                    chunk = audio[chunk_size * i : chunk_size * (i + 1)]\n                    i += 1\n                    if chunk:\n                        file = f\"/tmp/{i}.{format}\"\n                        chunk.export(file, format=format)\n                        yield file\n                        sleep(0.5)\n\n            stream_as_file_btn.click(\n                stream_file, [input_audio, format], stream_as_file_output\n            )\n\n            gr.Examples(\n                [[\"audio/cantina.wav\", \"wav\"], [\"audio/cantina.wav\", \"mp3\"]],\n                [input_audio, format],\n                fn=stream_file,\n                outputs=stream_as_file_output,\n            )\n\n        with gr.Column():\n            stream_as_bytes_btn = gr.Button(\"Stream as Bytes\")\n            stream_as_bytes_output = gr.Audio(format=\"bytes\", streaming=True)\n\n            def stream_bytes(audio_file):\n                chunk_size = 20_000\n                with open(audio_file, \"rb\") as f:\n                    while True:\n                        chunk = f.read(chunk_size)\n                        if chunk:\n                            yield chunk\n                            sleep(1)\n                        else:\n                            break\n            stream_as_bytes_btn.click(stream_bytes, input_audio, stream_as_bytes_output)\n\nif __name__ == \"__main__\":\n    demo.queue().launch()\n\n```\n\nFor a more detailed example, see our guide on performing [automatic speech recognition](/guides/real-time-speech-recognition) with Gradio.\n",tags:[],spaces:[],url:"/guides/reactive-interfaces/",contributor:null},{name:"four-kinds-of-interfaces",category:"building-interfaces",pretty_category:"Building Interfaces",guide_index:5,absolute_index:7,pretty_name:"Four Kinds Of Interfaces",content:"# The 4 Kinds of Gradio Interfaces\n\nSo far, we've always assumed that in order to build an Gradio demo, you need both inputs and outputs. But this isn't always the case for machine learning demos: for example, _unconditional image generation models_ don't take any input but produce an image as the output.\n\nIt turns out that the `gradio.Interface` class can actually handle 4 different kinds of demos:\n\n1. **Standard demos**: which have both separate inputs and outputs (e.g. an image classifier or speech-to-text model)\n2. **Output-only demos**: which don't take any input but produce on output (e.g. an unconditional image generation model)\n3. **Input-only demos**: which don't produce any output but do take in some sort of input (e.g. a demo that saves images that you upload to a persistent external database)\n4. **Unified demos**: which have both input and output components, but the input and output components _are the same_. This means that the output produced overrides the input (e.g. a text autocomplete model)\n\nDepending on the kind of demo, the user interface (UI) looks slightly different:\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/interfaces4.png)\n\nLet's see how to build each kind of demo using the `Interface` class, along with examples:\n\n## Standard demos\n\nTo create a demo that has both the input and the output components, you simply need to set the values of the `inputs` and `outputs` parameter in `Interface()`. Here's an example demo of a simple image filter:\n\n```python\nimport numpy as np\nimport gradio as gr\n\ndef sepia(input_img):\n    sepia_filter = np.array([\n        [0.393, 0.769, 0.189], \n        [0.349, 0.686, 0.168], \n        [0.272, 0.534, 0.131]\n    ])\n    sepia_img = input_img.dot(sepia_filter.T)\n    sepia_img /= sepia_img.max()\n    return sepia_img\n\ndemo = gr.Interface(sepia, gr.Image(), \"image\")\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/sepia_filter'>\u003C/gradio-app>\n\n## Output-only demos\n\nWhat about demos that only contain outputs? In order to build such a demo, you simply set the value of the `inputs` parameter in `Interface()` to `None`. Here's an example demo of a mock image generation model:\n\n```python\nimport time\n\nimport gradio as gr\n\n\ndef fake_gan():\n    time.sleep(1)\n    images = [\n            \"https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=387&q=80\",\n            \"https://images.unsplash.com/photo-1554151228-14d9def656e4?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=386&q=80\",\n            \"https://images.unsplash.com/photo-1542909168-82c3e7fdca5c?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8aHVtYW4lMjBmYWNlfGVufDB8fDB8fA%3D%3D&w=1000&q=80\",\n    ]\n    return images\n\n\ndemo = gr.Interface(\n    fn=fake_gan,\n    inputs=None,\n    outputs=gr.Gallery(label=\"Generated Images\", columns=[2]),\n    title=\"FD-GAN\",\n    description=\"This is a fake demo of a GAN. In reality, the images are randomly chosen from Unsplash.\",\n)\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/fake_gan_no_input'>\u003C/gradio-app>\n\n## Input-only demos\n\nSimilarly, to create a demo that only contains inputs, set the value of `outputs` parameter in `Interface()` to be `None`. Here's an example demo that saves any uploaded image to disk:\n\n```python\nimport random\nimport string\nimport gradio as gr \n\ndef save_image_random_name(image):\n    random_string = ''.join(random.choices(string.ascii_letters, k=20)) + '.png'\n    image.save(random_string)\n    print(f\"Saved image to {random_string}!\")\n\ndemo = gr.Interface(\n    fn=save_image_random_name, \n    inputs=gr.Image(type=\"pil\"), \n    outputs=None,\n)\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/save_file_no_output'>\u003C/gradio-app>\n\n## Unified demos\n\nA demo that has a single component as both the input and the output. It can simply be created by setting the values of the `inputs` and `outputs` parameter as the same component. Here's an example demo of a text generation model:\n\n```python\nimport gradio as gr\nfrom transformers import pipeline\n\ngenerator = pipeline('text-generation', model = 'gpt2')\n\ndef generate_text(text_prompt):\n  response = generator(text_prompt, max_length = 30, num_return_sequences=5)\n  return response[0]['generated_text']\n\ntextbox = gr.Textbox()\n\ndemo = gr.Interface(generate_text, textbox, textbox)\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/unified_demo_text_generation'>\u003C/gradio-app>\n",tags:[],spaces:[],url:"/guides/four-kinds-of-interfaces/",contributor:null}]},{category:"Additional Features",guides:[{name:"queuing",category:"additional-features",pretty_category:"Additional Features",guide_index:1,absolute_index:8,pretty_name:"Queuing",content:"# Queuing\n\nEvery Gradio app comes with a built-in queuing system that can scale to thousands of concurrent users. You can configure the queue by using `queue()` method which is supported by the `gr.Interface`, `gr.Blocks`, and `gr.ChatInterface` classes. \n\nFor example, you can control the number of requests processed at a single time by setting the `default_concurrency_limit` parameter of `queue()`, e.g.\n\n```python\ndemo = gr.Interface(...).queue(default_concurrency_limit=5)\ndemo.launch()\n```\n\nThis limits the number of requests processed for this event listener at a single time to 5. By default, the `default_concurrency_limit` is actually set to `1`, which means that when many users are using your app, only a single user's request will be processed at a time. This is because many machine learning functions consume a significant amount of memory and so it is only suitable to have a single user using the demo at a time. However, you can change this parameter in your demo easily.\n\nSee the [docs on queueing](/docs/gradio/interface#interface-queue) for more details on configuring the queuing parameters.\n\nYou can see analytics on the number and status of all requests processed by the queue by visiting the `/monitoring` endpoint of your app. This endpoint will print a secret URL to your console that links to the full analytics dashboard.",tags:[],spaces:[],url:"/guides/queuing/",contributor:null},{name:"streaming-outputs",category:"additional-features",pretty_category:"Additional Features",guide_index:2,absolute_index:9,pretty_name:"Streaming Outputs",content:"# Streaming outputs\n\nIn some cases, you may want to stream a sequence of outputs rather than show a single output at once. For example, you might have an image generation model and you want to show the image that is generated at each step, leading up to the final image. Or you might have a chatbot which streams its response one token at a time instead of returning it all at once.\n\nIn such cases, you can supply a **generator** function into Gradio instead of a regular function. Creating generators in Python is very simple: instead of a single `return` value, a function should `yield` a series of values instead. Usually the `yield` statement is put in some kind of loop. Here's an example of an generator that simply counts up to a given number:\n\n```python\ndef my_generator(x):\n    for i in range(x):\n        yield i\n```\n\nYou supply a generator into Gradio the same way as you would a regular function. For example, here's a a (fake) image generation model that generates noise for several steps before outputting an image using the `gr.Interface` class:\n\n```python\nimport gradio as gr\nimport numpy as np\nimport time\n\ndef fake_diffusion(steps):\n    rng = np.random.default_rng()\n    for i in range(steps):\n        time.sleep(1)\n        image = rng.random(size=(600, 600, 3))\n        yield image\n    image = np.ones((1000,1000,3), np.uint8)\n    image[:] = [255, 124, 0]\n    yield image\n\n\ndemo = gr.Interface(fake_diffusion,\n                    inputs=gr.Slider(1, 10, 3, step=1),\n                    outputs=\"image\")\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/fake_diffusion'>\u003C/gradio-app>\n\nNote that we've added a `time.sleep(1)` in the iterator to create an artificial pause between steps so that you are able to observe the steps of the iterator (in a real image generation model, this probably wouldn't be necessary).\n\nSimilarly, Gradio can handle streaming inputs, e.g. an image generation model that reruns every time a user types a letter in a textbox. This is covered in more details in our guide on building [reactive Interfaces](/guides/reactive-interfaces). \n",tags:[],spaces:[],url:"/guides/streaming-outputs/",contributor:null},{name:"alerts",category:"additional-features",pretty_category:"Additional Features",guide_index:3,absolute_index:10,pretty_name:"Alerts",content:"# Alerts\n\nYou may wish to display alerts to the user. To do so, raise a `gr.Error(\"custom message\")` in your function to halt the execution of your function and display an error message to the user.\n\nAlternatively, can issue `gr.Warning(\"custom message\")` or `gr.Info(\"custom message\")` by having them as standalone lines in your function, which will immediately display modals while continuing the execution of your function. The only difference between `gr.Info()` and `gr.Warning()` is the color of the alert. \n\n```python\ndef start_process(name):\n    gr.Info(\"Starting process\")\n    if name is None:\n        gr.Warning(\"Name is empty\")\n    ...\n    if success == False:\n        raise gr.Error(\"Process failed\")\n```\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> Note that `gr.Error()` is an exception that has to be raised, while `gr.Warning()` and `gr.Info()` are functions that are called directly.\u003C/p>\n\n",tags:[],spaces:[],url:"/guides/alerts/",contributor:null},{name:"styling",category:"additional-features",pretty_category:"Additional Features",guide_index:4,absolute_index:11,pretty_name:"Styling",content:"# Styling\n\nGradio themes are the easiest way to customize the look and feel of your app. You can choose from a variety of themes, or create your own. To do so, pass the `theme=` kwarg to the `Interface` constructor. For example:\n\n```python\ndemo = gr.Interface(..., theme=gr.themes.Monochrome())\n```\n\nGradio comes with a set of prebuilt themes which you can load from `gr.themes.*`. You can extend these themes or create your own themes from scratch - see the [theming guide](https://gradio.app/guides/theming-guide) for more details.\n\nFor additional styling ability, you can pass any CSS (as well as custom JavaScript) to your Gradio application. This is discussed in more detail in our [custom JS and CSS guide](/guides/custom-CSS-and-JS).\n\n",tags:[],spaces:[],url:"/guides/styling/",contributor:null},{name:"progress_bars",category:"additional-features",pretty_category:"Additional Features",guide_index:5,absolute_index:12,pretty_name:"Progress_bars",content:"# Progress bars\n\nGradio supports the ability to create custom Progress Bars so that you have customizability and control over the progress update that you show to the user. In order to enable this, simply add an argument to your method that has a default value of a `gr.Progress` instance. Then you can update the progress levels by calling this instance directly with a float between 0 and 1, or using the `tqdm()` method of the `Progress` instance to track progress over an iterable, as shown below.\n\n```python\nimport gradio as gr\nimport time\n\ndef slowly_reverse(word, progress=gr.Progress()):\n    progress(0, desc=\"Starting\")\n    time.sleep(1)\n    progress(0.05)\n    new_string = \"\"\n    for letter in progress.tqdm(word, desc=\"Reversing\"):\n        time.sleep(0.25)\n        new_string = letter + new_string\n    return new_string\n\ndemo = gr.Interface(slowly_reverse, gr.Text(), gr.Text())\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/progress_simple'>\u003C/gradio-app>\n\nIf you use the `tqdm` library, you can even report progress updates automatically from any `tqdm.tqdm` that already exists within your function by setting the default argument as `gr.Progress(track_tqdm=True)`!\n",tags:[],spaces:[],url:"/guides/progress_bars/",contributor:null},{name:"batch-functions",category:"additional-features",pretty_category:"Additional Features",guide_index:6,absolute_index:13,pretty_name:"Batch Functions",content:"# Batch functions\n\nGradio supports the ability to pass _batch_ functions. Batch functions are just\nfunctions which take in a list of inputs and return a list of predictions.\n\nFor example, here is a batched function that takes in two lists of inputs (a list of\nwords and a list of ints), and returns a list of trimmed words as output:\n\n```py\nimport time\n\ndef trim_words(words, lens):\n    trimmed_words = []\n    time.sleep(5)\n    for w, l in zip(words, lens):\n        trimmed_words.append(w[:int(l)])\n    return [trimmed_words]\n```\n\nThe advantage of using batched functions is that if you enable queuing, the Gradio server can automatically _batch_ incoming requests and process them in parallel,\npotentially speeding up your demo. Here's what the Gradio code looks like (notice the `batch=True` and `max_batch_size=16`)\n\nWith the `gr.Interface` class:\n\n```python\ndemo = gr.Interface(\n    fn=trim_words, \n    inputs=[\"textbox\", \"number\"], \n    outputs=[\"output\"],\n    batch=True, \n    max_batch_size=16\n)\n\ndemo.launch()\n```\n\nWith the `gr.Blocks` class:\n\n```py\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        word = gr.Textbox(label=\"word\")\n        leng = gr.Number(label=\"leng\")\n        output = gr.Textbox(label=\"Output\")\n    with gr.Row():\n        run = gr.Button()\n\n    event = run.click(trim_words, [word, leng], output, batch=True, max_batch_size=16)\n\ndemo.launch()\n```\n\nIn the example above, 16 requests could be processed in parallel (for a total inference time of 5 seconds), instead of each request being processed separately (for a total\ninference time of 80 seconds). Many Hugging Face `transformers` and `diffusers` models work very naturally with Gradio's batch mode: here's [an example demo using diffusers to\ngenerate images in batches](https://github.com/gradio-app/gradio/blob/main/demo/diffusers_with_batching/run.py)\n\n\n",tags:[],spaces:[],url:"/guides/batch-functions/",contributor:null},{name:"sharing-your-app",category:"additional-features",pretty_category:"Additional Features",guide_index:7,absolute_index:14,pretty_name:"Sharing Your App",content:"# Sharing Your App\n\nIn this Guide, we dive more deeply into the various aspects of sharing a Gradio app with others. We will cover:\n\n1. [Sharing demos with the share parameter](#sharing-demos)\n2. [Hosting on HF Spaces](#hosting-on-hf-spaces)\n3. [Embedding hosted spaces](#embedding-hosted-spaces)\n4. [Using the API page](#api-page)\n5. [Accessing network requests](#accessing-the-network-request-directly)\n6. [Mounting within FastAPI](#mounting-within-another-fast-api-app)\n7. [Authentication](#authentication)\n8. [Security and file access](#security-and-file-access)\n9. [Analytics](#analytics)\n\n## Sharing Demos\n\nGradio demos can be easily shared publicly by setting `share=True` in the `launch()` method. Like this:\n\n```python\nimport gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(fn=greet, inputs=\"textbox\", outputs=\"textbox\")\n    \ndemo.launch(share=True)  # Share your demo with just 1 extra parameter üöÄ\n```\n\nThis generates a public, shareable link that you can send to anybody! When you send this link, the user on the other side can try out the model in their browser. Because the processing happens on your device (as long as your device stays on), you don't have to worry about any packaging any dependencies. \n\n![sharing](https://github.com/gradio-app/gradio/blob/main/guides/assets/sharing.svg?raw=true)\n\n\nA share link usually looks something like this: **https://07ff8706ab.gradio.live**. Although the link is served through the Gradio Share Servers, these servers are only a proxy for your local server, and do not store any data sent through your app. Share links expire after 72 hours. (it is [also possible to set up your own Share Server](https://github.com/huggingface/frp/) on your own cloud server to overcome this restriction.)\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> Keep in mind that share links are publicly accessible, meaning that anyone can use your model for prediction! Therefore, make sure not to expose any sensitive information through the functions you write, or allow any critical changes to occur on your device. Or you can [add authentication to your Gradio app](#authentication) as discussed below.\u003C/p>\n\nNote that by default, `share=False`, which means that your server is only running locally. (This is the default, except in Google Colab notebooks, where share links are automatically created). As an alternative to using share links, you can use use [SSH port-forwarding](https://www.ssh.com/ssh/tunneling/example) to share your local server with specific users.\n\n\n## Hosting on HF Spaces\n\nIf you'd like to have a permanent link to your Gradio demo on the internet, use Hugging Face Spaces. [Hugging Face Spaces](http://huggingface.co/spaces/) provides the infrastructure to permanently host your machine learning model for free!\n\nAfter you have [created a free Hugging Face account](https://huggingface.co/join), you have two methods to deploy your Gradio app to Hugging Face Spaces:\n\n1. From terminal: run `gradio deploy` in your app directory. The CLI will gather some basic metadata and then launch your app. To update your space, you can re-run this command or enable the Github Actions option to automatically update the Spaces on `git push`.\n\n2. From your browser: Drag and drop a folder containing your Gradio model and all related files [here](https://huggingface.co/new-space). See [this guide how to host on Hugging Face Spaces](https://huggingface.co/blog/gradio-spaces) for more information, or watch the embedded video:\n\n\u003Cvideo autoplay muted loop>\n  \u003Csource src=\"https://github.com/gradio-app/gradio/blob/main/guides/assets/hf_demo.mp4?raw=true\" type=\"video/mp4\" />\n\u003C/video>\n\n\n## Embedding Hosted Spaces\n\nOnce you have hosted your app on Hugging Face Spaces (or on your own server), you may want to embed the demo on a different website, such as your blog or your portfolio. Embedding an interactive demo allows people to try out the machine learning model that you have built, without needing to download or install anything ‚Äî right in their browser! The best part is that you can embed interactive demos even in static websites, such as GitHub pages.\n\nThere are two ways to embed your Gradio demos. You can find quick links to both options directly on the Hugging Face Space page, in the \"Embed this Space\" dropdown option:\n\n![Embed this Space dropdown option](https://github.com/gradio-app/gradio/blob/main/guides/assets/embed_this_space.png?raw=true)\n\n### Embedding with Web Components\n\nWeb components typically offer a better experience to users than IFrames. Web components load lazily, meaning that they won't slow down the loading time of your website, and they automatically adjust their height based on the size of the Gradio app.\n\nTo embed with Web Components:\n\n1. Import the gradio JS library into into your site by adding the script below in your site (replace {GRADIO_VERSION} in the URL with the library version of Gradio you are using).\n\n```html\n\u003Cscript\n\ttype=\"module\"\n\tsrc=\"https://gradio.s3-us-west-2.amazonaws.com/{GRADIO_VERSION}/gradio.js\"\n>\u003C/script>\n```\n\n2. Add\n\n```html\n\u003Cgradio-app src=\"https://$your_space_host.hf.space\">\u003C/gradio-app>\n```\n\nelement where you want to place the app. Set the `src=` attribute to your Space's embed URL, which you can find in the \"Embed this Space\" button. For example:\n\n```html\n\u003Cgradio-app\n\tsrc=\"https://abidlabs-pytorch-image-classifier.hf.space\"\n>\u003C/gradio-app>\n```\n\n\u003Cscript>\nfetch(\"https://pypi.org/pypi/gradio/json\"\n).then(r => r.json()\n).then(obj => {\n    let v = obj.info.version;\n    content = document.querySelector('.prose');\n    content.innerHTML = content.innerHTML.replaceAll(\"{GRADIO_VERSION}\", v);\n});\n\u003C/script>\n\nYou can see examples of how web components look \u003Ca href=\"https://www.gradio.app\">on the Gradio landing page\u003C/a>.\n\nYou can also customize the appearance and behavior of your web component with attributes that you pass into the `\u003Cgradio-app>` tag:\n\n- `src`: as we've seen, the `src` attributes links to the URL of the hosted Gradio demo that you would like to embed\n- `space`: an optional shorthand if your Gradio demo is hosted on Hugging Face Space. Accepts a `username/space_name` instead of a full URL. Example: `gradio/Echocardiogram-Segmentation`. If this attribute attribute is provided, then `src` does not need to be provided.\n- `control_page_title`: a boolean designating whether the html title of the page should be set to the title of the Gradio app (by default `\"false\"`)\n- `initial_height`: the initial height of the web component while it is loading the Gradio app, (by default `\"300px\"`). Note that the final height is set based on the size of the Gradio app.\n- `container`: whether to show the border frame and information about where the Space is hosted (by default `\"true\"`)\n- `info`: whether to show just the information about where the Space is hosted underneath the embedded app (by default `\"true\"`)\n- `autoscroll`: whether to autoscroll to the output when prediction has finished (by default `\"false\"`)\n- `eager`: whether to load the Gradio app as soon as the page loads (by default `\"false\"`)\n- `theme_mode`: whether to use the `dark`, `light`, or default `system` theme mode (by default `\"system\"`)\n- `render`: an event that is triggered once the embedded space has finished rendering.\n\nHere's an example of how to use these attributes to create a Gradio app that does not lazy load and has an initial height of 0px.\n\n```html\n\u003Cgradio-app\n\tspace=\"gradio/Echocardiogram-Segmentation\"\n\teager=\"true\"\n\tinitial_height=\"0px\"\n>\u003C/gradio-app>\n```\n\nHere's another example of how to use the `render` event. An event listener is used to capture the `render` event and will call the `handleLoadComplete()` function once rendering is complete. \n\n```html\n\u003Cscript>\n\tfunction handleLoadComplete() {\n\t\tconsole.log(\"Embedded space has finished rendering\");\n\t}\n\n\tconst gradioApp = document.querySelector(\"gradio-app\");\n\tgradioApp.addEventListener(\"render\", handleLoadComplete);\n\u003C/script>\n```\n\n_Note: While Gradio's CSS will never impact the embedding page, the embedding page can affect the style of the embedded Gradio app. Make sure that any CSS in the parent page isn't so general that it could also apply to the embedded Gradio app and cause the styling to break. Element selectors such as `header { ... }` and `footer { ... }` will be the most likely to cause issues._\n\n### Embedding with IFrames\n\nTo embed with IFrames instead (if you cannot add javascript to your website, for example), add this element:\n\n```html\n\u003Ciframe src=\"https://$your_space_host.hf.space\">\u003C/iframe>\n```\n\nAgain, you can find the `src=` attribute to your Space's embed URL, which you can find in the \"Embed this Space\" button.\n\nNote: if you use IFrames, you'll probably want to add a fixed `height` attribute and set `style=\"border:0;\"` to remove the boreder. In addition, if your app requires permissions such as access to the webcam or the microphone, you'll need to provide that as well using the `allow` attribute.\n\n## API Page\n\nYou can use almost any Gradio app as an API! In the footer of a Gradio app [like this one](https://huggingface.co/spaces/gradio/hello_world), you'll see a \"Use via API\" link.\n\n![Use via API](https://github.com/gradio-app/gradio/blob/main/guides/assets/use_via_api.png?raw=true)\n\nThis is a page that lists the endpoints that can be used to query the Gradio app, via our supported clients: either [the Python client](https://gradio.app/guides/getting-started-with-the-python-client/), or [the JavaScript client](https://gradio.app/guides/getting-started-with-the-js-client/). For each endpoint, Gradio automatically generates the parameters and their types, as well as example inputs, like this.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/view-api.png)\n\nThe endpoints are automatically created when you launch a Gradio `Interface`. If you are using Gradio `Blocks`, you can also set up a Gradio API page, though we recommend that you explicitly name each event listener, such as\n\n```python\nbtn.click(add, [num1, num2], output, api_name=\"addition\")\n```\n\nThis will add and document the endpoint `/api/addition/` to the automatically generated API page. Otherwise, your API endpoints will appear as \"unnamed\" endpoints.\n\n## Accessing the Network Request Directly\n\nWhen a user makes a prediction to your app, you may need the underlying network request, in order to get the request headers (e.g. for advanced authentication), log the client's IP address, getting the query parameters, or for other reasons. Gradio supports this in a similar manner to FastAPI: simply add a function parameter whose type hint is `gr.Request` and Gradio will pass in the network request as that parameter. Here is an example:\n\n```python\nimport gradio as gr\n\ndef echo(text, request: gr.Request):\n    if request:\n        print(\"Request headers dictionary:\", request.headers)\n        print(\"IP address:\", request.client.host)\n        print(\"Query parameters:\", dict(request.query_params))\n    return text\n\nio = gr.Interface(echo, \"textbox\", \"textbox\").launch()\n```\n\nNote: if your function is called directly instead of through the UI (this happens, for\nexample, when examples are cached, or when the Gradio app is called via API), then `request` will be `None`. \nYou should handle this case explicitly to ensure that your app does not throw any errors. That is why\nwe have the explicit check `if request`.\n\n## Mounting Within Another FastAPI App\n\nIn some cases, you might have an existing FastAPI app, and you'd like to add a path for a Gradio demo.\nYou can easily do this with `gradio.mount_gradio_app()`.\n\nHere's a complete example:\n\n```python\nfrom fastapi import FastAPI\nimport gradio as gr\n\nCUSTOM_PATH = \"/gradio\"\n\napp = FastAPI()\n\n\n@app.get(\"/\")\ndef read_main():\n    return {\"message\": \"This is your main app\"}\n\n\nio = gr.Interface(lambda x: \"Hello, \" + x + \"!\", \"textbox\", \"textbox\")\napp = gr.mount_gradio_app(app, io, path=CUSTOM_PATH)\n\n\n# Run this from the terminal as you would normally start a FastAPI app: `uvicorn run:app`\n# and navigate to http://localhost:8000/gradio in your browser.\n\n```\n\nNote that this approach also allows you run your Gradio apps on custom paths (`http://localhost:8000/gradio` in the example above).\n\n\n## Authentication\n\n### Password-protected app\n\nYou may wish to put an authentication page in front of your app to limit who can open your app. With the `auth=` keyword argument in the `launch()` method, you can provide a tuple with a username and password, or a list of acceptable username/password tuples; Here's an example that provides password-based authentication for a single user named \"admin\":\n\n```python\ndemo.launch(auth=(\"admin\", \"pass1234\"))\n```\n\nFor more complex authentication handling, you can even pass a function that takes a username and password as arguments, and returns `True` to allow access, `False` otherwise.\n\nHere's an example of a function that accepts any login where the username and password are the same:\n\n```python\ndef same_auth(username, password):\n    return username == password\ndemo.launch(auth=same_auth)\n```\n\nIf you have multiple users, you may wish to customize the content that is shown depending on the user that is logged in. You can retrieve the logged in user by [accessing the network request directly](#accessing-the-network-request-directly) as discussed above, and then reading the `.username` attribute of the request. Here's an example:\n\n\n```python\nimport gradio as gr\n\ndef update_message(request: gr.Request):\n    return f\"Welcome, {request.username}\"\n\nwith gr.Blocks() as demo:\n    m = gr.Markdown()\n    demo.load(update_message, None, m)\n    \ndemo.launch(auth=[(\"Abubakar\", \"Abubakar\"), (\"Ali\", \"Ali\")])\n```\n\nNote: For authentication to work properly, third party cookies must be enabled in your browser. This is not the case by default for Safari or for Chrome Incognito Mode. \n\nIf users visit the `/logout` page of your Gradio app, they will automatically be logged out and session cookies deleted. This allows you to add logout functionality to your Gradio app as well. Let's update the previous example to include a log out button:\n\n```python\nimport gradio as gr\n\ndef update_message(request: gr.Request):\n    return f\"Welcome, {request.username}\"\n\nwith gr.Blocks() as demo:\n    m = gr.Markdown()\n    logout_button = gr.Button(\"Logout\", link=\"/logout\")\n    demo.load(update_message, None, m)\n    \ndemo.launch(auth=[(\"Pete\", \"Pete\"), (\"Dawood\", \"Dawood\")])\n```\n\nNote: Gradio's built-in authentication provides a straightforward and basic layer of access control but does not offer robust security features for applications that require stringent access controls (e.g.  multi-factor authentication, rate limiting, or automatic lockout policies).\n\n### OAuth (Login via Hugging Face)\n\nGradio natively supports OAuth login via Hugging Face. In other words, you can easily add a _\"Sign in with Hugging Face\"_ button to your demo, which allows you to get a user's HF username as well as other information from their HF profile. Check out [this Space](https://huggingface.co/spaces/Wauplin/gradio-oauth-demo) for a live demo.\n\nTo enable OAuth, you must set `hf_oauth: true` as a Space metadata in your README.md file. This will register your Space\nas an OAuth application on Hugging Face. Next, you can use `gr.LoginButton` to add a login button to\nyour Gradio app. Once a user is logged in with their HF account, you can retrieve their profile by adding a parameter of type\n`gr.OAuthProfile` to any Gradio function. The user profile will be automatically injected as a parameter value. If you want\nto perform actions on behalf of the user (e.g. list user's private repos, create repo, etc.), you can retrieve the user\ntoken by adding a parameter of type `gr.OAuthToken`. You must define which scopes you will use in your Space metadata\n(see [documentation](https://huggingface.co/docs/hub/spaces-oauth#scopes) for more details).\n\nHere is a short example:\n\n```py\nimport gradio as gr\nfrom huggingface_hub import whoami\n\ndef hello(profile: gr.OAuthProfile | None) -> str:\n    if profile is None:\n        return \"I don't know you.\"\n    return f\"Hello {profile.name}\"\n\ndef list_organizations(oauth_token: gr.OAuthToken | None) -> str:\n    if oauth_token is None:\n        return \"Please log in to list organizations.\"\n    org_names = [org[\"name\"] for org in whoami(oauth_token.token)[\"orgs\"]]\n    return f\"You belong to {', '.join(org_names)}.\"\n\nwith gr.Blocks() as demo:\n    gr.LoginButton()\n    m1 = gr.Markdown()\n    m2 = gr.Markdown()\n    demo.load(hello, inputs=None, outputs=m1)\n    demo.load(list_organizations, inputs=None, outputs=m2)\n\ndemo.launch()\n```\n\nWhen the user clicks on the login button, they get redirected in a new page to authorize your Space.\n\n\u003Ccenter>\n\u003Cimg src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/oauth_sign_in.png\" style=\"width:300px; max-width:80%\">\n\u003C/center>\n\nUsers can revoke access to their profile at any time in their [settings](https://huggingface.co/settings/connected-applications).\n\nAs seen above, OAuth features are available only when your app runs in a Space. However, you often need to test your app\nlocally before deploying it. To test OAuth features locally, your machine must be logged in to Hugging Face. Please run `huggingface-cli login` or set `HF_TOKEN` as environment variable with one of your access token. You can generate a new token in your settings page (https://huggingface.co/settings/tokens). Then, clicking on the `gr.LoginButton` will login your local Hugging Face profile, allowing you to debug your app with your Hugging Face account before deploying it to a Space.\n\n\n### OAuth (with external providers)\n\nIt is also possible to authenticate with external OAuth providers (e.g. Google OAuth) in your Gradio apps. To do this, first mount your Gradio app within a FastAPI app ([as discussed above](#mounting-within-another-fast-api-app)). Then, you must write an *authentication function*, which gets the user's username from the OAuth provider and returns it. This function should be passed to the `auth_dependency` parameter in `gr.mount_gradio_app`. \n\nSimilar to [FastAPI dependency functions](https://fastapi.tiangolo.com/tutorial/dependencies/), the function specified by `auth_dependency` will run before any Gradio-related route in your FastAPI app. The function should accept a single parameter: the FastAPI `Request` and return either a string (representing a user's username) or `None`. If a string is returned, the user will be able to access the Gradio-related routes in your FastAPI app. \n\nFirst, let's show a simplistic example to illustrate the `auth_dependency` parameter:\n\n```python\nfrom fastapi import FastAPI, Request\nimport gradio as gr\n\napp = FastAPI()\n\ndef get_user(request: Request):\n    return request.headers.get(\"user\")\n\ndemo = gr.Interface(lambda s: f\"Hello {s}!\", \"textbox\", \"textbox\")\n\napp = gr.mount_gradio_app(app, demo, path=\"/demo\", auth_dependency=get_user)\n\nif __name__ == '__main__':\n    uvicorn.run(app)\n```\n\nIn this example, only requests that include a \"user\" header will be allowed to access the Gradio app. Of course, this does not add much security, since any user can add this header in their request.\n\nHere's a more complete example showing how to add Google OAuth to a Gradio app (assuming you've already created OAuth Credentials on the [Google Developer Console](https://console.cloud.google.com/project)):\n\n```python\nimport os\nfrom authlib.integrations.starlette_client import OAuth, OAuthError\nfrom fastapi import FastAPI, Depends, Request\nfrom starlette.config import Config\nfrom starlette.responses import RedirectResponse\nfrom starlette.middleware.sessions import SessionMiddleware\nimport uvicorn\nimport gradio as gr\n\napp = FastAPI()\n\n# Replace these with your own OAuth settings\nGOOGLE_CLIENT_ID = \"...\"\nGOOGLE_CLIENT_SECRET = \"...\"\nSECRET_KEY = \"...\"\n\nconfig_data = {'GOOGLE_CLIENT_ID': GOOGLE_CLIENT_ID, 'GOOGLE_CLIENT_SECRET': GOOGLE_CLIENT_SECRET}\nstarlette_config = Config(environ=config_data)\noauth = OAuth(starlette_config)\noauth.register(\n    name='google',\n    server_metadata_url='https://accounts.google.com/.well-known/openid-configuration',\n    client_kwargs={'scope': 'openid email profile'},\n)\n\nSECRET_KEY = os.environ.get('SECRET_KEY') or \"a_very_secret_key\"\napp.add_middleware(SessionMiddleware, secret_key=SECRET_KEY)\n\n# Dependency to get the current user\ndef get_user(request: Request):\n    user = request.session.get('user')\n    if user:\n        return user['name']\n    return None\n\n@app.get('/')\ndef public(user: dict = Depends(get_user)):\n    if user:\n        return RedirectResponse(url='/gradio')\n    else:\n        return RedirectResponse(url='/login-demo')\n\n@app.route('/logout')\nasync def logout(request: Request):\n    request.session.pop('user', None)\n    return RedirectResponse(url='/')\n\n@app.route('/login')\nasync def login(request: Request):\n    redirect_uri = request.url_for('auth')\n    # If your app is running on https, you should ensure that the\n    # `redirect_uri` is https, e.g. uncomment the following lines:\n    # \n    # from urllib.parse import urlparse, urlunparse\n    # redirect_uri = urlunparse(urlparse(str(redirect_uri))._replace(scheme='https'))\n    return await oauth.google.authorize_redirect(request, redirect_uri)\n\n@app.route('/auth')\nasync def auth(request: Request):\n    try:\n        access_token = await oauth.google.authorize_access_token(request)\n    except OAuthError:\n        return RedirectResponse(url='/')\n    request.session['user'] = dict(access_token)[\"userinfo\"]\n    return RedirectResponse(url='/')\n\nwith gr.Blocks() as login_demo:\n    gr.Button(\"Login\", link=\"/login\")\n\napp = gr.mount_gradio_app(app, login_demo, path=\"/login-demo\")\n\ndef greet(request: gr.Request):\n    return f\"Welcome to Gradio, {request.username}\"\n\nwith gr.Blocks() as main_demo:\n    m = gr.Markdown(\"Welcome to Gradio!\")\n    gr.Button(\"Logout\", link=\"/logout\")\n    main_demo.load(greet, None, m)\n\napp = gr.mount_gradio_app(app, main_demo, path=\"/gradio\", auth_dependency=get_user)\n\nif __name__ == '__main__':\n    uvicorn.run(app)\n```\n\nThere are actually two separate Gradio apps in this example! One that simply displays a log in button (this demo is accessible to any user), while the other main demo is only accessible to users that are logged in. You can try this example out on [this Space](https://huggingface.co/spaces/gradio/oauth-example).\n\n\n\n## Security and File Access\n\nSharing your Gradio app with others (by hosting it on Spaces, on your own server, or through temporary share links) **exposes** certain files on the host machine to users of your Gradio app.\n\nIn particular, Gradio apps ALLOW users to access to four kinds of files:\n\n- **Temporary files created by Gradio.** These are files that are created by Gradio as part of running your prediction function. For example, if your prediction function returns a video file, then Gradio will save that video to a temporary cache on your device and then send the path to the file to the front end. You can customize the location of temporary cache files created by Gradio by setting the environment variable `GRADIO_TEMP_DIR` to an absolute path, such as `/home/usr/scripts/project/temp/`. You can delete the files created by your app when it shuts down with the `delete_cache` parameter of `gradio.Blocks`, `gradio.Interface`, and `gradio.ChatInterface`. This parameter is a tuple of integers of the form `[frequency, age]` where `frequency` is how often to delete files and `age` is the time in seconds since the file was last modified.\n\n\n- **Cached examples created by Gradio.** These are files that are created by Gradio as part of caching examples for faster runtimes, if you set `cache_examples=True` or `cache_examples=\"lazy\"` in `gr.Interface()`, `gr.ChatInterface()` or in `gr.Examples()`. By default, these files are saved in the `gradio_cached_examples/` subdirectory within your app's working directory. You can customize the location of cached example files created by Gradio by setting the environment variable `GRADIO_EXAMPLES_CACHE` to an absolute path or a path relative to your working directory.\n\n- **Files that you explicitly allow via the `allowed_paths` parameter in `launch()`**. This parameter allows you to pass in a list of additional directories or exact filepaths you'd like to allow users to have access to. (By default, this parameter is an empty list).\n\n- **Static files that you explicitly set via the `gr.set_static_paths` function**. This parameter allows you to pass in a list of directories or filenames that will be considered static. This means that they will not be copied to the cache and will be served directly from your computer. This can help save disk space and reduce the time your app takes to launch but be mindful of possible security implications.\n\nGradio DOES NOT ALLOW access to:\n\n- **Files that you explicitly block via the `blocked_paths` parameter in `launch()`**. You can pass in a list of additional directories or exact filepaths to the `blocked_paths` parameter in `launch()`. This parameter takes precedence over the files that Gradio exposes by default or by the `allowed_paths`.\n\n- **Any other paths on the host machine**. Users should NOT be able to access other arbitrary paths on the host.\n\nSharing your Gradio application will also allow users to upload files to your computer or server. You can set a maximum file size for uploads to prevent abuse and to preserve disk space. You can do this with the `max_file_size` parameter of `.launch`. For example, the following two code snippets limit file uploads to 5 megabytes per file.\n\n```python\nimport gradio as gr\n\ndemo = gr.Interface(lambda x: x, \"image\", \"image\")\n\ndemo.launch(max_file_size=\"5mb\")\n# or\ndemo.launch(max_file_size=5 * gr.FileSize.MB)\n```\n\nPlease make sure you are running the latest version of `gradio` for these security settings to apply.\n\n## Analytics\n\nBy default, Gradio collects certain analytics to help us better understand the usage of the `gradio` library. This includes the following information:\n\n* What environment the Gradio app is running on (e.g. Colab Notebook, Hugging Face Spaces)\n* What input/output components are being used in the Gradio app\n* Whether the Gradio app is utilizing certain advanced features, such as `auth` or `show_error` \n* The IP address which is used solely to measure the number of unique developers using Gradio \n* The version of Gradio that is running \n\nNo information is collected from _users_ of your Gradio app. If you'd like to diable analytics altogether, you can do so by setting the `analytics_enabled` parameter to `False` in `gr.Blocks`, `gr.Interface`, or `gr.ChatInterface`. Or, you can set the GRADIO_ANALYTICS_ENABLED environment variable to `\"False\"` to apply this to all Gradio apps created across your system.\n\n*Note*: this reflects the analytics policy as of `gradio>=4.32.0`. \n",tags:[],spaces:[],url:"/guides/sharing-your-app/",contributor:null}]},{category:"Building With Blocks",guides:[{name:"blocks-and-event-listeners",category:"building-with-blocks",pretty_category:"Building With Blocks",guide_index:1,absolute_index:15,pretty_name:"Blocks And Event Listeners",content:"# Blocks and Event Listeners\n\nWe briefly descirbed the Blocks class in the [Quickstart](/main/guides/quickstart#custom-demos-with-gr-blocks) as a way to build custom demos. Let's dive deeper. \n\n\n## Blocks Structure\n\nTake a look at the demo below.\n\n```python\nimport gradio as gr\n\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\n\nwith gr.Blocks() as demo:\n    name = gr.Textbox(label=\"Name\")\n    output = gr.Textbox(label=\"Output Box\")\n    greet_btn = gr.Button(\"Greet\")\n    greet_btn.click(fn=greet, inputs=name, outputs=output, api_name=\"greet\")\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/hello_blocks'>\u003C/gradio-app>\n\n- First, note the `with gr.Blocks() as demo:` clause. The Blocks app code will be contained within this clause.\n- Next come the Components. These are the same Components used in `Interface`. However, instead of being passed to some constructor, Components are automatically added to the Blocks as they are created within the `with` clause.\n- Finally, the `click()` event listener. Event listeners define the data flow within the app. In the example above, the listener ties the two Textboxes together. The Textbox `name` acts as the input and Textbox `output` acts as the output to the `greet` method. This dataflow is triggered when the Button `greet_btn` is clicked. Like an Interface, an event listener can take multiple inputs or outputs.\n\nYou can also attach event listeners using decorators - skip the `fn` argument and assign `inputs` and `outputs` directly:\n\n```python\nimport gradio as gr\n\n\nwith gr.Blocks() as demo:\n    name = gr.Textbox(label=\"Name\")\n    output = gr.Textbox(label=\"Output Box\")\n    greet_btn = gr.Button(\"Greet\")\n\n    @greet_btn.click(inputs=name, outputs=output)\n    def greet(name):\n        return \"Hello \" + name + \"!\"\n\n   \n\ndemo.launch()\n```\n\n## Event Listeners and Interactivity\n\nIn the example above, you'll notice that you are able to edit Textbox `name`, but not Textbox `output`. This is because any Component that acts as an input to an event listener is made interactive. However, since Textbox `output` acts only as an output, Gradio determines that it should not be made interactive. You can override the default behavior and directly configure the interactivity of a Component with the boolean `interactive` keyword argument.\n\n```python\noutput = gr.Textbox(label=\"Output\", interactive=True)\n```\n\n_Note_: What happens if a Gradio component is neither an input nor an output? If a component is constructed with a default value, then it is presumed to be displaying content and is rendered non-interactive. Otherwise, it is rendered interactive. Again, this behavior can be overridden by specifying a value for the `interactive` argument.\n\n## Types of Event Listeners\n\nTake a look at the demo below:\n\n```python\nimport gradio as gr\n\ndef welcome(name):\n    return f\"Welcome to Gradio, {name}!\"\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\n    \"\"\"\n    # Hello World!\n    Start typing below to see the output.\n    \"\"\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n    inp.change(welcome, inp, out)\n\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/blocks_hello'>\u003C/gradio-app>\n\nInstead of being triggered by a click, the `welcome` function is triggered by typing in the Textbox `inp`. This is due to the `change()` event listener. Different Components support different event listeners. For example, the `Video` Component supports a `play()` event listener, triggered when a user presses play. See the [Docs](http://gradio.app/docs#components) for the event listeners for each Component.\n\n## Multiple Data Flows\n\nA Blocks app is not limited to a single data flow the way Interfaces are. Take a look at the demo below:\n\n```python\nimport gradio as gr\n\ndef increase(num):\n    return num + 1\n\nwith gr.Blocks() as demo:\n    a = gr.Number(label=\"a\")\n    b = gr.Number(label=\"b\")\n    atob = gr.Button(\"a > b\")\n    btoa = gr.Button(\"b > a\")\n    atob.click(increase, a, b)\n    btoa.click(increase, b, a)\n\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/reversible_flow'>\u003C/gradio-app>\n\nNote that `num1` can act as input to `num2`, and also vice-versa! As your apps get more complex, you will have many data flows connecting various Components.\n\nHere's an example of a \"multi-step\" demo, where the output of one model (a speech-to-text model) gets fed into the next model (a sentiment classifier).\n\n```python\nfrom transformers import pipeline\n\nimport gradio as gr\n\nasr = pipeline(\"automatic-speech-recognition\", \"facebook/wav2vec2-base-960h\")\nclassifier = pipeline(\"text-classification\")\n\n\ndef speech_to_text(speech):\n    text = asr(speech)[\"text\"]\n    return text\n\n\ndef text_to_sentiment(text):\n    return classifier(text)[0][\"label\"]\n\n\ndemo = gr.Blocks()\n\nwith demo:\n    audio_file = gr.Audio(type=\"filepath\")\n    text = gr.Textbox()\n    label = gr.Label()\n\n    b1 = gr.Button(\"Recognize Speech\")\n    b2 = gr.Button(\"Classify Sentiment\")\n\n    b1.click(speech_to_text, inputs=audio_file, outputs=text)\n    b2.click(text_to_sentiment, inputs=text, outputs=label)\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/blocks_speech_text_sentiment'>\u003C/gradio-app>\n\n## Function Input List vs Dict\n\nThe event listeners you've seen so far have a single input component. If you'd like to have multiple input components pass data to the function, you have two options on how the function can accept input component values:\n\n1. as a list of arguments, or\n2. as a single dictionary of values, keyed by the component\n\nLet's see an example of each:\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    a = gr.Number(label=\"a\")\n    b = gr.Number(label=\"b\")\n    with gr.Row():\n        add_btn = gr.Button(\"Add\")\n        sub_btn = gr.Button(\"Subtract\")\n    c = gr.Number(label=\"sum\")\n\n    def add(num1, num2):\n        return num1 + num2\n    add_btn.click(add, inputs=[a, b], outputs=c)\n\n    def sub(data):\n        return data[a] - data[b]\n    sub_btn.click(sub, inputs={a, b}, outputs=c)\n\n\ndemo.launch()\n```\n\nBoth `add()` and `sub()` take `a` and `b` as inputs. However, the syntax is different between these listeners.\n\n1. To the `add_btn` listener, we pass the inputs as a list. The function `add()` takes each of these inputs as arguments. The value of `a` maps to the argument `num1`, and the value of `b` maps to the argument `num2`.\n2. To the `sub_btn` listener, we pass the inputs as a set (note the curly brackets!). The function `sub()` takes a single dictionary argument `data`, where the keys are the input components, and the values are the values of those components.\n\nIt is a matter of preference which syntax you prefer! For functions with many input components, option 2 may be easier to manage.\n\n\u003Cgradio-app space='gradio/calculator_list_and_dict'>\u003C/gradio-app>\n\n## Function Return List vs Dict\n\nSimilarly, you may return values for multiple output components either as:\n\n1. a list of values, or\n2. a dictionary keyed by the component\n\nLet's first see an example of (1), where we set the values of two output components by returning two values:\n\n```python\nwith gr.Blocks() as demo:\n    food_box = gr.Number(value=10, label=\"Food Count\")\n    status_box = gr.Textbox()\n    def eat(food):\n        if food > 0:\n            return food - 1, \"full\"\n        else:\n            return 0, \"hungry\"\n    gr.Button(\"EAT\").click(\n        fn=eat,\n        inputs=food_box,\n        outputs=[food_box, status_box]\n    )\n```\n\nAbove, each return statement returns two values corresponding to `food_box` and `status_box`, respectively.\n\nInstead of returning a list of values corresponding to each output component in order, you can also return a dictionary, with the key corresponding to the output component and the value as the new value. This also allows you to skip updating some output components.\n\n```python\nwith gr.Blocks() as demo:\n    food_box = gr.Number(value=10, label=\"Food Count\")\n    status_box = gr.Textbox()\n    def eat(food):\n        if food > 0:\n            return {food_box: food - 1, status_box: \"full\"}\n        else:\n            return {status_box: \"hungry\"}\n    gr.Button(\"EAT\").click(\n        fn=eat,\n        inputs=food_box,\n        outputs=[food_box, status_box]\n    )\n```\n\nNotice how when there is no food, we only update the `status_box` element. We skipped updating the `food_box` component.\n\nDictionary returns are helpful when an event listener affects many components on return, or conditionally affects outputs and not others.\n\nKeep in mind that with dictionary returns, we still need to specify the possible outputs in the event listener.\n\n## Updating Component Configurations\n\nThe return value of an event listener function is usually the updated value of the corresponding output Component. Sometimes we want to update the configuration of the Component as well, such as the visibility. In this case, we return a new Component, setting the properties we want to change.\n\n```python\nimport gradio as gr\n\n\ndef change_textbox(choice):\n    if choice == \"short\":\n        return gr.Textbox(lines=2, visible=True)\n    elif choice == \"long\":\n        return gr.Textbox(lines=8, visible=True, value=\"Lorem ipsum dolor sit amet\")\n    else:\n        return gr.Textbox(visible=False)\n\n\nwith gr.Blocks() as demo:\n    radio = gr.Radio(\n        [\"short\", \"long\", \"none\"], label=\"What kind of essay would you like to write?\"\n    )\n    text = gr.Textbox(lines=2, interactive=True, show_copy_button=True)\n    radio.change(fn=change_textbox, inputs=radio, outputs=text)\n\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/blocks_essay_simple'>\u003C/gradio-app>\n\nSee how we can configure the Textbox itself through a new `gr.Textbox()` method. The `value=` argument can still be used to update the value along with Component configuration. Any arguments we do not set will use their previous values.\n\n## Examples\n\nJust like with `gr.Interface`, you can also add examples for your functions when you are working with `gr.Blocks`. In this case, instantiate a `gr.Examples` similar to how you would instantiate any other component. The constructor of `gr.Examples` takes two required arguments:\n\n* `examples`: a nested list of examples, in which the outer list consists of examples and each inner list consists of an input corresponding to each input component\n* `inputs`: the component or list of components that should be populated when the examples are clicked\n\nYou can also set `cache_examples=True` similar to `gr.Interface`, in which case two additional arguments must be provided:\n\n* `outputs`: the component or list of components corresponding to the output of the examples\n* `fn`: the function to run to generate the outputs corresponding to the examples\n\nHere's an example showing how to use `gr.Examples` in a `gr.Blocks` app:\n\n```python\nimport gradio as gr\n\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        return num1 / num2\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            num_1 = gr.Number(value=4)\n            operation = gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"])\n            num_2 = gr.Number(value=0)\n            submit_btn = gr.Button(value=\"Calculate\")\n        with gr.Column():\n            result = gr.Number()\n\n    submit_btn.click(\n        calculator, inputs=[num_1, operation, num_2], outputs=[result], api_name=False\n    )\n    examples = gr.Examples(\n        examples=[\n            [5, \"add\", 3],\n            [4, \"divide\", 2],\n            [-4, \"multiply\", 2.5],\n            [0, \"subtract\", 1.2],\n        ],\n        inputs=[num_1, operation, num_2],\n    )\n\nif __name__ == \"__main__\":\n    demo.launch(show_api=False)\n\n```\n\n**Note**: In Gradio 4.0 or later, when you click on examples, not only does the value of the input component update to the example value, but the component's configuration also reverts to the properties with which you constructed the component. This ensures that the examples are compatible with the component even if its configuration has been changed. \n\n\n\n## Running Events Consecutively\n\nYou can also run events consecutively by using the `then` method of an event listener. This will run an event after the previous event has finished running. This is useful for running events that update components in multiple steps.\n\nFor example, in the chatbot example below, we first update the chatbot with the user message immediately, and then update the chatbot with the computer response after a simulated delay.\n\n```python\nimport gradio as gr\nimport random\nimport time\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    msg = gr.Textbox()\n    clear = gr.Button(\"Clear\")\n\n    def user(user_message, history):\n        return \"\", history + [[user_message, None]]\n\n    def bot(history):\n        bot_message = random.choice([\"How are you?\", \"I love you\", \"I'm very hungry\"])\n        time.sleep(2)\n        history[-1][1] = bot_message\n        return history\n\n    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n        bot, chatbot, chatbot\n    )\n    clear.click(lambda: None, None, chatbot, queue=False)\n    \ndemo.queue()\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/chatbot_consecutive'>\u003C/gradio-app>\n\nThe `.then()` method of an event listener executes the subsequent event regardless of whether the previous event raised any errors. If you'd like to only run subsequent events if the previous event executed successfully, use the `.success()` method, which takes the same arguments as `.then()`.\n\n## Running Events Continuously\n\nYou can run events on a fixed schedule using the `every` parameter of the event listener. This will run the event `every` number of seconds while the client connection is open. If the connection is closed, the event will stop running after the following iteration. Note that this does not take into account the runtime of the event itself. So a function with a 1 second runtime running with `every=5`, would actually run every 6 seconds. Also note that this parameter does not apply to the `js` function, only the Python function associated with the event listener.\n\nHere is an example of a sine curve that updates every second!\n\n```python\nimport math\nimport gradio as gr\nimport plotly.express as px\nimport numpy as np\n\n\nplot_end = 2 * math.pi\n\n\ndef get_plot(period=1):\n    global plot_end\n    x = np.arange(plot_end - 2 * math.pi, plot_end, 0.02)\n    y = np.sin(2*math.pi*period * x)\n    fig = px.line(x=x, y=y)\n    plot_end += 2 * math.pi\n    if plot_end > 1000:\n        plot_end = 2 * math.pi\n    return fig\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            gr.Markdown(\"Change the value of the slider to automatically update the plot\")\n            period = gr.Slider(label=\"Period of plot\", value=1, minimum=0, maximum=10, step=1)\n            plot = gr.Plot(label=\"Plot (updates every half second)\")\n\n    dep = demo.load(get_plot, None, plot, every=1)\n    period.change(get_plot, period, plot, every=1, cancels=[dep])\n\n\nif __name__ == \"__main__\":\n    demo.queue().launch()\n\n```\n\u003Cgradio-app space='gradio/sine_curve'>\u003C/gradio-app>\n\n## Gathering Event Data\n\nYou can gather specific data about an event by adding the associated event data class as a type hint to an argument in the event listener function.\n\nFor example, event data for `.select()` can be type hinted by a `gradio.SelectData` argument. This event is triggered when a user selects some part of the triggering component, and the event data includes information about what the user specifically selected. If a user selected a specific word in a `Textbox`, a specific image in a `Gallery`, or a specific cell in a `DataFrame`, the event data argument would contain information about the specific selection.\n\nIn the 2 player tic-tac-toe demo below, a user can select a cell in the `DataFrame` to make a move. The event data argument contains information about the specific cell that was selected. We can first check to see if the cell is empty, and then update the cell with the user's move.\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    turn = gr.Textbox(\"X\", interactive=False, label=\"Turn\")\n    board = gr.Dataframe(value=[[\"\", \"\", \"\"]] * 3, interactive=False, type=\"array\")\n\n    def place(board, turn, evt: gr.SelectData):\n        if evt.value:\n            return board, turn\n        board[evt.index[0]][evt.index[1]] = turn\n        turn = \"O\" if turn == \"X\" else \"X\"\n        return board, turn\n\n    board.select(place, [board, turn], [board, turn], show_progress=\"hidden\")\n\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/tictactoe'>\u003C/gradio-app>\n\n## Binding Multiple Triggers to a Function\n\nOften times, you may want to bind multiple triggers to the same function. For example, you may want to allow a user to click a submit button, or press enter to submit a form. You can do this using the `gr.on` method and passing a list of triggers to the `trigger`.\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    name = gr.Textbox(label=\"Name\")\n    output = gr.Textbox(label=\"Output Box\")\n    greet_btn = gr.Button(\"Greet\")\n    trigger = gr.Textbox(label=\"Trigger Box\")\n    trigger2 = gr.Textbox(label=\"Trigger Box\")\n\n    def greet(name, evt_data: gr.EventData):\n        return \"Hello \" + name + \"!\", evt_data.target.__class__.__name__\n    \n    def clear_name(evt_data: gr.EventData):\n        return \"\", evt_data.target.__class__.__name__\n    \n    gr.on(\n        triggers=[name.submit, greet_btn.click],\n        fn=greet,\n        inputs=name,\n        outputs=[output, trigger],\n    ).then(clear_name, outputs=[name, trigger2])\n\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/on_listener_basic'>\u003C/gradio-app>\n\nYou can use decorator syntax as well:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    name = gr.Textbox(label=\"Name\")\n    output = gr.Textbox(label=\"Output Box\")\n    greet_btn = gr.Button(\"Greet\")\n\n    @gr.on(triggers=[name.submit, greet_btn.click], inputs=name, outputs=output)\n    def greet(name):\n        return \"Hello \" + name + \"!\"\n\n\ndemo.launch()\n\n```\n\nYou can use `gr.on` to create \"live\" events by binding to the `change` event of components that implement it. If you do not specify any triggers, the function will automatically bind to all `change` event of all input components that include a `change` event (for example `gr.Textbox` has a `change` event whereas `gr.Button` does not).\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        num1 = gr.Slider(1, 10)\n        num2 = gr.Slider(1, 10)\n        num3 = gr.Slider(1, 10)\n    output = gr.Number(label=\"Sum\")\n\n    @gr.on(inputs=[num1, num2, num3], outputs=output)\n    def sum(a, b, c):\n        return a + b + c\n\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/on_listener_live'>\u003C/gradio-app>\n\nYou can follow `gr.on` with `.then`, just like any regular event listener. This handy method should save you from having to write a lot of repetitive code!\n",tags:[],spaces:[],url:"/guides/blocks-and-event-listeners/",contributor:null},{name:"controlling-layout",category:"building-with-blocks",pretty_category:"Building With Blocks",guide_index:2,absolute_index:16,pretty_name:"Controlling Layout",content:"# Controlling Layout\n\nBy default, Components in Blocks are arranged vertically. Let's take a look at how we can rearrange Components. Under the hood, this layout structure uses the [flexbox model of web development](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox).\n\n## Rows\n\nElements within a `with gr.Row` clause will all be displayed horizontally. For example, to display two Buttons side by side:\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        btn1 = gr.Button(\"Button 1\")\n        btn2 = gr.Button(\"Button 2\")\n```\n\nTo make every element in a Row have the same height, use the `equal_height` argument of the `style` method.\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row(equal_height=True):\n        textbox = gr.Textbox()\n        btn2 = gr.Button(\"Button 2\")\n```\n\nThe widths of elements in a Row can be controlled via a combination of `scale` and `min_width` arguments that are present in every Component.\n\n- `scale` is an integer that defines how an element will take up space in a Row. If scale is set to `0`, the element will not expand to take up space. If scale is set to `1` or greater, the element will expand. Multiple elements in a row will expand proportional to their scale. Below, `btn2` will expand twice as much as `btn1`, while `btn0` will not expand at all:\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        btn0 = gr.Button(\"Button 0\", scale=0)\n        btn1 = gr.Button(\"Button 1\", scale=1)\n        btn2 = gr.Button(\"Button 2\", scale=2)\n```\n\n- `min_width` will set the minimum width the element will take. The Row will wrap if there isn't sufficient space to satisfy all `min_width` values.\n\nLearn more about Rows in the [docs](https://gradio.app/docs/row).\n\n## Columns and Nesting\n\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        text1 = gr.Textbox(label=\"t1\")\n        slider2 = gr.Textbox(label=\"s2\")\n        drop3 = gr.Dropdown([\"a\", \"b\", \"c\"], label=\"d3\")\n    with gr.Row():\n        with gr.Column(scale=1, min_width=600):\n            text1 = gr.Textbox(label=\"prompt 1\")\n            text2 = gr.Textbox(label=\"prompt 2\")\n            inbtw = gr.Button(\"Between\")\n            text4 = gr.Textbox(label=\"prompt 1\")\n            text5 = gr.Textbox(label=\"prompt 2\")\n        with gr.Column(scale=2, min_width=600):\n            img1 = gr.Image(\"images/cheetah.jpg\")\n            btn = gr.Button(\"Go\")\n\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/rows_and_columns'>\u003C/gradio-app>\n\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width.\n\nLearn more about Columns in the [docs](https://gradio.app/docs/column).\n\n# Dimensions\n\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specific design requirements. When omitted, Gradio uses default dimensions suited for most use cases.\n\nBelow is an example illustrating the use of viewport width (vw):\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    im = gr.ImageEditor(\n        width=\"50vw\",\n    )\n\ndemo.launch()\n```\n\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\n\n\n```python\nimport gradio as gr\n\ncss = \"\"\"\n.container {\n    height: 100vh;\n}\n\"\"\"\n\nwith gr.Blocks(css=css) as demo:\n    with gr.Column(elem_classes=[\"container\"]):\n        name = gr.Chatbot(value=[[\"1\", \"2\"]], height=\"70%\")\n\ndemo.launch()\n```\n\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height.\n\nYou can apply any valid CSS unit for these parameters. For a comprehensive list of CSS units, refer to [this guide](https://www.w3schools.com/cssref/css_units.php). We recommend you always consider responsiveness and test your interfaces on various screen sizes to ensure a consistent user experience.\n\n\n\n## Tabs and Accordions\n\nYou can also create Tabs using the `with gr.Tab('tab_name'):` clause. Any component created inside of a `with gr.Tab('tab_name'):` context appears in that tab. Consecutive Tab clauses are grouped together so that a single tab can be selected at one time, and only the components within that Tab's context are shown.\n\nFor example:\n\n```python\nimport numpy as np\nimport gradio as gr\n\n\ndef flip_text(x):\n    return x[::-1]\n\n\ndef flip_image(x):\n    return np.fliplr(x)\n\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Flip text or image files using this demo.\")\n    with gr.Tab(\"Flip Text\"):\n        text_input = gr.Textbox()\n        text_output = gr.Textbox()\n        text_button = gr.Button(\"Flip\")\n    with gr.Tab(\"Flip Image\"):\n        with gr.Row():\n            image_input = gr.Image()\n            image_output = gr.Image()\n        image_button = gr.Button(\"Flip\")\n\n    with gr.Accordion(\"Open for More!\", open=False):\n        gr.Markdown(\"Look at me...\")\n        temp_slider = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=0.1,\n            step=0.1,\n            interactive=True,\n            label=\"Slide me\",\n        )\n        temp_slider.change(lambda x: x, [temp_slider])\n\n    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/blocks_flipper'>\u003C/gradio-app>\n\nAlso note the `gr.Accordion('label')` in this example. The Accordion is a layout that can be toggled open or closed. Like `Tabs`, it is a layout element that can selectively hide or show content. Any components that are defined inside of a `with gr.Accordion('label'):` will be hidden or shown when the accordion's toggle icon is clicked.\n\nLearn more about [Tabs](https://gradio.app/docs/tab) and [Accordions](https://gradio.app/docs/accordion) in the docs.\n\n## Visibility\n\nBoth Components and Layout elements have a `visible` argument that can set initially and also updated. Setting `gr.Column(visible=...)` on a Column can be used to show or hide a set of Components.\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    error_box = gr.Textbox(label=\"Error\", visible=False)\n\n    name_box = gr.Textbox(label=\"Name\")\n    age_box = gr.Number(label=\"Age\", minimum=0, maximum=100)\n    symptoms_box = gr.CheckboxGroup([\"Cough\", \"Fever\", \"Runny Nose\"])\n    submit_btn = gr.Button(\"Submit\")\n\n    with gr.Column(visible=False) as output_col:\n        diagnosis_box = gr.Textbox(label=\"Diagnosis\")\n        patient_summary_box = gr.Textbox(label=\"Patient Summary\")\n\n    def submit(name, age, symptoms):\n        if len(name) == 0:\n            return {error_box: gr.Textbox(value=\"Enter name\", visible=True)}\n        return {\n            output_col: gr.Column(visible=True),\n            diagnosis_box: \"covid\" if \"Cough\" in symptoms else \"flu\",\n            patient_summary_box: f\"{name}, {age} y/o\",\n        }\n\n    submit_btn.click(\n        submit,\n        [name_box, age_box, symptoms_box],\n        [error_box, diagnosis_box, patient_summary_box, output_col],\n    )\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/blocks_form'>\u003C/gradio-app>\n\n## Variable Number of Outputs\n\nBy adjusting the visibility of components in a dynamic way, it is possible to create\ndemos with Gradio that support a _variable numbers of outputs_. Here's a very simple example\nwhere the number of output textboxes is controlled by an input slider:\n\n```python\nimport gradio as gr\n\nmax_textboxes = 10\n\ndef variable_outputs(k):\n    k = int(k)\n    return [gr.Textbox(visible=True)]*k + [gr.Textbox(visible=False)]*(max_textboxes-k)\n\nwith gr.Blocks() as demo:\n    s = gr.Slider(1, max_textboxes, value=max_textboxes, step=1, label=\"How many textboxes to show:\")\n    textboxes = []\n    for i in range(max_textboxes):\n        t = gr.Textbox(f\"Textbox {i}\")\n        textboxes.append(t)\n\n    s.change(variable_outputs, s, textboxes)\n\nif __name__ == \"__main__\":\n   demo.launch()\n\n```\n\u003Cgradio-app space='gradio/variable_outputs'>\u003C/gradio-app>\n\n## Defining and Rendering Components Separately\n\nIn some cases, you might want to define components before you actually render them in your UI. For instance, you might want to show an examples section using `gr.Examples` above the corresponding `gr.Textbox` input. Since `gr.Examples` requires as a parameter the input component object, you will need to first define the input component, but then render it later, after you have defined the `gr.Examples` object.\n\nThe solution to this is to define the `gr.Textbox` outside of the `gr.Blocks()` scope and use the component's `.render()` method wherever you'd like it placed in the UI.\n\nHere's a full code example:\n\n```python\ninput_textbox = gr.Textbox()\n\nwith gr.Blocks() as demo:\n    gr.Examples([\"hello\", \"bonjour\", \"merhaba\"], input_textbox)\n    input_textbox.render()\n```\n",tags:[],spaces:[],url:"/guides/controlling-layout/",contributor:null},{name:"state-in-blocks",category:"building-with-blocks",pretty_category:"Building With Blocks",guide_index:3,absolute_index:17,pretty_name:"State In Blocks",content:"# State in Blocks\n\nWe covered [State in Interfaces](https://gradio.app/interface-state), this guide takes a look at state in Blocks, which works mostly the same.\n\n## Global State\n\nGlobal state in Blocks works the same as in Interface. Any variable created outside a function call is a reference shared between all users.\n\n## Session State\n\nGradio supports session **state**, where data persists across multiple submits within a page session, in Blocks apps as well. To reiterate, session data is _not_ shared between different users of your model. To store data in a session state, you need to do three things:\n\n1. Create a `gr.State()` object. If there is a default value to this stateful object, pass that into the constructor.\n2. In the event listener, put the `State` object as an input and output.\n3. In the event listener function, add the variable to the input parameters and the return value.\n\nLet's take a look at a game of hangman.\n\n```python\nimport gradio as gr\n\nsecret_word = \"gradio\"\n\nwith gr.Blocks() as demo:    \n    used_letters_var = gr.State([])\n    with gr.Row() as row:\n        with gr.Column():\n            input_letter = gr.Textbox(label=\"Enter letter\")\n            btn = gr.Button(\"Guess Letter\")\n        with gr.Column():\n            hangman = gr.Textbox(\n                label=\"Hangman\",\n                value=\"_\"*len(secret_word)\n            )\n            used_letters_box = gr.Textbox(label=\"Used Letters\")\n\n    def guess_letter(letter, used_letters):\n        used_letters.append(letter)\n        answer = \"\".join([\n            (letter if letter in used_letters else \"_\")\n            for letter in secret_word\n        ])\n        return {\n            used_letters_var: used_letters,\n            used_letters_box: \", \".join(used_letters),\n            hangman: answer\n        }\n    btn.click(\n        guess_letter, \n        [input_letter, used_letters_var],\n        [used_letters_var, used_letters_box, hangman]\n        )\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/hangman'>\u003C/gradio-app>\n\nLet's see how we do each of the 3 steps listed above in this game:\n\n1. We store the used letters in `used_letters_var`. In the constructor of `State`, we set the initial value of this to `[]`, an empty list.\n2. In `btn.click()`, we have a reference to `used_letters_var` in both the inputs and outputs.\n3. In `guess_letter`, we pass the value of this `State` to `used_letters`, and then return an updated value of this `State` in the return statement.\n\nWith more complex apps, you will likely have many State variables storing session state in a single Blocks app.\n\nLearn more about `State` in the [docs](https://gradio.app/docs/state).\n",tags:[],spaces:[],url:"/guides/state-in-blocks/",contributor:null},{name:"dynamic-apps-with-render-decorator",category:"building-with-blocks",pretty_category:"Building With Blocks",guide_index:4,absolute_index:18,pretty_name:"Dynamic Apps With Render Decorator",content:"# Dynamic Apps with the Render Decorator\n\nThe components and event listeners you define in a Blocks so far have been fixed - once the demo was launched, new components and listeners could not be added, and existing one could not be removed. \n\nThe `@gr.render` decorator introduces the ability to dynamically change this. Let's take a look. \n\n## Dynamic Number of Components\n\nIn the example below, we will create a variable number of Textboxes. When the user edits the input Textbox, we create a Textbox for each letter in the input. Try it out below:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    input_text = gr.Textbox(label=\"input\")\n\n    @gr.render(inputs=input_text)\n    def show_split(text):\n        if len(text) == 0:\n            gr.Markdown(\"## No Input Provided\")\n        else:\n            for letter in text:\n                gr.Textbox(letter)\n\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/render_split_simple'>\u003C/gradio-app>\n\nSee how we can now create a variable number of Textboxes using our custom logic - in this case, a simple `for` loop. The `@gr.render` decorator enables this with the following steps:\n\n1. Create a function and attach the @gr.render decorator to it.\n2. Add the input components to the `inputs=` argument of @gr.render, and create a corresponding argument in your function for each component. This function will automatically re-run on any change to a component.\n3. Add all components inside the function that you want to render based on the inputs.\n\nNow whenever the inputs change, the function re-runs, and replaces the components created from the previous function run with the latest run. Pretty straightforward! Let's add a little more complexity to this app:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    input_text = gr.Textbox(label=\"input\")\n    mode = gr.Radio([\"textbox\", \"button\"], value=\"textbox\")\n\n    @gr.render(inputs=[input_text, mode], triggers=[input_text.submit])\n    def show_split(text, mode):\n        if len(text) == 0:\n            gr.Markdown(\"## No Input Provided\")\n        else:\n            for letter in text:\n                if mode == \"textbox\":\n                    gr.Textbox(letter)\n                else:\n                    gr.Button(letter)\n\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/render_split'>\u003C/gradio-app>\n\nBy default, `@gr.render` re-runs are triggered by the `.load` listener to the app and the `.change` listener to any input component provided. We can override this by explicitly setting the triggers in the decorator, as we have in this app to only trigger on `input_text.submit` instead. \nIf you are setting custom triggers, and you also want an automatic render at the start of the app, make sure to add `demo.load` to your list of triggers.\n\n## Dynamic Event Listeners\n\nIf you're creating components, you probably want to attach event listeners to them as well. Let's take a look at an example that takes in a variable number of Textbox as input, and merges all the text into a single box.\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    text_count = gr.State(1)\n    add_btn = gr.Button(\"Add Box\")\n    add_btn.click(lambda x: x + 1, text_count, text_count)\n\n    @gr.render(inputs=text_count)\n    def render_count(count):\n        boxes = []\n        for i in range(count):\n            box = gr.Textbox(key=i, label=f\"Box {i}\")\n            boxes.append(box)\n\n        def merge(*args):\n            return \" \".join(args)\n        \n        merge_btn.click(merge, boxes, output)\n\n\n    merge_btn = gr.Button(\"Merge\")\n    output = gr.Textbox(label=\"Merged Output\")\n\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/render_merge_simple'>\u003C/gradio-app>\n\nLet's take a look at what's happening here:\n\n1. The state variable `text_count` is keeping track of the number of Textboxes to create. By clicking on the Add button, we increase `text_count` which triggers the render decorator.\n2. Note that in every single Textbox we create in the render function, we explicitly set a `key=` argument. This key allows us to preserve the value of this Component between re-renders. If you type in a value in a textbox, and then click the Add button, all the Textboxes re-render, but their values aren't cleared because the `key=` maintains the the value of a Component across a render.\n3. We've stored the Textboxes created in a list, and provide this list as input to the merge button event listener. Note that **all event listeners that use Components created inside a render function must also be defined inside that render function**. The event listener can still reference Components outside the render function, as we do here by referencing `merge_btn` and `output` which are both defined outside the render function.\n\nJust as with Components, whenever a function re-renders, the event listeners created from the previous render are cleared and the new event listeners from the latest run are attached. \n\nThis allows us to create highly customizable and complex interactions! \n\n## Putting it Together\n\nLet's look at two examples that use all the features above. First, try out the to-do list app below: \n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    \n    tasks = gr.State([])\n    new_task = gr.Textbox(label=\"Task Name\", autofocus=True)\n\n    def add_task(tasks, new_task_name):\n        return tasks + [{\"name\": new_task_name, \"complete\": False}], \"\"\n\n    new_task.submit(add_task, [tasks, new_task], [tasks, new_task])\n\n    @gr.render(inputs=tasks)\n    def render_todos(task_list):\n        complete = [task for task in task_list if task[\"complete\"]]\n        incomplete = [task for task in task_list if not task[\"complete\"]]\n        gr.Markdown(f\"### Incomplete Tasks ({len(incomplete)})\")\n        for task in incomplete:\n            with gr.Row():\n                gr.Textbox(task['name'], show_label=False, container=False)\n                done_btn = gr.Button(\"Done\", scale=0)\n                def mark_done(task=task):\n                    task[\"complete\"] = True\n                    return task_list\n                done_btn.click(mark_done, None, [tasks])\n\n                delete_btn = gr.Button(\"Delete\", scale=0, variant=\"stop\")\n                def delete(task=task):\n                    task_list.remove(task)\n                    return task_list\n                delete_btn.click(delete, None, [tasks])\n\n        gr.Markdown(f\"### Complete Tasks ({len(complete)})\")\n        for task in complete:\n            gr.Textbox(task['name'], show_label=False, container=False)\n\n\n\n\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/todo_list'>\u003C/gradio-app>\n\nNote that almost the entire app is inside a single `gr.render` that reacts to the tasks `gr.State` variable. This variable is a nested list, which presents some complexity. If you design a `gr.render` to react to a list or dict structure, ensure you do the following:\n\n1. Any event listener that modifies a state variable in a manner that should trigger a re-render must set the state variable as an output. This lets Gradio know to check if the variable has changed behind the scenes. \n2. In a `gr.render`, if a variable in a loop is used inside an event listener function, that variable should be \"frozen\" via setting it to itself as a default argument in the function header. See how we have `task=task` in both `mark_done` and `delete`. This freezes the variable to its \"loop-time\" value.\n\nLet's take a look at one last example that uses everything we learned. Below is an audio mixer. Provide multiple audio tracks and mix them together.\n\n```python\nimport gradio as gr\nimport numpy as np\n\nwith gr.Blocks() as demo:\n    track_count = gr.State(1)\n    add_track_btn = gr.Button(\"Add Track\")\n\n    add_track_btn.click(lambda count: count + 1, track_count, track_count)\n\n    @gr.render(inputs=track_count)\n    def render_tracks(count):\n        audios = []\n        volumes = []\n        with gr.Row():\n            for i in range(count):\n                with gr.Column(variant=\"panel\", min_width=200):\n                    gr.Textbox(placeholder=\"Track Name\", key=f\"name-{i}\", show_label=False)\n                    track_audio = gr.Audio(label=f\"Track {i}\", key=f\"track-{i}\")\n                    track_volume = gr.Slider(0, 100, value=100, label=\"Volume\", key=f\"volume-{i}\")\n                    audios.append(track_audio)\n                    volumes.append(track_volume)\n\n            def merge(data):\n                sr, output = None, None\n                for audio, volume in zip(audios, volumes):\n                    sr, audio_val = data[audio]\n                    volume_val = data[volume]\n                    final_track = audio_val * (volume_val / 100)\n                    if output is None:\n                        output = final_track\n                    else:\n                        min_shape = tuple(min(s1, s2) for s1, s2 in zip(output.shape, final_track.shape))\n                        trimmed_output = output[:min_shape[0], ...][:, :min_shape[1], ...] if output.ndim > 1 else output[:min_shape[0]]\n                        trimmed_final = final_track[:min_shape[0], ...][:, :final_track[1], ...] if final_track.ndim > 1 else final_track[:min_shape[0]]\n                        output += trimmed_output + trimmed_final\n                return (sr, output)\n            \n            merge_btn.click(merge, set(audios + volumes), output_audio)\n\n    merge_btn = gr.Button(\"Merge Tracks\")\n    output_audio = gr.Audio(label=\"Output\", interactive=False)\n                    \ndemo.launch()\n```\n\u003Cgradio-app space='gradio/audio_mixer'>\u003C/gradio-app>\n\nTwo things to not in this app:\n1. Here we provide `key=` to all the components! We need to do this so that if we add another track after setting the values for an existing track, our input values to the existing track do not get reset on re-render.\n2. When there are lots of components of different types and arbitrary counts passed to an event listener, it is easier to use the set and dictionary notation for inputs rather than list notation. Above, we make one large set of all the input `gr.Audio` and `gr.Slider` components when we pass the inputs to the `merge` function. In the function body we query the component values as a dict.\n\nThe `gr.render` expands gradio capabilities extensively - see what you can make out of it! \n",tags:[],spaces:[],url:"/guides/dynamic-apps-with-render-decorator/",contributor:null},{name:"custom-CSS-and-JS",category:"building-with-blocks",pretty_category:"Building With Blocks",guide_index:5,absolute_index:19,pretty_name:"Custom CSS And JS",content:"# Customizing your demo with CSS and Javascript\n\nGradio allows you to customize your demo in several ways. You can customize the layout of your demo, add custom HTML, and add custom theming as well. This tutorial will go beyond that and walk you through how to add custom CSS and JavaScript code to your demo in order to add custom styling, animations, custom UI functionality, analytics, and more.\n\n## Adding custom CSS to your demo\n\nGradio themes are the easiest way to customize the look and feel of your app. You can choose from a variety of themes, or create your own. To do so, pass the `theme=` kwarg to the `Blocks` constructor. For example:\n\n```python\nwith gr.Blocks(theme=gr.themes.Glass()):\n    ...\n```\n\nGradio comes with a set of prebuilt themes which you can load from `gr.themes.*`. You can extend these themes or create your own themes from scratch - see the [Theming guide](/guides/theming-guide) for more details.\n\nFor additional styling ability, you can pass any CSS to your app using the `css=` kwarg. You can either the filepath to a CSS file, or a string of CSS code.\n\n**Warning**: The use of query selectors in custom JS and CSS is _not_ guaranteed to work across Gradio versions as the Gradio HTML DOM may change. We recommend using query selectors sparingly.\n\nThe base class for the Gradio app is `gradio-container`, so here's an example that changes the background color of the Gradio app:\n\n```python\nwith gr.Blocks(css=\".gradio-container {background-color: red}\") as demo:\n    ...\n```\n\nIf you'd like to reference external files in your css, preface the file path (which can be a relative or absolute path) with `\"file=\"`, for example:\n\n```python\nwith gr.Blocks(css=\".gradio-container {background: url('file=clouds.jpg')}\") as demo:\n    ...\n```\n\nNote: By default, files in the host machine are not accessible to users running the Gradio app. As a result, you should make sure that any referenced files (such as `clouds.jpg` here) are either URLs or allowed via the `allow_list` parameter in `launch()`. Read more in our [section on Security and File Access](/guides/sharing-your-app#security-and-file-access).\n\n\n## The `elem_id` and `elem_classes` Arguments\n\nYou can `elem_id` to add an HTML element `id` to any component, and `elem_classes` to add a class or list of classes. This will allow you to select elements more easily with CSS. This approach is also more likely to be stable across Gradio versions as built-in class names or ids may change (however, as mentioned in the warning above, we cannot guarantee complete compatibility between Gradio versions if you use custom CSS as the DOM elements may themselves change).\n\n```python\ncss = \"\"\"\n#warning {background-color: #FFCCCB}\n.feedback textarea {font-size: 24px !important}\n\"\"\"\n\nwith gr.Blocks(css=css) as demo:\n    box1 = gr.Textbox(value=\"Good Job\", elem_classes=\"feedback\")\n    box2 = gr.Textbox(value=\"Failure\", elem_id=\"warning\", elem_classes=\"feedback\")\n```\n\nThe CSS `#warning` ruleset will only target the second Textbox, while the `.feedback` ruleset will target both. Note that when targeting classes, you might need to put the `!important` selector to override the default Gradio styles.\n\n## Adding custom JavaScript to your demo\n\nThere are 3 ways to add javascript code to your Gradio demo:\n\n1. You can add JavaScript code as a string or as a filepath to the `js` parameter of the `Blocks` or `Interface` initializer. This will run the JavaScript code when the demo is first loaded.\n\nBelow is an example of adding custom js to show an animated welcome message when the demo first loads.\n\n```python\nimport gradio as gr\n\ndef welcome(name):\n    return f\"Welcome to Gradio, {name}!\"\n\njs = \"\"\"\nfunction createGradioAnimation() {\n    var container = document.createElement('div');\n    container.id = 'gradio-animation';\n    container.style.fontSize = '2em';\n    container.style.fontWeight = 'bold';\n    container.style.textAlign = 'center';\n    container.style.marginBottom = '20px';\n\n    var text = 'Welcome to Gradio!';\n    for (var i = 0; i \u003C text.length; i++) {\n        (function(i){\n            setTimeout(function(){\n                var letter = document.createElement('span');\n                letter.style.opacity = '0';\n                letter.style.transition = 'opacity 0.5s';\n                letter.innerText = text[i];\n\n                container.appendChild(letter);\n\n                setTimeout(function() {\n                    letter.style.opacity = '1';\n                }, 50);\n            }, i * 250);\n        })(i);\n    }\n\n    var gradioContainer = document.querySelector('.gradio-container');\n    gradioContainer.insertBefore(container, gradioContainer.firstChild);\n\n    return 'Animation created';\n}\n\"\"\"\nwith gr.Blocks(js=js) as demo:\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n    inp.change(welcome, inp, out)\n\ndemo.launch()\n```\n\u003Cgradio-app space='gradio/blocks_js_load'>\u003C/gradio-app>\n\nNote: You can also supply your custom js code as a file path. For example, if you have a file called `custom.js` in the same directory as your Python script, you can add it to your demo like so: `with gr.Blocks(js=\"custom.js\") as demo:`. Same goes for `Interface` (ex: `gr.Interface(..., js=\"custom.js\")`).\n\n2. When using `Blocks` and event listeners, events have a `js` argument that can take a JavaScript function as a string and treat it just like a Python event listener function. You can pass both a JavaScript function and a Python function (in which case the JavaScript function is run first) or only Javascript (and set the Python `fn` to `None`). Take a look at the code below:\n   \n```python\nimport gradio as gr\n\nblocks = gr.Blocks()\n\nwith blocks as demo:\n    subject = gr.Textbox(placeholder=\"subject\")\n    verb = gr.Radio([\"ate\", \"loved\", \"hated\"])\n    object = gr.Textbox(placeholder=\"object\")\n\n    with gr.Row():\n        btn = gr.Button(\"Create sentence.\")\n        reverse_btn = gr.Button(\"Reverse sentence.\")\n        foo_bar_btn = gr.Button(\"Append foo\")\n        reverse_then_to_the_server_btn = gr.Button(\n            \"Reverse sentence and send to server.\"\n        )\n\n    def sentence_maker(w1, w2, w3):\n        return f\"{w1} {w2} {w3}\"\n\n    output1 = gr.Textbox(label=\"output 1\")\n    output2 = gr.Textbox(label=\"verb\")\n    output3 = gr.Textbox(label=\"verb reversed\")\n    output4 = gr.Textbox(label=\"front end process and then send to backend\")\n\n    btn.click(sentence_maker, [subject, verb, object], output1)\n    reverse_btn.click(\n        None, [subject, verb, object], output2, js=\"(s, v, o) => o + ' ' + v + ' ' + s\"\n    )\n    verb.change(lambda x: x, verb, output3, js=\"(x) => [...x].reverse().join('')\")\n    foo_bar_btn.click(None, [], subject, js=\"(x) => x + ' foo'\")\n\n    reverse_then_to_the_server_btn.click(\n        sentence_maker,\n        [subject, verb, object],\n        output4,\n        js=\"(s, v, o) => [s, v, o].map(x => [...x].reverse().join(''))\",\n    )\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/blocks_js_methods'>\u003C/gradio-app>\n\n3. Lastly, you can add JavaScript code to the `head` param of the `Blocks` initializer. This will add the code to the head of the HTML document. For example, you can add Google Analytics to your demo like so:\n\n\n```python\nhead = f\"\"\"\n\u003Cscript async src=\"https://www.googletagmanager.com/gtag/js?id={google_analytics_tracking_id}\">\u003C/script>\n\u003Cscript>\n  window.dataLayer = window.dataLayer || [];\n  function gtag(){{dataLayer.push(arguments);}}\n  gtag('js', new Date());\n  gtag('config', '{google_analytics_tracking_id}');\n\u003C/script>\n\"\"\"\n\nwith gr.Blocks(head=head) as demo:\n    ...demo code...\n```\n\nThe `head` parameter accepts any HTML tags you would normally insert into the `\u003Chead>` of a page. For example, you can also include `\u003Cmeta>` tags to `head`.\n\nNote that injecting custom HTML can affect browser behavior and compatibility (e.g. keyboard shortcuts). You should test your interface across different browsers and be mindful of how scripts may interact with browser defaults.\nHere's an example where pressing `Shift + s` triggers the `click` event of a specific `Button` component if the browser focus is _not_ on an input component (e.g. `Textbox` component):\n\n```python\nimport gradio as gr\n\nshortcut_js = \"\"\"\n\u003Cscript>\nfunction shortcuts(e) {\n    var event = document.all ? window.event : e;\n    switch (e.target.tagName.toLowerCase()) {\n        case \"input\":\n        case \"textarea\":\n        break;\n        default:\n        if (e.key.toLowerCase() == \"s\" && e.shiftKey) {\n            document.getElementById(\"my_btn\").click();\n        }\n    }\n}\ndocument.addEventListener('keypress', shortcuts, false);\n\u003C/script>\n\"\"\"\n\nwith gr.Blocks(head=shortcut_js) as demo:\n    action_button = gr.Button(value=\"Name\", elem_id=\"my_btn\")\n    textbox = gr.Textbox()\n    action_button.click(lambda : \"button pressed\", None, textbox)\n    \ndemo.launch()\n```\n",tags:[],spaces:[],url:"/guides/custom-CSS-and-JS/",contributor:null},{name:"using-blocks-like-functions",category:"building-with-blocks",pretty_category:"Building With Blocks",guide_index:6,absolute_index:20,pretty_name:"Using Blocks Like Functions",content:"# Using Gradio Blocks Like Functions\n\n\n\n**Prerequisite**: This Guide builds on the Blocks Introduction. Make sure to [read that guide first](https://gradio.app/blocks-and-event-listeners).\n\n## Introduction\n\nDid you know that apart from being a full-stack machine learning demo, a Gradio Blocks app is also a regular-old python function!?\n\nThis means that if you have a gradio Blocks (or Interface) app called `demo`, you can use `demo` like you would any python function.\n\nSo doing something like `output = demo(\"Hello\", \"friend\")` will run the first event defined in `demo` on the inputs \"Hello\" and \"friend\" and store it\nin the variable `output`.\n\nIf I put you to sleep ü•±, please bear with me! By using apps like functions, you can seamlessly compose Gradio apps.\nThe following section will show how.\n\n## Treating Blocks like functions\n\nLet's say we have the following demo that translates english text to german text.\n\n```python\nimport gradio as gr\n\nfrom transformers import pipeline\n\npipe = pipeline(\"translation\", model=\"t5-base\")\n\n\ndef translate(text):\n    return pipe(text)[0][\"translation_text\"]\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            english = gr.Textbox(label=\"English text\")\n            translate_btn = gr.Button(value=\"Translate\")\n        with gr.Column():\n            german = gr.Textbox(label=\"German Text\")\n\n    translate_btn.click(translate, inputs=english, outputs=german, api_name=\"translate-to-german\")\n    examples = gr.Examples(examples=[\"I went to the supermarket yesterday.\", \"Helen is a good swimmer.\"],\n                           inputs=[english])\n\ndemo.launch()\n```\n\nI already went ahead and hosted it in Hugging Face spaces at [gradio/english_translator](https://huggingface.co/spaces/gradio/english_translator).\n\nYou can see the demo below as well:\n\n\u003Cgradio-app space='gradio/english_translator'>\u003C/gradio-app>\n\nNow, let's say you have an app that generates english text, but you wanted to additionally generate german text.\n\nYou could either:\n\n1. Copy the source code of my english-to-german translation and paste it in your app.\n\n2. Load my english-to-german translation in your app and treat it like a normal python function.\n\nOption 1 technically always works, but it often introduces unwanted complexity.\n\nOption 2 lets you borrow the functionality you want without tightly coupling our apps.\n\nAll you have to do is call the `Blocks.load` class method in your source file.\nAfter that, you can use my translation app like a regular python function!\n\nThe following code snippet and demo shows how to use `Blocks.load`.\n\nNote that the variable `english_translator` is my english to german app, but its used in `generate_text` like a regular function.\n\n```python\nimport gradio as gr\n\nfrom transformers import pipeline\n\nenglish_translator = gr.load(name=\"spaces/gradio/english_translator\")\nenglish_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n\n\ndef generate_text(text):\n    english_text = english_generator(text)[0][\"generated_text\"]\n    german_text = english_translator(english_text)\n    return english_text, german_text\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            seed = gr.Text(label=\"Input Phrase\")\n        with gr.Column():\n            english = gr.Text(label=\"Generated English Text\")\n            german = gr.Text(label=\"Generated German Text\")\n    btn = gr.Button(\"Generate\")\n    btn.click(generate_text, inputs=[seed], outputs=[english, german])\n    gr.Examples([\"My name is Clara and I am\"], inputs=[seed])\n\ndemo.launch()\n```\n\n\u003Cgradio-app space='gradio/generate_english_german'>\u003C/gradio-app>\n\n## How to control which function in the app to use\n\nIf the app you are loading defines more than one function, you can specify which function to use\nwith the `fn_index` and `api_name` parameters.\n\nIn the code for our english to german demo, you'll see the following line:\n\n```python\ntranslate_btn.click(translate, inputs=english, outputs=german, api_name=\"translate-to-german\")\n```\n\nThe `api_name` gives this function a unique name in our app. You can use this name to tell gradio which\nfunction in the upstream space you want to use:\n\n```python\nenglish_generator(text, api_name=\"translate-to-german\")[0][\"generated_text\"]\n```\n\nYou can also use the `fn_index` parameter.\nImagine my app also defined an english to spanish translation function.\nIn order to use it in our text generation app, we would use the following code:\n\n```python\nenglish_generator(text, fn_index=1)[0][\"generated_text\"]\n```\n\nFunctions in gradio spaces are zero-indexed, so since the spanish translator would be the second function in my space,\nyou would use index 1.\n\n## Parting Remarks\n\nWe showed how treating a Blocks app like a regular python helps you compose functionality across different apps.\nAny Blocks app can be treated like a function, but a powerful pattern is to `load` an app hosted on\n[Hugging Face Spaces](https://huggingface.co/spaces) prior to treating it like a function in your own app.\nYou can also load models hosted on the [Hugging Face Model Hub](https://huggingface.co/models) - see the [Using Hugging Face Integrations](/using_hugging_face_integrations) guide for an example.\n\n### Happy building! ‚öíÔ∏è\n",tags:["TRANSLATION","HUB","SPACES"],spaces:[],url:"/guides/using-blocks-like-functions/",contributor:null}]},{category:"Chatbots",guides:[{name:"creating-a-chatbot-fast",category:"chatbots",pretty_category:"Chatbots",guide_index:1,absolute_index:21,pretty_name:"Creating A Chatbot Fast",content:"# How to Create a Chatbot with Gradio\n\n\n\n## Introduction\n\nChatbots are a popular application of large language models. Using `gradio`, you can easily build a demo of your chatbot model and share that with your users, or try it yourself using an intuitive chatbot UI.\n\nThis tutorial uses `gr.ChatInterface()`, which is a high-level abstraction that allows you to create your chatbot UI fast, often with a single line of code. The chatbot interface that we create will look something like this:\n\n\u003Cgradio-app space='gradio/chatinterface_streaming_echo'>\u003C/gradio-app>\n\nWe'll start with a couple of simple examples, and then show how to use `gr.ChatInterface()` with real language models from several popular APIs and libraries, including `langchain`, `openai`, and Hugging Face.\n\n**Prerequisites**: please make sure you are using the **latest version** version of Gradio:\n\n```bash\n$ pip install --upgrade gradio\n```\n\n## Defining a chat function\n\nWhen working with `gr.ChatInterface()`, the first thing you should do is define your chat function. Your chat function should take two arguments: `message` and then `history` (the arguments can be named anything, but must be in this order).\n\n- `message`: a `str` representing the user's input.\n- `history`: a `list` of `list` representing the conversations up until that point. Each inner list consists of two `str` representing a pair: `[user input, bot response]`.\n\nYour function should return a single string response, which is the bot's response to the particular user input `message`. Your function can take into account the `history` of messages, as well as the current message.\n\nLet's take a look at a few examples.\n\n## Example: a chatbot that responds yes or no\n\nLet's write a chat function that responds `Yes` or `No` randomly.\n\nHere's our chat function:\n\n```python\nimport random\n\ndef random_response(message, history):\n    return random.choice([\"Yes\", \"No\"])\n```\n\nNow, we can plug this into `gr.ChatInterface()` and call the `.launch()` method to create the web interface:\n\n```python\nimport gradio as gr\n\ngr.ChatInterface(random_response).launch()\n```\n\nThat's it! Here's our running demo, try it out:\n\n\u003Cgradio-app space='gradio/chatinterface_random_response'>\u003C/gradio-app>\n\n## Another example using the user's input and history\n\nOf course, the previous example was very simplistic, it didn't even take user input or the previous history into account! Here's another simple example showing how to incorporate a user's input as well as the history.\n\n```python\nimport random\nimport gradio as gr\n\ndef alternatingly_agree(message, history):\n    if len(history) % 2 == 0:\n        return f\"Yes, I do think that '{message}'\"\n    else:\n        return \"I don't think so\"\n\ngr.ChatInterface(alternatingly_agree).launch()\n```\n\n## Streaming chatbots\n\nIn your chat function, you can use `yield` to generate a sequence of partial responses, each replacing the previous ones. This way, you'll end up with a streaming chatbot. It's that simple!\n\n```python\nimport time\nimport gradio as gr\n\ndef slow_echo(message, history):\n    for i in range(len(message)):\n        time.sleep(0.3)\n        yield \"You typed: \" + message[: i+1]\n\ngr.ChatInterface(slow_echo).launch()\n```\n\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> While the response is streaming, the \"Submit\" button turns into a \"Stop\" button that can be used to stop the generator function. You can customize the appearance and behavior of the \"Stop\" button using the `stop_btn` parameter.\u003C/p>\n\n## Customizing your chatbot\n\nIf you're familiar with Gradio's `Interface` class, the `gr.ChatInterface` includes many of the same arguments that you can use to customize the look and feel of your Chatbot. For example, you can:\n\n- add a title and description above your chatbot using `title` and `description` arguments.\n- add a theme or custom css using `theme` and `css` arguments respectively.\n- add `examples` and even enable `cache_examples`, which make it easier for users to try it out .\n- You can change the text or disable each of the buttons that appear in the chatbot interface: `submit_btn`, `retry_btn`, `undo_btn`, `clear_btn`.\n\nIf you want to customize the `gr.Chatbot` or `gr.Textbox` that compose the `ChatInterface`, then you can pass in your own chatbot or textbox as well. Here's an example of how we can use these parameters:\n\n```python\nimport gradio as gr\n\ndef yes_man(message, history):\n    if message.endswith(\"?\"):\n        return \"Yes\"\n    else:\n        return \"Ask me anything!\"\n\ngr.ChatInterface(\n    yes_man,\n    chatbot=gr.Chatbot(height=300),\n    textbox=gr.Textbox(placeholder=\"Ask me a yes or no question\", container=False, scale=7),\n    title=\"Yes Man\",\n    description=\"Ask Yes Man any question\",\n    theme=\"soft\",\n    examples=[\"Hello\", \"Am I cool?\", \"Are tomatoes vegetables?\"],\n    cache_examples=True,\n    retry_btn=None,\n    undo_btn=\"Delete Previous\",\n    clear_btn=\"Clear\",\n).launch()\n```\n\nIn particular, if you'd like to add a \"placeholder\" for your chat interface, which appears before the user has started chatting, you can do so using the `placeholder` argument of `gr.Chatbot`, which accepts Markdown or HTML. \n\n```python\ngr.ChatInterface(\n    yes_man,\n    chatbot=gr.Chatbot(placeholder=\"\u003Cstrong>Your Personal Yes-Man\u003C/strong>\u003Cbr>Ask Me Anything\"),\n...\n```\n\nThe placeholder appears vertically and horizontally centered in the chatbot.\n\n## Add Multimodal Capability to your chatbot\n\nYou may want to add multimodal capability to your chatbot. For example, you may want users to be able to easily upload images or files to your chatbot and ask questions about it. You can make your chatbot \"multimodal\" by passing in a single parameter (`multimodal=True`) to the `gr.ChatInterface` class.\n\n\n```python\nimport gradio as gr\nimport time\n\ndef count_files(message, history):\n    num_files = len(message[\"files\"])\n    return f\"You uploaded {num_files} files\"\n\ndemo = gr.ChatInterface(fn=count_files, examples=[{\"text\": \"Hello\", \"files\": []}], title=\"Echo Bot\", multimodal=True)\n\ndemo.launch()\n```\n\nWhen `multimodal=True`, the signature of `fn` changes slightly. The first parameter of your function should accept a dictionary consisting of the submitted text and uploaded files that looks like this: `{\"text\": \"user input\", \"file\": [\"file_path1\", \"file_path2\", ...]}`. Similarly, any examples you provide should be in a dictionary of this form. Your function should still return a single `str` message. \u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> If you'd like to customize the UI/UX of the textbox for your multimodal chatbot, you should pass in an instance of `gr.MultimodalTextbox` to the `textbox` argument of `ChatInterface` instead of an instance of `gr.Textbox`.\u003C/p>\n\n## Additional Inputs\n\nYou may want to add additional parameters to your chatbot and expose them to your users through the Chatbot UI. For example, suppose you want to add a textbox for a system prompt, or a slider that sets the number of tokens in the chatbot's response. The `ChatInterface` class supports an `additional_inputs` parameter which can be used to add additional input components.\n\nThe `additional_inputs` parameters accepts a component or a list of components. You can pass the component instances directly, or use their string shortcuts (e.g. `\"textbox\"` instead of `gr.Textbox()`). If you pass in component instances, and they have _not_ already been rendered, then the components will appear underneath the chatbot (and any examples) within a `gr.Accordion()`. You can set the label of this accordion using the `additional_inputs_accordion_name` parameter.\n\nHere's a complete example:\n\n```python\nimport gradio as gr\nimport time\n\n\ndef echo(message, history, system_prompt, tokens):\n    response = f\"System prompt: {system_prompt}\\n Message: {message}.\"\n    for i in range(min(len(response), int(tokens))):\n        time.sleep(0.05)\n        yield response[: i + 1]\n\n\ndemo = gr.ChatInterface(\n    echo,\n    additional_inputs=[\n        gr.Textbox(\"You are helpful AI.\", label=\"System Prompt\"),\n        gr.Slider(10, 100),\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.queue().launch()\n\n```\n\nIf the components you pass into the `additional_inputs` have already been rendered in a parent `gr.Blocks()`, then they will _not_ be re-rendered in the accordion. This provides flexibility in deciding where to lay out the input components. In the example below, we position the `gr.Textbox()` on top of the Chatbot UI, while keeping the slider underneath.\n\n```python\nimport gradio as gr\nimport time\n\ndef echo(message, history, system_prompt, tokens):\n    response = f\"System prompt: {system_prompt}\\n Message: {message}.\"\n    for i in range(min(len(response), int(tokens))):\n        time.sleep(0.05)\n        yield response[: i+1]\n\nwith gr.Blocks() as demo:\n    system_prompt = gr.Textbox(\"You are helpful AI.\", label=\"System Prompt\")\n    slider = gr.Slider(10, 100, render=False)\n\n    gr.ChatInterface(\n        echo, additional_inputs=[system_prompt, slider]\n    )\n\ndemo.launch()\n```\n\nIf you need to create something even more custom, then its best to construct the chatbot UI using the low-level `gr.Blocks()` API. We have [a dedicated guide for that here](/guides/creating-a-custom-chatbot-with-blocks).\n\n## Using your chatbot via an API\n\nOnce you've built your Gradio chatbot and are hosting it on [Hugging Face Spaces](https://hf.space) or somewhere else, then you can query it with a simple API at the `/chat` endpoint. The endpoint just expects the user's message (and potentially additional inputs if you have set any using the `additional_inputs` parameter), and will return the response, internally keeping track of the messages sent so far.\n\n[](https://github.com/gradio-app/gradio/assets/1778297/7b10d6db-6476-4e2e-bebd-ecda802c3b8f)\n\nTo use the endpoint, you should use either the [Gradio Python Client](/guides/getting-started-with-the-python-client) or the [Gradio JS client](/guides/getting-started-with-the-js-client).\n\n## A `langchain` example\n\nNow, let's actually use the `gr.ChatInterface` with some real large language models. We'll start by using `langchain` on top of `openai` to build a general-purpose streaming chatbot application in 19 lines of code. You'll need to have an OpenAI key for this example (keep reading for the free, open-source equivalent!)\n\n```python\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import AIMessage, HumanMessage\nimport openai\nimport gradio as gr\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # Replace with your key\n\nllm = ChatOpenAI(temperature=1.0, model='gpt-3.5-turbo-0613')\n\ndef predict(message, history):\n    history_langchain_format = []\n    for human, ai in history:\n        history_langchain_format.append(HumanMessage(content=human))\n        history_langchain_format.append(AIMessage(content=ai))\n    history_langchain_format.append(HumanMessage(content=message))\n    gpt_response = llm(history_langchain_format)\n    return gpt_response.content\n\ngr.ChatInterface(predict).launch()\n```\n\n## A streaming example using `openai`\n\nOf course, we could also use the `openai` library directy. Here a similar example, but this time with streaming results as well:\n\n```python\nfrom openai import OpenAI\nimport gradio as gr\n\napi_key = \"sk-...\"  # Replace with your key\nclient = OpenAI(api_key=api_key)\n\ndef predict(message, history):\n    history_openai_format = []\n    for human, assistant in history:\n        history_openai_format.append({\"role\": \"user\", \"content\": human })\n        history_openai_format.append({\"role\": \"assistant\", \"content\":assistant})\n    history_openai_format.append({\"role\": \"user\", \"content\": message})\n  \n    response = client.chat.completions.create(model='gpt-3.5-turbo',\n    messages= history_openai_format,\n    temperature=1.0,\n    stream=True)\n\n    partial_message = \"\"\n    for chunk in response:\n        if chunk.choices[0].delta.content is not None:\n              partial_message = partial_message + chunk.choices[0].delta.content\n              yield partial_message\n\ngr.ChatInterface(predict).launch()\n```\n\n## Example using a local, open-source LLM with Hugging Face\n\nOf course, in many cases you want to run a chatbot locally. Here's the equivalent example using Together's RedePajama model, from Hugging Face (this requires you to have a GPU with CUDA).\n\n```python\nimport gradio as gr\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\nfrom threading import Thread\n\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\", torch_dtype=torch.float16)\nmodel = model.to('cuda:0')\n\nclass StopOnTokens(StoppingCriteria):\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        stop_ids = [29, 0]\n        for stop_id in stop_ids:\n            if input_ids[0][-1] == stop_id:\n                return True\n        return False\n\ndef predict(message, history):\n    history_transformer_format = history + [[message, \"\"]]\n    stop = StopOnTokens()\n\n    messages = \"\".join([\"\".join([\"\\n\u003Chuman>:\"+item[0], \"\\n\u003Cbot>:\"+item[1]])\n                for item in history_transformer_format])\n\n    model_inputs = tokenizer([messages], return_tensors=\"pt\").to(\"cuda\")\n    streamer = TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)\n    generate_kwargs = dict(\n        model_inputs,\n        streamer=streamer,\n        max_new_tokens=1024,\n        do_sample=True,\n        top_p=0.95,\n        top_k=1000,\n        temperature=1.0,\n        num_beams=1,\n        stopping_criteria=StoppingCriteriaList([stop])\n        )\n    t = Thread(target=model.generate, kwargs=generate_kwargs)\n    t.start()\n\n    partial_message = \"\"\n    for new_token in streamer:\n        if new_token != '\u003C':\n            partial_message += new_token\n            yield partial_message\n\ngr.ChatInterface(predict).launch()\n```\n\nWith those examples, you should be all set to create your own Gradio Chatbot demos soon! For building even more custom Chatbot applications, check out [a dedicated guide](/guides/creating-a-custom-chatbot-with-blocks) using the low-level `gr.Blocks()` API.\n",tags:["NLP","TEXT","CHAT"],spaces:[],url:"/guides/creating-a-chatbot-fast/",contributor:null},{name:"creating-a-custom-chatbot-with-blocks",category:"chatbots",pretty_category:"Chatbots",guide_index:2,absolute_index:22,pretty_name:"Creating A Custom Chatbot With Blocks",content:"# How to Create a Custom Chatbot with Gradio Blocks\n\n\n\n\n## Introduction\n\n**Important Note**: if you are getting started, we recommend using the `gr.ChatInterface` to create chatbots -- its a high-level abstraction that makes it possible to create beautiful chatbot applications fast, often with a single line of code. [Read more about it here](/guides/creating-a-chatbot-fast).\n\nThis tutorial will show how to make chatbot UIs from scratch with Gradio's low-level Blocks API. This will give you full control over your Chatbot UI. You'll start by first creating a a simple chatbot to display text, a second one to stream text responses, and finally a chatbot that can handle media files as well. The chatbot interface that we create will look something like this:\n\n\u003Cgradio-app space='gradio/chatbot_streaming'>\u003C/gradio-app>\n\n**Prerequisite**: We'll be using the `gradio.Blocks` class to build our Chatbot demo.\nYou can [read the Guide to Blocks first](https://gradio.app/blocks-and-event-listeners) if you are not already familiar with it. Also please make sure you are using the **latest version** version of Gradio: `pip install --upgrade gradio`.\n\n## A Simple Chatbot Demo\n\nLet's start with recreating the simple demo above. As you may have noticed, our bot simply randomly responds \"How are you?\", \"I love you\", or \"I'm very hungry\" to any input. Here's the code to create this with Gradio:\n\n```python\nimport gradio as gr\nimport random\nimport time\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    msg = gr.Textbox()\n    clear = gr.ClearButton([msg, chatbot])\n\n    def respond(message, chat_history):\n        bot_message = random.choice([\"How are you?\", \"I love you\", \"I'm very hungry\"])\n        chat_history.append((message, bot_message))\n        time.sleep(2)\n        return \"\", chat_history\n\n    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n\ndemo.launch()\n\n```\n\nThere are three Gradio components here:\n\n- A `Chatbot`, whose value stores the entire history of the conversation, as a list of response pairs between the user and bot.\n- A `Textbox` where the user can type their message, and then hit enter/submit to trigger the chatbot response\n- A `ClearButton` button to clear the Textbox and entire Chatbot history\n\nWe have a single function, `respond()`, which takes in the entire history of the chatbot, appends a random message, waits 1 second, and then returns the updated chat history. The `respond()` function also clears the textbox when it returns.\n\nOf course, in practice, you would replace `respond()` with your own more complex function, which might call a pretrained model or an API, to generate a response.\n\n\u003Cgradio-app space='gradio/chatbot_simple'>\u003C/gradio-app>\n\n## Add Streaming to your Chatbot\n\nThere are several ways we can improve the user experience of the chatbot above. First, we can stream responses so the user doesn't have to wait as long for a message to be generated. Second, we can have the user message appear immediately in the chat history, while the chatbot's response is being generated. Here's the code to achieve that:\n\n```python\nimport gradio as gr\nimport random\nimport time\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    msg = gr.Textbox()\n    clear = gr.Button(\"Clear\")\n\n    def user(user_message, history):\n        return \"\", history + [[user_message, None]]\n\n    def bot(history):\n        bot_message = random.choice([\"How are you?\", \"I love you\", \"I'm very hungry\"])\n        history[-1][1] = \"\"\n        for character in bot_message:\n            history[-1][1] += character\n            time.sleep(0.05)\n            yield history\n\n    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n        bot, chatbot, chatbot\n    )\n    clear.click(lambda: None, None, chatbot, queue=False)\n    \ndemo.queue()\ndemo.launch()\n\n```\n\nYou'll notice that when a user submits their message, we now _chain_ three event events with `.then()`:\n\n1. The first method `user()` updates the chatbot with the user message and clears the input field. This method also makes the input field non interactive so that the user can't send another message while the chatbot is responding. Because we want this to happen instantly, we set `queue=False`, which would skip any queue had it been enabled. The chatbot's history is appended with `(user_message, None)`, the `None` signifying that the bot has not responded.\n\n2. The second method, `bot()` updates the chatbot history with the bot's response. Instead of creating a new message, we just replace the previously-created `None` message with the bot's response. Finally, we construct the message character by character and `yield` the intermediate outputs as they are being constructed. Gradio automatically turns any function with the `yield` keyword [into a streaming output interface](/guides/key-features/#iterative-outputs).\n\n3. The third method makes the input field interactive again so that users can send another message to the bot.\n\nOf course, in practice, you would replace `bot()` with your own more complex function, which might call a pretrained model or an API, to generate a response.\n\nFinally, we enable queuing by running `demo.queue()`, which is required for streaming intermediate outputs. You can try the improved chatbot by scrolling to the demo at the top of this page.\n\n## Liking / Disliking Chat Messages\n\nOnce you've created your `gr.Chatbot`, you can add the ability for users to like or dislike messages. This can be useful if you would like users to vote on a bot's responses or flag inappropriate results. \n\nTo add this functionality to your Chatbot, simply attach a `.like()` event to your Chatbot. A chatbot that has the `.like()` event will automatically feature a thumbs-up icon and a thumbs-down icon next to every bot message. \n\nThe `.like()` method requires you to pass in a function that is called when a user clicks on these icons. In your function, you should have an argument whose type is `gr.LikeData`. Gradio will automatically supply the parameter to this argument with an object that contains information about the liked or disliked message. Here's a simplistic example of how you can have users like or dislike chat messages:\n\n```py\nimport gradio as gr\n\ndef greet(history, input):\n    return history + [(input, \"Hello, \" + input)]\n\ndef vote(data: gr.LikeData):\n    if data.liked:\n        print(\"You upvoted this response: \" + data.value)\n    else:\n        print(\"You downvoted this response: \" + data.value)\n    \n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    textbox = gr.Textbox()\n    textbox.submit(greet, [chatbot, textbox], [chatbot])\n    chatbot.like(vote, None, None)  # Adding this line causes the like/dislike icons to appear in your chatbot\n    \ndemo.launch()\n```\n\n## Adding Markdown, Images, Audio, or Videos\n\nThe `gr.Chatbot` component supports a subset of markdown including bold, italics, and code. For example, we could write a function that responds to a user's message, with a bold **That's cool!**, like this:\n\n```py\ndef bot(history):\n    response = \"**That's cool!**\"\n    history[-1][1] = response\n    return history\n```\n\nIn addition, it can handle media files, such as images, audio, and video. You can use the `MultimodalTextbox` component to easily upload all types of media files to your chatbot. To pass in a media file, we must pass in the file as a tuple of two strings, like this: `(filepath, alt_text)`. The `alt_text` is optional, so you can also just pass in a tuple with a single element `(filepath,)`, like this:\n\n```python\ndef add_message(history, message):\n    for x in message[\"files\"]:\n        history.append(((x[\"path\"],), None))  \n    if message[\"text\"] is not None:\n        history.append((message[\"text\"], None))\n    return history, gr.MultimodalTextbox(value=None, interactive=False, file_types=[\"image\"])\n```\n\nPutting this together, we can create a _multimodal_ chatbot with a multimodal textbox for a user to submit text and media files. The rest of the code looks pretty much the same as before:\n\n```python\nimport gradio as gr\nimport os\nimport time\n\n# Chatbot demo with multimodal input (text, markdown, LaTeX, code blocks, image, audio, & video). Plus shows support for streaming text.\n\n\ndef print_like_dislike(x: gr.LikeData):\n    print(x.index, x.value, x.liked)\n\ndef add_message(history, message):\n    for x in message[\"files\"]:\n        history.append(((x,), None))\n    if message[\"text\"] is not None:\n        history.append((message[\"text\"], None))\n    return history, gr.MultimodalTextbox(value=None, interactive=False)\n\ndef bot(history):\n    response = \"**That's cool!**\"\n    history[-1][1] = \"\"\n    for character in response:\n        history[-1][1] += character\n        time.sleep(0.05)\n        yield history\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot(\n        [],\n        elem_id=\"chatbot\",\n        bubble_full_width=False\n    )\n\n    chat_input = gr.MultimodalTextbox(interactive=True, file_types=[\"image\"], placeholder=\"Enter message or upload file...\", show_label=False)\n\n    chat_msg = chat_input.submit(add_message, [chatbot, chat_input], [chatbot, chat_input])\n    bot_msg = chat_msg.then(bot, chatbot, chatbot, api_name=\"bot_response\")\n    bot_msg.then(lambda: gr.MultimodalTextbox(interactive=True), None, [chat_input])\n\n    chatbot.like(print_like_dislike, None, None)\n\ndemo.queue()\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/chatbot_multimodal'>\u003C/gradio-app>\n\nAnd you're done! That's all the code you need to build an interface for your chatbot model. Finally, we'll end our Guide with some links to Chatbots that are running on Spaces so that you can get an idea of what else is possible:\n\n- [project-baize/Baize-7B](https://huggingface.co/spaces/project-baize/Baize-7B): A stylized chatbot that allows you to stop generation as well as regenerate responses.\n- [MAGAer13/mPLUG-Owl](https://huggingface.co/spaces/MAGAer13/mPLUG-Owl): A multimodal chatbot that allows you to upvote and downvote responses.\n",tags:["NLP","TEXT","CHAT"],spaces:["https://huggingface.co/spaces/gradio/chatbot_streaming","https://huggingface.co/spaces/project-baize/Baize-7B,"],url:"/guides/creating-a-custom-chatbot-with-blocks/",contributor:null},{name:"creating-a-discord-bot-from-a-gradio-app",category:"chatbots",pretty_category:"Chatbots",guide_index:3,absolute_index:23,pretty_name:"Creating A Discord Bot From A Gradio App",content:"# üöÄ Creating Discord Bots from Gradio Apps üöÄ\n\n\n\nWe're excited to announce that Gradio can now automatically create a discord bot from a deployed app! ü§ñ\n\nDiscord is a popular communication platform that allows users to chat and interact with each other in real-time. By turning your Gradio app into a Discord bot, you can bring cutting edge AI to your discord server and give your community a whole new way to interact.\n\n## üíª How does it work? üíª\n\nWith `gradio_client` version `0.3.0`, any gradio `ChatInterface` app on the internet can automatically be deployed as a discord bot via the `deploy_discord` method of the `Client` class.\n\nTechnically, any gradio app that exposes an api route that takes in a single string and outputs a single string can be deployed to discord. In this guide, we will focus on `gr.ChatInterface` as those apps naturally lend themselves to discord's chat functionality.\n\n## üõ†Ô∏è Requirements üõ†Ô∏è\n\nMake sure you have the latest `gradio_client` and `gradio` versions installed.\n\n```bash\npip install gradio_client>=0.3.0 gradio>=3.38.0\n```\n\nAlso, make sure you have a [Hugging Face account](https://huggingface.co/) and a [write access token](https://huggingface.co/docs/hub/security-tokens).\n\n‚ö†Ô∏è Tip ‚ö†Ô∏è: Make sure you login to the Hugging Face Hub by running `huggingface-cli login`. This will let you skip passing your token in all subsequent commands in this guide.\n\n## üèÉ‚Äç‚ôÄÔ∏è Quickstart üèÉ‚Äç‚ôÄÔ∏è\n\n### Step 1: Implementing our chatbot\n\nLet's build a very simple Chatbot using `ChatInterface` that simply repeats the user message. Write the following code into an `app.py`\n\n```python\nimport gradio as gr\n\ndef slow_echo(message, history):\n    return message\n\ndemo = gr.ChatInterface(slow_echo).queue().launch()\n```\n\n### Step 2: Deploying our App\n\nIn order to create a discord bot for our app, it must be accessible over the internet. In this guide, we will use the `gradio deploy` command to deploy our chatbot to Hugging Face spaces from the command line. Run the following command.\n\n```bash\ngradio deploy --title echo-chatbot --app-file app.py\n```\n\nThis command will ask you some questions, e.g. requested hardware, requirements, but the default values will suffice for this guide.\nNote the URL of the space that was created. Mine is https://huggingface.co/spaces/freddyaboulton/echo-chatbot\n\n### Step 3: Creating a Discord Bot\n\nTurning our space into a discord bot is also a one-liner thanks to the `gradio deploy-discord`. Run the following command:\n\n```bash\ngradio deploy-discord --src freddyaboulton/echo-chatbot\n```\n\n‚ùóÔ∏è Advanced ‚ùóÔ∏è: If you already have a discord bot token you can pass it to the `deploy-discord` command. Don't worry, if you don't have one yet!\n\n```bash\ngradio deploy-discord --src freddyaboulton/echo-chatbot --discord-bot-token \u003Ctoken>\n```\n\nNote the URL that gets printed out to the console. Mine is https://huggingface.co/spaces/freddyaboulton/echo-chatbot-gradio-discord-bot\n\n### Step 4: Getting a Discord Bot Token\n\nIf you didn't have a discord bot token for step 3, go to the URL that got printed in the console and follow the instructions there.\nOnce you obtain a token, run the command again but this time pass in the token:\n\n```bash\ngradio deploy-discord --src freddyaboulton/echo-chatbot --discord-bot-token \u003Ctoken>\n```\n\n### Step 5: Add the bot to your server\n\nVisit the space of your discord bot. You should see \"Add this bot to your server by clicking this link:\" followed by a URL. Go to that URL and add the bot to your server!\n\n### Step 6: Use your bot!\n\nBy default the bot can be called by starting a message with `/chat`, e.g. `/chat \u003Cyour prompt here>`.\n\n‚ö†Ô∏è Tip ‚ö†Ô∏è: If either of the deployed spaces goes to sleep, the bot will stop working. By default, spaces go to sleep after 48 hours of inactivity. You can upgrade the hardware of your space to prevent it from going to sleep. See this [guide](https://huggingface.co/docs/hub/spaces-gpus#using-gpu-spaces) for more information.\n\n\u003Cimg src=\"https://gradio-builds.s3.amazonaws.com/demo-files/discordbots/guide/echo_slash.gif\">\n\n### Using the `gradio_client.Client` Class\n\nYou can also create a discord bot from a deployed gradio app with python.\n\n```python\nimport gradio_client as grc\ngrc.Client(\"freddyaboulton/echo-chatbot\").deploy_discord()\n```\n\n## ü¶æ Using State of The Art LLMs ü¶æ\n\nWe have created an organization on Hugging Face called [gradio-discord-bots](https://huggingface.co/gradio-discord-bots) containing several template spaces that explain how to deploy state of the art LLMs powered by gradio as discord bots.\n\nThe easiest way to get started is by deploying Meta's Llama 2 LLM with 70 billion parameter. Simply go to this [space](https://huggingface.co/spaces/gradio-discord-bots/Llama-2-70b-chat-hf) and follow the instructions.\n\nThe deployment can be done in one line! ü§Ø\n\n```python\nimport gradio_client as grc\ngrc.Client(\"ysharma/Explore_llamav2_with_TGI\").deploy_discord(to_id=\"llama2-70b-discord-bot\")\n```\n\n## ü¶ú Additional LLMs ü¶ú\n\nIn addition to Meta's 70 billion Llama 2 model, we have prepared template spaces for the following LLMs and deployment options:\n\n- [gpt-3.5-turbo](https://huggingface.co/spaces/gradio-discord-bots/gpt-35-turbo), powered by openai. Required OpenAI key.\n- [falcon-7b-instruct](https://huggingface.co/spaces/gradio-discord-bots/falcon-7b-instruct) powered by Hugging Face Inference Endpoints.\n- [Llama-2-13b-chat-hf](https://huggingface.co/spaces/gradio-discord-bots/Llama-2-13b-chat-hf) powered by Hugging Face Inference Endpoints.\n- [Llama-2-13b-chat-hf](https://huggingface.co/spaces/gradio-discord-bots/llama-2-13b-chat-transformers) powered by Hugging Face transformers.\n\nTo deploy any of these models to discord, simply follow the instructions in the linked space for that model.\n\n## Deploying non-chat gradio apps to discord\n\nAs mentioned above, you don't need a `gr.ChatInterface` if you want to deploy your gradio app to discord. All that's needed is an api route that takes in a single string and outputs a single string.\n\nThe following code will deploy a space that translates english to german as a discord bot.\n\n```python\nimport gradio_client as grc\nclient = grc.Client(\"freddyaboulton/english-to-german\")\nclient.deploy_discord(api_names=['german'])\n```\n\n## Conclusion\n\nThat's it for this guide! We're really excited about this feature. Tag [@Gradio](https://twitter.com/Gradio) on twitter and show us how your discord community interacts with your discord bots.\n",tags:["NLP","TEXT","CHAT"],spaces:[],url:"/guides/creating-a-discord-bot-from-a-gradio-app/",contributor:null}]},{category:"Custom Components",guides:[{name:"custom-components-in-five-minutes",category:"custom-components",pretty_category:"Custom Components",guide_index:1,absolute_index:24,pretty_name:"Custom Components In Five Minutes",content:"# Custom Components in 5 minutes\n\nGradio 4.0 introduces Custom Components -- the ability for developers to create their own custom components and use them in Gradio apps.\nYou can publish your components as Python packages so that other users can use them as well.\nUsers will be able to use all of Gradio's existing functions, such as `gr.Blocks`, `gr.Interface`, API usage, themes, etc. with Custom Components.\nThis guide will cover how to get started making custom components.\n\n## Installation\n\nYou will need to have:\n\n* Python 3.8+ (\u003Ca href=\"https://www.python.org/downloads/\" target=\"_blank\">install here\u003C/a>)\n* pip 21.3+ (`python -m pip install --upgrade pip`)\n* Node.js v16.14+ (\u003Ca href=\"https://nodejs.dev/en/download/package-manager/\" target=\"_blank\">install here\u003C/a>)\n* npm 9+ (\u003Ca href=\"https://docs.npmjs.com/downloading-and-installing-node-js-and-npm/\" target=\"_blank\">install here\u003C/a>)\n* Gradio 4.0+ (`pip install --upgrade gradio`)\n\n## The Workflow\n\nThe Custom Components workflow consists of 4 steps: create, dev, build, and publish.\n\n1. create: creates a template for you to start developing a custom component.\n2. dev: launches a development server with a sample app & hot reloading allowing you to easily develop your custom component\n3. build: builds a python package containing to your custom component's Python and JavaScript code -- this makes things official!\n4. publish: uploads your package to [PyPi](https://pypi.org/) and/or a sample app to [HuggingFace Spaces](https://hf.co/spaces).\n\nEach of these steps is done via the Custom Component CLI. You can invoke it with `gradio cc` or `gradio component`\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> Run `gradio cc --help` to get a help menu of all available commands. There are some commands that are not covered in this guide. You can also append `--help` to any command name to bring up a help page for that command, e.g. `gradio cc create --help`.\u003C/p>\n\n## 1. create\n\nBootstrap a new template by running the following in any working directory:\n\n```bash\ngradio cc create MyComponent --template SimpleTextbox\n```\n\nInstead of `MyComponent`, give your component any name.\n\nInstead of `SimpleTextbox`, you can use any Gradio component as a template. `SimpleTextbox` is actually a special component that a stripped-down version of the `Textbox` component that makes it particularly useful when creating your first custom component.\nSome other components that are good if you are starting out: `SimpleDropdown`, `SimpleImage`, or `File`.\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> Run `gradio cc show` to get a list of available component templates.\u003C/p>\n\nThe `create` command will:\n\n1. Create a directory with your component's name in lowercase with the following structure:\n```directory\n- backend/ \u003C- The python code for your custom component\n- frontend/ \u003C- The javascript code for your custom component\n- demo/ \u003C- A sample app using your custom component. Modify this to develop your component!\n- pyproject.toml \u003C- Used to build the package and specify package metadata.\n```\n\n2. Install the component in development mode\n\nEach of the directories will have the code you need to get started developing!\n\n## 2. dev\n\nOnce you have created your new component, you can start a development server by `entering the directory` and running\n\n```bash\ngradio cc dev\n```\n\nYou'll see several lines that are printed to the console.\nThe most important one is the one that says:\n\n> Frontend Server (Go here): http://localhost:7861/\n\nThe port number might be different for you.\nClick on that link to launch the demo app in hot reload mode.\nNow, you can start making changes to the backend and frontend you'll see the results reflected live in the sample app!\nWe'll go through a real example in a later guide.\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> You don't have to run dev mode from your custom component directory. The first argument to `dev` mode is the path to the directory. By default it uses the current directory.\u003C/p>\n\n## 3. build\n\nOnce you are satisfied with your custom component's implementation, you can `build` it to use it outside of the development server.\n\nFrom your component directory, run:\n\n```bash\ngradio cc build\n```\n\nThis will create a `tar.gz` and `.whl` file in a `dist/` subdirectory.\nIf you or anyone installs that `.whl` file (`pip install \u003Cpath-to-whl>`) they will be able to use your custom component in any gradio app!\n\nThe `build` command will also generate documentation for your custom component. This takes the form of an interactive space and a static `README.md`. You can disable this by passing `--no-generate-docs`. You can read more about the documentation generator in [the dedicated guide](https://gradio.app/guides/documenting-custom-components).\n\n## 4. publish\n\nRight now, your package is only available on a `.whl` file on your computer.\nYou can share that file with the world with the `publish` command!\n\nSimply run the following command from your component directory:\n\n```bash\ngradio cc publish\n```\n\nThis will guide you through the following process:\n\n1. Upload your distribution files to PyPi. This is optional. If you decide to upload to PyPi, you will need a PyPI username and password. You can get one [here](https://pypi.org/account/register/).\n2. Upload a demo of your component to hugging face spaces. This is also optional.\n\n\nHere is an example of what publishing looks like:\n\n\u003Cvideo autoplay muted loop>\n  \u003Csource src=\"https://gradio-builds.s3.amazonaws.com/assets/text_with_attachments_publish.mov\" type=\"video/mp4\" />\n\u003C/video>\n\n\n## Conclusion\n\nNow that you know the high-level workflow of creating custom components, you can go in depth in the next guides!\nAfter reading the guides, check out this [collection](https://huggingface.co/collections/gradio/custom-components-65497a761c5192d981710b12) of custom components on the HuggingFace Hub so you can learn from other's code.\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> If you want to start off from someone else's custom component see this [guide](./frequently-asked-questions#do-i-always-need-to-start-my-component-from-scratch).\u003C/p>\n",tags:[],spaces:[],url:"/guides/custom-components-in-five-minutes/",contributor:null},{name:"key-component-concepts",category:"custom-components",pretty_category:"Custom Components",guide_index:2,absolute_index:25,pretty_name:"Key Component Concepts",content:"# Gradio Components: The Key Concepts\n\nIn this section, we discuss a few important concepts when it comes to components in Gradio.\nIt's important to understand these concepts when developing your own component.\nOtherwise, your component may behave very different to other Gradio components!\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong>  You can skip this section if you are familiar with the internals of the Gradio library, such as each component's preprocess and postprocess methods.\u003C/p>\n\n## Interactive vs Static\n\nEvery component in Gradio comes in a `static` variant, and most come in an `interactive` version as well.\nThe `static` version is used when a component is displaying a value, and the user can **NOT** change that value by interacting with it. \nThe `interactive` version is used when the user is able to change the value by interacting with the Gradio UI.\n\nLet's see some examples:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n   gr.Textbox(value=\"Hello\", interactive=True)\n   gr.Textbox(value=\"Hello\", interactive=False)\n\ndemo.launch()\n\n```\nThis will display two textboxes.\nThe only difference: you'll be able to edit the value of the Gradio component on top, and you won't be able to edit the variant on the bottom (i.e. the textbox will be disabled).\n\nPerhaps a more interesting example is with the `Image` component:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n   gr.Image(interactive=True)\n   gr.Image(interactive=False)\n\ndemo.launch()\n```\n\nThe interactive version of the component is much more complex -- you can upload images or snap a picture from your webcam -- while the static version can only be used to display images.\n\nNot every component has a distinct interactive version. For example, the `gr.AnnotatedImage` only appears as a static version since there's no way to interactively change the value of the annotations or the image.\n\n### What you need to remember\n\n* Gradio will use the interactive version (if available) of a component if that component is used as the **input** to any event; otherwise, the static version will be used.\n\n* When you design custom components, you **must** accept the boolean interactive keyword in the constructor of your Python class. In the frontend, you **may** accept the `interactive` property, a `bool` which represents whether the component should be static or interactive. If you do not use this property in the frontend, the component will appear the same in interactive or static mode.\n\n## The value and how it is preprocessed/postprocessed\n\nThe most important attribute of a component is its `value`.\nEvery component has a `value`.\nThe value that is typically set by the user in the frontend (if the component is interactive) or displayed to the user (if it is static). \nIt is also this value that is sent to the backend function when a user triggers an event, or returned by the user's function e.g. at the end of a prediction.\n\nSo this value is passed around quite a bit, but sometimes the format of the value needs to change between the frontend and backend. \nTake a look at this example:\n\n```python\nimport numpy as np\nimport gradio as gr\n\ndef sepia(input_img):\n    sepia_filter = np.array([\n        [0.393, 0.769, 0.189], \n        [0.349, 0.686, 0.168], \n        [0.272, 0.534, 0.131]\n    ])\n    sepia_img = input_img.dot(sepia_filter.T)\n    sepia_img /= sepia_img.max()\n    return sepia_img\n\ndemo = gr.Interface(sepia, gr.Image(shape=(200, 200)), \"image\")\ndemo.launch()\n```\n\nThis will create a Gradio app which has an `Image` component as the input and the output. \nIn the frontend, the Image component will actually **upload** the file to the server and send the **filepath** but this is converted to a `numpy` array before it is sent to a user's function. \nConversely, when the user returns a `numpy` array from their function, the numpy array is converted to a file so that it can be sent to the frontend and displayed by the `Image` component.\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> By default, the `Image` component sends numpy arrays to the python function because it is a common choice for machine learning engineers, though the Image component also supports other formats using the `type` parameter.  Read the `Image` docs [here](https://www.gradio.app/docs/image) to learn more.\u003C/p>\n\nEach component does two conversions:\n\n1. `preprocess`: Converts the `value` from the format sent by the frontend to the format expected by the python function. This usually involves going from a web-friendly **JSON** structure to a **python-native** data structure, like a `numpy` array or `PIL` image. The `Audio`, `Image` components are good examples of `preprocess` methods.\n\n2. `postprocess`: Converts the value returned by the python function to the format expected by the frontend. This usually involves going from a **python-native** data-structure, like a `PIL` image to a **JSON** structure.\n\n### What you need to remember\n\n* Every component must implement `preprocess` and `postprocess` methods. In the rare event that no conversion needs to happen, simply return the value as-is. `Textbox` and `Number` are examples of this. \n\n* As a component author, **YOU** control the format of the data displayed in the frontend as well as the format of the data someone using your component will receive. Think of an ergonomic data-structure a **python** developer will find intuitive, and control the conversion from a **Web-friendly JSON** data structure (and vice-versa) with `preprocess` and `postprocess.`\n\n## The \"Example Version\" of a Component\n\nGradio apps support providing example inputs -- and these are very useful in helping users get started using your Gradio app. \nIn `gr.Interface`, you can provide examples using the `examples` keyword, and in `Blocks`, you can provide examples using the special `gr.Examples` component.\n\nAt the bottom of this screenshot, we show a miniature example image of a cheetah that, when clicked, will populate the same image in the input Image component:\n\n![img](https://user-images.githubusercontent.com/1778297/277548211-a3cb2133-2ffc-4cdf-9a83-3e8363b57ea6.png)\n\n\nTo enable the example view, you must have the following two files in the top of the `frontend` directory:\n\n* `Example.svelte`: this corresponds to the \"example version\" of your component\n* `Index.svelte`: this corresponds to the \"regular version\"\n\nIn the backend, you typically don't need to do anything. The user-provided example `value` is processed using the same `.postprocess()` method described earlier. If you'd like to do process the data differently (for example, if the `.postprocess()` method is computationally expensive), then you can write your own `.process_example()` method for your custom component, which will be used instead. \n\nThe `Example.svelte` file and `process_example()` method will be covered in greater depth in the dedicated [frontend](./frontend) and [backend](./backend) guides respectively.\n\n### What you need to remember\n\n* If you expect your component to be used as input, it is important to define an \"Example\" view.\n* If you don't, Gradio will use a default one but it won't be as informative as it can be!\n\n## Conclusion\n\nNow that you know the most important pieces to remember about Gradio components, you can start to design and build your own!",tags:[],spaces:[],url:"/guides/key-component-concepts/",contributor:null},{name:"configuration",category:"custom-components",pretty_category:"Custom Components",guide_index:3,absolute_index:26,pretty_name:"Configuration",content:"# Configuring Your Custom Component\n\nThe custom components workflow focuses on [convention over configuration](https://en.wikipedia.org/wiki/Convention_over_configuration) to reduce the number of decisions you as a developer need to make when developing your custom component.\nThat being said, you can still configure some aspects of the custom component package and directory.\nThis guide will cover how.\n\n## The Package Name\n\nBy default, all custom component packages are called `gradio_\u003Ccomponent-name>` where `component-name` is the name of the component's python class in lowercase.\n\nAs an example, let's walkthrough changing the name of a component from `gradio_mytextbox` to `supertextbox`. \n\n1. Modify the `name` in the `pyproject.toml` file. \n\n```bash\n[project]\nname = \"supertextbox\"\n```\n\n2. Change all occurrences of `gradio_\u003Ccomponent-name>` in `pyproject.toml` to `\u003Ccomponent-name>`\n\n```bash\n[tool.hatch.build]\nartifacts = [\"/backend/supertextbox/templates\", \"*.pyi\"]\n\n[tool.hatch.build.targets.wheel]\npackages = [\"/backend/supertextbox\"]\n```\n\n3. Rename the `gradio_\u003Ccomponent-name>` directory in `backend/` to `\u003Ccomponent-name>`\n\n```bash\nmv backend/gradio_mytextbox backend/supertextbox\n```\n\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> Remember to change the import statement in `demo/app.py`!\u003C/p>\n\n## Top Level Python Exports\n\nBy default, only the custom component python class is a top level export. \nThis means that when users type `from gradio_\u003Ccomponent-name> import ...`, the only class that will be available is the custom component class.\nTo add more classes as top level exports, modify the `__all__` property in `__init__.py`\n\n```python\nfrom .mytextbox import MyTextbox\nfrom .mytextbox import AdditionalClass, additional_function\n\n__all__ = ['MyTextbox', 'AdditionalClass', 'additional_function']\n```\n\n## Python Dependencies\n\nYou can add python dependencies by modifying the `dependencies` key in `pyproject.toml`\n\n```bash\ndependencies = [\"gradio\", \"numpy\", \"PIL\"]\n```\n\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> Remember to run `gradio cc install` when you add dependencies!\u003C/p>\n\n## Javascript Dependencies\n\nYou can add JavaScript dependencies by modifying the `\"dependencies\"` key in `frontend/package.json`\n\n```json\n\"dependencies\": {\n    \"@gradio/atoms\": \"0.2.0-beta.4\",\n    \"@gradio/statustracker\": \"0.3.0-beta.6\",\n    \"@gradio/utils\": \"0.2.0-beta.4\",\n    \"your-npm-package\": \"\u003Cversion>\"\n}\n```\n\n## Directory Structure\n\nBy default, the CLI will place the Python code in `backend` and the JavaScript code in `frontend`.\nIt is not recommended to change this structure since it makes it easy for a potential contributor to look at your source code and know where everything is.\nHowever, if you did want to this is what you would have to do:\n\n1. Place the Python code in the subdirectory of your choosing. Remember to modify the `[tool.hatch.build]` `[tool.hatch.build.targets.wheel]` in the `pyproject.toml` to match!\n\n2. Place the JavaScript code in the subdirectory of your choosing.\n\n2. Add the `FRONTEND_DIR` property on the component python class. It must be the relative path from the file where the class is defined to the location of the JavaScript directory.\n\n```python\nclass SuperTextbox(Component):\n    FRONTEND_DIR = \"../../frontend/\"\n```\n\nThe JavaScript and Python directories must be under the same common directory!\n\n## Conclusion\n\n\nSticking to the defaults will make it easy for others to understand and contribute to your custom component.\nAfter all, the beauty of open source is that anyone can help improve your code!\nBut if you ever need to deviate from the defaults, you know how!",tags:[],spaces:[],url:"/guides/configuration/",contributor:null},{name:"backend",category:"custom-components",pretty_category:"Custom Components",guide_index:4,absolute_index:27,pretty_name:"Backend",content:"# The Backend üêç\n\nThis guide will cover everything you need to know to implement your custom component's backend processing.\n\n## Which Class to Inherit From\n\nAll components inherit from one of three classes `Component`, `FormComponent`, or `BlockContext`.\nYou need to inherit from one so that your component behaves like all other gradio components.\nWhen you start from a template with `gradio cc create --template`, you don't need to worry about which one to choose since the template uses the correct one. \nFor completeness, and in the event that you need to make your own component from scratch, we explain what each class is for.\n\n* `FormComponent`: Use this when you want your component to be grouped together in the same `Form` layout with other `FormComponents`. The `Slider`, `Textbox`, and `Number` components are all `FormComponents`.\n* `BlockContext`: Use this when you want to place other components \"inside\" your component. This enabled `with MyComponent() as component:` syntax.\n* `Component`: Use this for all other cases.\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> If your component supports streaming output, inherit from the `StreamingOutput` class.\u003C/p>\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> If you inherit from `BlockContext`, you also need to set the metaclass to be `ComponentMeta`. See example below.\u003C/p>\n\n```python\nfrom gradio.blocks import BlockContext\nfrom gradio.component_meta import ComponentMeta\n\n\n\n\n@document()\nclass Row(BlockContext, metaclass=ComponentMeta):\n    pass\n```\n\n## The methods you need to implement\n\nWhen you inherit from any of these classes, the following methods must be implemented.\nOtherwise the Python interpreter will raise an error when you instantiate your component!\n\n### `preprocess` and `postprocess`\n\nExplained in the [Key Concepts](./key-component-concepts#the-value-and-how-it-is-preprocessed-postprocessed) guide. \nThey handle the conversion from the data sent by the frontend to the format expected by the python function.\n\n```python\n    def preprocess(self, x: Any) -> Any:\n        \"\"\"\n        Convert from the web-friendly (typically JSON) value in the frontend to the format expected by the python function.\n        \"\"\"\n        return x\n\n    def postprocess(self, y):\n        \"\"\"\n        Convert from the data returned by the python function to the web-friendly (typically JSON) value expected by the frontend.\n        \"\"\"\n        return y\n```\n\n### `process_example`\n\nTakes in the original Python value and returns the modified value that should be displayed in the examples preview in the app. \nIf not provided, the `.postprocess()` method is used instead. Let's look at the following example from the `SimpleDropdown` component.\n\n```python\ndef process_example(self, input_data):\n    return next((c[0] for c in self.choices if c[1] == input_data), None)\n```\n\nSince `self.choices` is a list of tuples corresponding to (`display_name`, `value`), this converts the value that a user provides to the display value (or if the value is not present in `self.choices`, it is converted to `None`).\n\n\n### `api_info`\n\nA JSON-schema representation of the value that the `preprocess` expects. \nThis powers api usage via the gradio clients. \nYou do **not** need to implement this yourself if you components specifies a `data_model`. \nThe `data_model` in the following section.\n\n```python\ndef api_info(self) -> dict[str, list[str]]:\n    \"\"\"\n    A JSON-schema representation of the value that the `preprocess` expects and the `postprocess` returns.\n    \"\"\"\n    pass\n```\n\n### `example_payload`\n\nAn example payload for your component, e.g. something that can be passed into the `.preprocess()` method\nof your component. The example input is displayed in the `View API` page of a Gradio app that uses your custom component. \nMust be JSON-serializable. If your component expects a file, it is best to use a publicly accessible URL.\n\n```python\ndef example_payload(self) -> Any:\n    \"\"\"\n    The example inputs for this component for API usage. Must be JSON-serializable.\n    \"\"\"\n    pass\n```\n\n### `example_value`\n\nAn example value for your component, e.g. something that can be passed into the `.postprocess()` method\nof your component. This is used as the example value in the default app that is created in custom component development.\n\n```python\ndef example_payload(self) -> Any:\n    \"\"\"\n    The example inputs for this component for API usage. Must be JSON-serializable.\n    \"\"\"\n    pass\n```\n\n### `flag`\n\nWrite the component's value to a format that can be stored in the `csv` or `json` file used for flagging.\nYou do **not** need to implement this yourself if you components specifies a `data_model`. \nThe `data_model` in the following section.\n\n```python\ndef flag(self, x: Any | GradioDataModel, flag_dir: str | Path = \"\") -> str:\n    pass\n```\n\n### `read_from_flag`\nConvert from the format stored in the `csv` or `json` file used for flagging to the component's python `value`.\nYou do **not** need to implement this yourself if you components specifies a `data_model`. \nThe `data_model` in the following section.\n\n```python\ndef read_from_flag(\n    self,\n    x: Any,\n) -> GradioDataModel | Any:\n    \"\"\"\n    Convert the data from the csv or jsonl file into the component state.\n    \"\"\"\n    return x\n```\n\n## The `data_model`\n\nThe `data_model` is how you define the expected data format your component's value will be stored in the frontend.\nIt specifies the data format your `preprocess` method expects and the format the `postprocess` method returns.\nIt is not necessary to define a `data_model` for your component but it greatly simplifies the process of creating a custom component.\nIf you define a custom component you only need to implement four methods - `preprocess`, `postprocess`, `example_payload`, and `example_value`!\n\nYou define a `data_model` by defining a [pydantic model](https://docs.pydantic.dev/latest/concepts/models/#basic-model-usage) that inherits from either `GradioModel` or `GradioRootModel`.\n\nThis is best explained with an example. Let's look at the core `Video` component, which stores the video data as a JSON object with two keys `video` and `subtitles` which point to separate files.\n\n```python\nfrom gradio.data_classes import FileData, GradioModel\n\nclass VideoData(GradioModel):\n    video: FileData\n    subtitles: Optional[FileData] = None\n\nclass Video(Component):\n    data_model = VideoData\n```\n\nBy adding these four lines of code, your component automatically implements the methods needed for API usage, the flagging methods, and example caching methods!\nIt also has the added benefit of self-documenting your code.\nAnyone who reads your component code will know exactly the data it expects.\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> If your component expects files to be uploaded from the frontend, your must use the `FileData` model! It will be explained in the following section. \u003C/p>\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> Read the pydantic docs [here](https://docs.pydantic.dev/latest/concepts/models/#basic-model-usage).\u003C/p>\n\nThe difference between a `GradioModel` and a `GradioRootModel` is that the `RootModel` will not serialize the data to a dictionary.\nFor example, the `Names` model will serialize the data to `{'names': ['freddy', 'pete']}` whereas the `NamesRoot` model will serialize it to `['freddy', 'pete']`.\n\n```python\nfrom typing import List\n\nclass Names(GradioModel):\n    names: List[str]\n\nclass NamesRoot(GradioRootModel):\n    root: List[str]\n```\n\nEven if your component does not expect a \"complex\" JSON data structure it can be beneficial to define a `GradioRootModel` so that you don't have to worry about implementing the API and flagging methods.\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> Use classes from the Python typing library to type your models. e.g. `List` instead of `list`.\u003C/p>\n\n## Handling Files\n\nIf your component expects uploaded files as input, or returns saved files to the frontend, you **MUST** use the `FileData` to type the files in your `data_model`.\n\nWhen you use the `FileData`:\n\n* Gradio knows that it should allow serving this file to the frontend. Gradio automatically blocks requests to serve arbitrary files in the computer running the server.\n\n* Gradio will automatically place the file in a cache so that duplicate copies of the file don't get saved.\n\n* The client libraries will automatically know that they should upload input files prior to sending the request. They will also automatically download files.\n\nIf you do not use the `FileData`, your component will not work as expected!\n\n\n## Adding Event Triggers To Your Component\n\nThe events triggers for your component are defined in the `EVENTS` class attribute.\nThis is a list that contains the string names of the events.\nAdding an event to this list will automatically add a method with that same name to your component!\n\nYou can import the `Events` enum from `gradio.events` to access commonly used events in the core gradio components.\n\nFor example, the following code will define `text_submit`, `file_upload` and `change` methods in the `MyComponent` class.\n\n```python\nfrom gradio.events import Events\nfrom gradio.components import FormComponent\n\nclass MyComponent(FormComponent):\n\n    EVENTS = [\n        \"text_submit\",\n        \"file_upload\",\n        Events.change\n    ]\n```\n\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> Don't forget to also handle these events in the JavaScript code!\u003C/p>\n\n## Conclusion\n\n",tags:[],spaces:[],url:"/guides/backend/",contributor:null},{name:"frontend",category:"custom-components",pretty_category:"Custom Components",guide_index:5,absolute_index:28,pretty_name:"Frontend",content:"# The Frontend üåê‚≠êÔ∏è\n\nThis guide will cover everything you need to know to implement your custom component's frontend.\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> Gradio components use Svelte. Writing Svelte is fun! If you're not familiar with it, we recommend checking out their interactive [guide](https://learn.svelte.dev/tutorial/welcome-to-svelte).\u003C/p>\n\n## The directory structure \n\nThe frontend code should have, at minimum, three files:\n\n* `Index.svelte`: This is the main export and where your component's layout and logic should live.\n* `Example.svelte`: This is where the example view of the component is defined.\n\nFeel free to add additional files and subdirectories. \nIf you want to export any additional modules, remember to modify the `package.json` file\n\n```json\n\"exports\": {\n    \".\": \"./Index.svelte\",\n    \"./example\": \"./Example.svelte\",\n    \"./package.json\": \"./package.json\"\n},\n```\n\n## The Index.svelte file\n\nYour component should expose the following props that will be passed down from the parent Gradio application.\n\n```typescript\nimport type { LoadingStatus } from \"@gradio/statustracker\";\nimport type { Gradio } from \"@gradio/utils\";\n\nexport let gradio: Gradio\u003C{\n    event_1: never;\n    event_2: never;\n}>;\n\nexport let elem_id = \"\";\nexport let elem_classes: string[] = [];\nexport let scale: number | null = null;\nexport let min_width: number | undefined = undefined;\nexport let loading_status: LoadingStatus | undefined = undefined;\nexport let mode: \"static\" | \"interactive\";\n```\n\n* `elem_id` and `elem_classes` allow Gradio app developers to target your component with custom CSS and JavaScript from the Python `Blocks` class.\n\n* `scale` and `min_width` allow Gradio app developers to control how much space your component takes up in the UI.\n\n* `loading_status` is used to display a loading status over the component when it is the output of an event.\n\n* `mode` is how the parent Gradio app tells your component whether the `interactive` or `static` version should be displayed.\n\n* `gradio`: The `gradio` object is created by the parent Gradio app. It stores some application-level configuration that will be useful in your component, like internationalization. You must use it to dispatch events from your component.\n\nA minimal `Index.svelte` file would look like:\n\n```svelte\n\u003Cscript lang=\"ts\">\n\timport type { LoadingStatus } from \"@gradio/statustracker\";\n    import { Block } from \"@gradio/atoms\";\n\timport { StatusTracker } from \"@gradio/statustracker\";\n\timport type { Gradio } from \"@gradio/utils\";\n\n\texport let gradio: Gradio\u003C{\n\t\tevent_1: never;\n\t\tevent_2: never;\n\t}>;\n\n    export let value = \"\";\n\texport let elem_id = \"\";\n\texport let elem_classes: string[] = [];\n\texport let scale: number | null = null;\n\texport let min_width: number | undefined = undefined;\n\texport let loading_status: LoadingStatus | undefined = undefined;\n    export let mode: \"static\" | \"interactive\";\n\u003C/script>\n\n\u003CBlock\n\tvisible={true}\n\t{elem_id}\n\t{elem_classes}\n\t{scale}\n\t{min_width}\n\tallow_overflow={false}\n\tpadding={true}\n>\n\t{#if loading_status}\n\t\t\u003CStatusTracker\n\t\t\tautoscroll={gradio.autoscroll}\n\t\t\ti18n={gradio.i18n}\n\t\t\t{...loading_status}\n\t\t/>\n\t{/if}\n    \u003Cp>{value}\u003C/p>\n\u003C/Block>\n```\n\n## The Example.svelte file\n\nThe `Example.svelte` file should expose the following props:\n\n```typescript\n    export let value: string;\n    export let type: \"gallery\" | \"table\";\n    export let selected = false;\n    export let index: number;\n```\n\n* `value`: The example value that should be displayed.\n\n* `type`: This is a variable that can be either `\"gallery\"` or `\"table\"` depending on how the examples are displayed. The `\"gallery\"` form is used when the examples correspond to a single input component, while the `\"table\"` form is used when a user has multiple input components, and the examples need to populate all of them. \n\n* `selected`: You can also adjust how the examples are displayed if a user \"selects\" a particular example by using the selected variable.\n\n* `index`: The current index of the selected value.\n\n* Any additional props your \"non-example\" component takes!\n\nThis is the `Example.svelte` file for the code `Radio` component:\n\n```svelte\n\u003Cscript lang=\"ts\">\n\texport let value: string;\n\texport let type: \"gallery\" | \"table\";\n\texport let selected = false;\n\u003C/script>\n\n\u003Cdiv\n\tclass:table={type === \"table\"}\n\tclass:gallery={type === \"gallery\"}\n\tclass:selected\n>\n\t{value}\n\u003C/div>\n\n\u003Cstyle>\n\t.gallery {\n\t\tpadding: var(--size-1) var(--size-2);\n\t}\n\u003C/style>\n```\n\n## Handling Files\n\nIf your component deals with files, these files **should** be uploaded to the backend server. \nThe `@gradio/client` npm package provides the `upload` and `prepare_files` utility functions to help you do this.\n\nThe `prepare_files` function will convert the browser's `File` datatype to gradio's internal `FileData` type.\nYou should use the `FileData` data in your component to keep track of uploaded files.\n\nThe `upload` function will upload an array of `FileData` values to the server.\n\nHere's an example of loading files from an `\u003Cinput>` element when its value changes.\n\n\n```svelte\n\u003Cscript lang=\"ts\">\n    import { upload, prepare_files, type FileData } from \"@gradio/client\";\n    export let root;\n    export let value;\n    let uploaded_files;\n\n    async function handle_upload(file_data: FileData[]): Promise\u003Cvoid> {\n        await tick();\n        uploaded_files = await upload(file_data, root);\n    }\n\n    async function loadFiles(files: FileList): Promise\u003Cvoid> {\n        let _files: File[] = Array.from(files);\n        if (!files.length) {\n            return;\n        }\n        if (file_count === \"single\") {\n            _files = [files[0]];\n        }\n        let file_data = await prepare_files(_files);\n        await handle_upload(file_data);\n    }\n\n    async function loadFilesFromUpload(e: Event): Promise\u003Cvoid> {\n\t\tconst target = e.target;\n\n\t\tif (!target.files) return;\n\t\tawait loadFiles(target.files);\n\t}\n\u003C/script>\n\n\u003Cinput\n    type=\"file\"\n    on:change={loadFilesFromUpload}\n    multiple={true}\n/>\n```\n\nThe component exposes a prop named `root`. \nThis is passed down by the parent gradio app and it represents the base url that the files will be uploaded to and fetched from.\n\nFor WASM support, you should get the upload function from the `Context` and pass that as the third parameter of the `upload` function.\n\n```typescript\n\u003Cscript lang=\"ts\">\n    import { getContext } from \"svelte\";\n    const upload_fn = getContext\u003Ctypeof upload_files>(\"upload_files\");\n\n    async function handle_upload(file_data: FileData[]): Promise\u003Cvoid> {\n        await tick();\n        await upload(file_data, root, upload_fn);\n    }\n\u003C/script>\n```\n\n## Leveraging Existing Gradio Components\n\nMost of Gradio's frontend components are published on [npm](https://www.npmjs.com/), the javascript package repository.\nThis means that you can use them to save yourself time while incorporating common patterns in your component, like uploading files.\nFor example, the `@gradio/upload` package has `Upload` and `ModifyUpload` components for properly uploading files to the Gradio server. \nHere is how you can use them to create a user interface to upload and display PDF files.\n\n```svelte\n\u003Cscript>\n\timport { type FileData, Upload, ModifyUpload } from \"@gradio/upload\";\n\timport { Empty, UploadText, BlockLabel } from \"@gradio/atoms\";\n\u003C/script>\n\n\u003CBlockLabel Icon={File} label={label || \"PDF\"} />\n{#if value === null && interactive}\n    \u003CUpload\n        filetype=\"application/pdf\"\n        on:load={handle_load}\n        {root}\n        >\n        \u003CUploadText type=\"file\" i18n={gradio.i18n} />\n    \u003C/Upload>\n{:else if value !== null}\n    {#if interactive}\n        \u003CModifyUpload i18n={gradio.i18n} on:clear={handle_clear}/>\n    {/if}\n    \u003Ciframe title={value.orig_name || \"PDF\"} src={value.data} height=\"{height}px\" width=\"100%\">\u003C/iframe>\n{:else}\n    \u003CEmpty size=\"large\"> \u003CFile/> \u003C/Empty>\t\n{/if}\n```\n\nYou can also combine existing Gradio components to create entirely unique experiences.\nLike rendering a gallery of chatbot conversations. \nThe possibilities are endless, please read the documentation on our javascript packages [here](https://gradio.app/main/docs/js).\nWe'll be adding more packages and documentation over the coming weeks!\n\n## Matching Gradio Core's Design System\n\nYou can explore our component library via Storybook. You'll be able to interact with our components and see them in their various states.\n\nFor those interested in design customization, we provide the CSS variables consisting of our color palette, radii, spacing, and the icons we use - so you can easily match up your custom component with the style of our core components. This Storybook will be regularly updated with any new additions or changes.\n\n[Storybook Link](https://gradio.app/main/docs/js/storybook)\n\n## Custom configuration\n\nIf you want to make use of the vast vite ecosystem, you can use the `gradio.config.js` file to configure your component's build process. This allows you to make use of tools like tailwindcss, mdsvex, and more.\n\nCurrently, it is possible to configure the following:\n\nVite options:\n- `plugins`: A list of vite plugins to use.\n\nSvelte options:\n- `preprocess`: A list of svelte preprocessors to use.\n- `extensions`: A list of file extensions to compile to `.svelte` files.\n\nThe `gradio.config.js` file should be placed in the root of your component's `frontend` directory. A default config file is created for you when you create a new component. But you can also create your own config file, if one doesn't exist, and use it to customize your component's build process.\n\n### Example for a Vite plugin\n\nCustom components can use Vite plugins to customize the build process. Check out the [Vite Docs](https://vitejs.dev/guide/using-plugins.html) for more information. \n\nHere we configure [TailwindCSS](https://tailwindcss.com), a utility-first CSS framework. Setup is easiest using the version 4 prerelease. \n\n```\nnpm install tailwindcss@next @tailwindcss/vite@next\n```\n\nIn `gradio.config.js`:\n\n```typescript\nimport tailwindcss from \"@tailwindcss/vite\";\nexport default {\n    plugins: [tailwindcss()]\n};\n```\n\nThen create a `style.css` file with the following content:\n\n```css\n@import \"tailwindcss\";\n```\n\nImport this file into `Index.svelte`. Note, that you need to import the css file containing `@import` and cannot just use a `\u003Cstyle>` tag and use `@import` there. \n\n```svelte\n\u003Cscript lang=\"ts\">\n[...]\nimport \"./style.css\";\n[...]\n\u003C/script>\n```\n\n### Example for Svelte options\n\nIn `gradio.config.js` you can also specify a some Svelte options to apply to the Svelte compilation. In this example we will add support for [`mdsvex`](https://mdsvex.pngwn.io), a Markdown preprocessor for Svelte. \n\nIn order to do this we will need to add a [Svelte Preprocessor](https://svelte.dev/docs/svelte-compiler#preprocess) to the `svelte` object in `gradio.config.js` and configure the [`extensions`](https://github.com/sveltejs/vite-plugin-svelte/blob/HEAD/docs/config.md#config-file) field. Other options are not currently supported.\n\nFirst, install the `mdsvex` plugin:\n\n```bash\nnpm install mdsvex\n```\n\nThen add the following to `gradio.config.js`:\n\n```typescript\nimport { mdsvex } from \"mdsvex\";\n\nexport default {\n    svelte: {\n        preprocess: [\n            mdsvex()\n        ],\n        extensions: [\".svelte\", \".svx\"]\n    }\n};\n```\n\nNow we can create `mdsvex` documents in our component's `frontend` directory and they will be compiled to `.svelte` files.\n\n```md\n\u003C!-- HelloWorld.svx -->\n\n\u003Cscript lang=\"ts\">\n    import { Block } from \"@gradio/atoms\";\n\n    export let title = \"Hello World\";\n\u003C/script>\n\n\u003CBlock label=\"Hello World\">\n\n# {title}\n\nThis is a markdown file.\n\n\u003C/Block>\n```\n\nWe can then use the `HelloWorld.svx` file in our components:\n\n```svelte\n\u003Cscript lang=\"ts\">\n    import HelloWorld from \"./HelloWorld.svx\";\n\u003C/script>\n\n\u003CHelloWorld />\n```\n\n## Conclusion\n\nYou now how to create delightful frontends for your components!\n\n",tags:[],spaces:[],url:"/guides/frontend/",contributor:null},{name:"frequently-asked-questions",category:"custom-components",pretty_category:"Custom Components",guide_index:6,absolute_index:29,pretty_name:"Frequently Asked Questions",content:"# Frequently Asked Questions\n\n## What do I need to install before using Custom Components?\nBefore using Custom Components, make sure you have Python 3.8+, Node.js v16.14+, npm 9+, and Gradio 4.0+ installed.\n\n## What templates can I use to create my custom component?\nRun `gradio cc show` to see the list of built-in templates.\nYou can also start off from other's custom components!\nSimply `git clone` their repository and make your modifications.\n\n## What is the development server?\nWhen you run `gradio cc dev`, a development server will load and run a Gradio app of your choosing.\nThis is like when you run `python \u003Capp-file>.py`, however the `gradio` command will hot reload so you can instantly see your changes. \n\n## The development server didn't work for me \n\n**1. Check your terminal and browser console**\n\nMake sure there are no syntax errors or other obvious problems in your code. Exceptions triggered from python will be displayed in the terminal. Exceptions from javascript will be displayed in the browser console and/or the terminal.\n\n**2. Are you developing on Windows?**\n\nChrome on Windows will block the local compiled svelte files for security reasons. We recommend developing your custom component in the windows subsystem for linux (WSL) while the team looks at this issue.\n\n**3. Inspect the window.__GRADIO_CC__ variable**\n\nIn the browser console, print the `window.__GRADIO__CC` variable (just type it into the console). If it is an empty object, that means\nthat the CLI could not find your custom component source code. Typically, this happens when the custom component is installed in a different virtual environment than the one used to run the dev command. Please use the `--python-path` and `gradio-path` CLI arguments to specify the path of the python and gradio executables for the environment your component is installed in. For example, if you are using a virtualenv located at `/Users/mary/venv`, pass in `/Users/mary/bin/python` and `/Users/mary/bin/gradio` respectively.\n\nIf the `window.__GRADIO__CC` variable is not empty (see below for an example), then the dev server should be working correctly. \n\n![](https://gradio-builds.s3.amazonaws.com/demo-files/gradio_CC_DEV.png)\n\n**4. Make sure you are using a virtual environment**\nIt is highly recommended you use a virtual environment to prevent conflicts with other python dependencies installed in your system.\n\n\n## Do I always need to start my component from scratch?\nNo! You can start off from an existing gradio component as a template, see the [five minute guide](./custom-components-in-five-minutes).\nYou can also start from an existing custom component if you'd like to tweak it further. Once you find the source code of a custom component you like, clone the code to your computer and run `gradio cc install`. Then you can run the development server to make changes.If you run into any issues, contact the author of the component by opening an issue in their repository. The [gallery](https://www.gradio.app/custom-components/gallery) is a good place to look for published components. For example, to start from the [PDF component](https://www.gradio.app/custom-components/gallery?id=freddyaboulton%2Fgradio_pdf), clone the space with `git clone https://huggingface.co/spaces/freddyaboulton/gradio_pdf`, `cd` into the `src` directory, and run `gradio cc install`.\n\n\n## Do I need to host my custom component on HuggingFace Spaces?\nYou can develop and build your custom component without hosting or connecting to HuggingFace.\nIf you would like to share your component with the gradio community, it is recommended to publish your package to PyPi and host a demo on HuggingFace so that anyone can install it or try it out.\n\n## What methods are mandatory for implementing a custom component in Gradio?\n\nYou must implement the `preprocess`, `postprocess`, `example_payload`, and `example_value` methods. If your component does not use a data model, you must also define the `api_info`, `flag`, and `read_from_flag` methods. Read more in the [backend guide](./backend).\n\n## What is the purpose of a `data_model` in Gradio custom components?\n\nA `data_model` defines the expected data format for your component, simplifying the component development process and self-documenting your code. It streamlines API usage and example caching.\n\n## Why is it important to use `FileData` for components dealing with file uploads?\n\nUtilizing `FileData` is crucial for components that expect file uploads. It ensures secure file handling, automatic caching, and streamlined client library functionality.\n\n## How can I add event triggers to my custom Gradio component?\n\nYou can define event triggers in the `EVENTS` class attribute by listing the desired event names, which automatically adds corresponding methods to your component.\n\n## Can I implement a custom Gradio component without defining a `data_model`?\n\nYes, it is possible to create custom components without a `data_model`, but you are going to have to manually implement `api_info`, `flag`, and `read_from_flag` methods.\n\n## Are there sample custom components I can learn from?\n\nWe have prepared this [collection](https://huggingface.co/collections/gradio/custom-components-65497a761c5192d981710b12) of custom components on the HuggingFace Hub that you can use to get started!\n\n## How can I find custom components created by the Gradio community?\n\nWe're working on creating a gallery to make it really easy to discover new custom components.\nIn the meantime, you can search for HuggingFace Spaces that are tagged as a `gradio-custom-component` [here](https://huggingface.co/search/full-text?q=gradio-custom-component&type=space)",tags:[],spaces:[],url:"/guides/frequently-asked-questions/",contributor:null},{name:"pdf-component-example",category:"custom-components",pretty_category:"Custom Components",guide_index:7,absolute_index:30,pretty_name:"Pdf Component Example",content:"# Case Study: A Component to Display PDFs\n\nLet's work through an example of building a custom gradio component for displaying PDF files.\nThis component will come in handy for showcasing [document question answering](https://huggingface.co/models?pipeline_tag=document-question-answering&sort=trending) models, which typically work on PDF input.\nThis is a sneak preview of what our finished component will look like:\n\n![demo](https://gradio-builds.s3.amazonaws.com/assets/PDFDisplay.png)\n\n## Step 0: Prerequisites\nMake sure you have gradio 4.0 installed as well as node 18+.\nAs of the time of publication, the latest release is 4.1.1.\nAlso, please read the [Five Minute Tour](./custom-components-in-five-minutes) of custom components and the [Key Concepts](./key-component-concepts) guide before starting.\n\n\n## Step 1: Creating the custom component\n\nNavigate to a directory of your choosing and run the following command:\n\n```bash\ngradio cc create PDF\n```\n\n\nTip: You should change the name of the component.\nSome of the screenshots assume the component is callled `PDF` but the concepts are the same!\n\nThis will create a subdirectory called `pdf` in your current working directory.\nThere are three main subdirectories in `pdf`: `frontend`, `backend`, and `demo`.\nIf you open `pdf` in your code editor, it will look like this:\n\n![directory structure](https://gradio-builds.s3.amazonaws.com/assets/pdf-guide/CodeStructure.png)\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> For this demo we are not templating off a current gradio component. But you can see the list of available templates with `gradio cc show` and then pass the template name to the `--template` option, e.g. `gradio cc create \u003CName> --template \u003Cfoo>`\u003C/p>\n\n## Step 2: Frontend - modify javascript dependencies\n\nWe're going to use the [pdfjs](https://mozilla.github.io/pdf.js/) javascript library to display the pdfs in the frontend. \nLet's start off by adding it to our frontend project's dependencies, as well as adding a couple of other projects we'll need.\n\nFrom within the `frontend` directory, run `npm install @gradio/client @gradio/upload @gradio/icons @gradio/button` and `npm install --save-dev pdfjs-dist@3.11.174`.\nAlso, let's uninstall the `@zerodevx/svelte-json-view` dependency by running `npm uninstall @zerodevx/svelte-json-view`.\n\nThe complete `package.json` should look like this:\n\n```json\n{\n  \"name\": \"gradio_pdf\",\n  \"version\": \"0.2.0\",\n  \"description\": \"Gradio component for displaying PDFs\",\n  \"type\": \"module\",\n  \"author\": \"\",\n  \"license\": \"ISC\",\n  \"private\": false,\n  \"main_changeset\": true,\n  \"exports\": {\n    \".\": \"./Index.svelte\",\n    \"./example\": \"./Example.svelte\",\n    \"./package.json\": \"./package.json\"\n  },\n  \"devDependencies\": {\n    \"pdfjs-dist\": \"3.11.174\"\n  },\n  \"dependencies\": {\n    \"@gradio/atoms\": \"0.2.0\",\n    \"@gradio/statustracker\": \"0.3.0\",\n    \"@gradio/utils\": \"0.2.0\",\n    \"@gradio/client\": \"0.7.1\",\n    \"@gradio/upload\": \"0.3.2\",\n    \"@gradio/icons\": \"0.2.0\",\n    \"@gradio/button\": \"0.2.3\",\n    \"pdfjs-dist\": \"3.11.174\"\n  }\n}\n```\n\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> Running `npm install` will install the latest version of the package available. You can install a specific version with `npm install package@\u003Cversion>`.  You can find all of the gradio javascript package documentation [here](https://www.gradio.app/main/docs/js). It is recommended you use the same versions as me as the API can change.\u003C/p>\n\nNavigate to `Index.svelte` and delete mentions of `JSONView`\n\n```ts\nimport { JsonView } from \"@zerodevx/svelte-json-view\";\n```\n\n```svelte\n\u003CJsonView json={value} />\n```\n\n## Step 3: Frontend - Launching the Dev Server\n\nRun the `dev` command to launch the development server.\nThis will open the demo in `demo/app.py` in an environment where changes to the `frontend` and `backend` directories will reflect instantaneously in the launched app.\n\nAfter launching the dev server, you should see a link printed to your console that says `Frontend Server (Go here): ... `.\n \n![](https://gradio-builds.s3.amazonaws.com/assets/pdf-guide/dev_server_terminal.png)\n\nYou should see the following:\n\n![](https://gradio-builds.s3.amazonaws.com/assets/pdf-guide/frontend_start.png)\n\n\nIts not impressive yet but we're ready to start coding!\n\n## Step 4: Frontend - The basic skeleton\n\nWe're going to start off by first writing the skeleton of our frontend and then adding the pdf rendering logic.\nAdd the following imports and expose the following properties to the top of your file in the `\u003Cscript>` tag.\nYou may get some warnings from your code editor that some props are not used.\nThat's ok.\n\n```ts\n    import { tick } from \"svelte\";\n    import type { Gradio } from \"@gradio/utils\";\n    import { Block, BlockLabel } from \"@gradio/atoms\";\n    import { File } from \"@gradio/icons\";\n    import { StatusTracker } from \"@gradio/statustracker\";\n    import type { LoadingStatus } from \"@gradio/statustracker\";\n    import type { FileData } from \"@gradio/client\";\n    import { Upload, ModifyUpload } from \"@gradio/upload\";\n\n\texport let elem_id = \"\";\n\texport let elem_classes: string[] = [];\n\texport let visible = true;\n\texport let value: FileData | null = null;\n\texport let container = true;\n\texport let scale: number | null = null;\n\texport let root: string;\n\texport let height: number | null = 500;\n\texport let label: string;\n\texport let proxy_url: string;\n\texport let min_width: number | undefined = undefined;\n\texport let loading_status: LoadingStatus;\n\texport let gradio: Gradio\u003C{\n\t\tchange: never;\n\t\tupload: never;\n\t}>;\n\n    let _value = value;\n    let old_value = _value;\n```\n\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> The `gradio`` object passed in here contains some metadata about the application as well as some utility methods. One of these utilities is a dispatch method. We want to dispatch change and upload events whenever our PDF is changed or updated. This line provides type hints that these are the only events we will be dispatching.\u003C/p>\n\nWe want our frontend component to let users upload a PDF document if there isn't one already loaded.\nIf it is loaded, we want to display it underneath a \"clear\" button that lets our users upload a new document. \nWe're going to use the `Upload` and `ModifyUpload` components that come with the `@gradio/upload` package to do this.\nUnderneath the `\u003C/script>` tag, delete all the current code and add the following:\n\n```svelte\n\u003CBlock {visible} {elem_id} {elem_classes} {container} {scale} {min_width}>\n    {#if loading_status}\n        \u003CStatusTracker\n            autoscroll={gradio.autoscroll}\n            i18n={gradio.i18n}\n            {...loading_status}\n        />\n    {/if}\n    \u003CBlockLabel\n        show_label={label !== null}\n        Icon={File}\n        float={value === null}\n        label={label || \"File\"}\n    />\n    {#if _value}\n        \u003CModifyUpload i18n={gradio.i18n} absolute />\n    {:else}\n        \u003CUpload\n            filetype={\"application/pdf\"}\n            file_count=\"single\"\n            {root}\n        >\n            Upload your PDF\n        \u003C/Upload>\n    {/if}\n\u003C/Block>\n```\n\nYou should see the following when you navigate to your app after saving your current changes:\n\n![](https://gradio-builds.s3.amazonaws.com/assets/pdf-guide/frontend_1.png)\n\n## Step 5: Frontend - Nicer Upload Text\n\nThe `Upload your PDF` text looks a bit small and barebones. \nLets customize it!\n\nCreate a new file called `PdfUploadText.svelte` and copy the following code.\nIts creating a new div to display our \"upload text\" with some custom styling.\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> Notice that we're leveraging Gradio core's existing css variables here: `var(--size-60)` and `var(--body-text-color-subdued)`. This allows our component to work nicely in light mode and dark mode, as well as with Gradio's built-in themes.\u003C/p>\n\n\n```svelte\n\u003Cscript lang=\"ts\">\n\timport { Upload as UploadIcon } from \"@gradio/icons\";\n\texport let hovered = false;\n\n\u003C/script>\n\n\u003Cdiv class=\"wrap\">\n\t\u003Cspan class=\"icon-wrap\" class:hovered>\u003CUploadIcon /> \u003C/span>\n    Drop PDF\n    \u003Cspan class=\"or\">- or -\u003C/span>\n    Click to Upload\n\u003C/div>\n\n\u003Cstyle>\n\t.wrap {\n\t\tdisplay: flex;\n\t\tflex-direction: column;\n\t\tjustify-content: center;\n\t\talign-items: center;\n\t\tmin-height: var(--size-60);\n\t\tcolor: var(--block-label-text-color);\n\t\tline-height: var(--line-md);\n\t\theight: 100%;\n\t\tpadding-top: var(--size-3);\n\t}\n\n\t.or {\n\t\tcolor: var(--body-text-color-subdued);\n\t\tdisplay: flex;\n\t}\n\n\t.icon-wrap {\n\t\twidth: 30px;\n\t\tmargin-bottom: var(--spacing-lg);\n\t}\n\n\t@media (--screen-md) {\n\t\t.wrap {\n\t\t\tfont-size: var(--text-lg);\n\t\t}\n\t}\n\n\t.hovered {\n\t\tcolor: var(--color-accent);\n\t}\n\u003C/style>\n```\n\nNow import `PdfUploadText.svelte` in your `\u003Cscript>` and pass it to the `Upload` component!\n\n```svelte\n\timport PdfUploadText from \"./PdfUploadText.svelte\";\n\n...\n\n    \u003CUpload\n        filetype={\"application/pdf\"}\n        file_count=\"single\"\n        {root}\n    >\n        \u003CPdfUploadText />\n    \u003C/Upload>\n```\n\nAfter saving your code, the frontend should now look like this:\n\n![](https://gradio-builds.s3.amazonaws.com/assets/pdf-guide/better_upload.png)\n\n## Step 6: PDF Rendering logic\n\nThis is the most advanced javascript part.\nIt took me a while to figure it out!\nDo not worry if you have trouble, the important thing is to not be discouraged üí™\nAsk for help in the gradio [discord](https://discord.gg/hugging-face-879548962464493619) if you need and ask for help.\n\nWith that out of the way, let's start off by importing `pdfjs` and loading the code of the pdf worker from the mozilla cdn.\n\n```ts\n\timport pdfjsLib from \"pdfjs-dist\";\n    ...\n    pdfjsLib.GlobalWorkerOptions.workerSrc =  \"https://cdn.bootcss.com/pdf.js/3.11.174/pdf.worker.js\";\n```\n\nAlso create the following variables:\n\n```ts\n    let pdfDoc;\n    let numPages = 1;\n    let currentPage = 1;\n    let canvasRef;\n```\n\nNow, we will use `pdfjs` to render a given page of the PDF onto an `html` document.\nAdd the following code to `Index.svelte`:\n\n```ts\n    async function get_doc(value: FileData) {\n        const loadingTask = pdfjsLib.getDocument(value.url);\n        pdfDoc = await loadingTask.promise;\n        numPages = pdfDoc.numPages;\n        render_page();\n    }\n\n    function render_page() {\n    // Render a specific page of the PDF onto the canvas\n        pdfDoc.getPage(currentPage).then(page => {\n            const ctx  = canvasRef.getContext('2d')\n            ctx.clearRect(0, 0, canvasRef.width, canvasRef.height);\n            let viewport = page.getViewport({ scale: 1 });\n            let scale = height / viewport.height;\n            viewport = page.getViewport({ scale: scale });\n\n            const renderContext = {\n                canvasContext: ctx,\n                viewport,\n            };\n            canvasRef.width = viewport.width;\n            canvasRef.height = viewport.height;\n            page.render(renderContext);\n        });\n    }\n\n    // If the value changes, render the PDF of the currentPage\n    $: if(JSON.stringify(old_value) != JSON.stringify(_value)) {\n        if (_value){\n            get_doc(_value);\n        }\n        old_value = _value;\n        gradio.dispatch(\"change\");\n    }\n```\n\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> The `$:` syntax in svelte is how you declare statements to be reactive. Whenever any of the inputs of the statement change, svelte will automatically re-run that statement.\u003C/p>\n\nNow place the `canvas` underneath the `ModifyUpload` component:\n\n```svelte\n\u003Cdiv class=\"pdf-canvas\" style=\"height: {height}px\">\n    \u003Ccanvas bind:this={canvasRef}>\u003C/canvas>\n\u003C/div>\n```\n\nAnd add the following styles to the `\u003Cstyle>` tag:\n\n```svelte\n\u003Cstyle>\n    .pdf-canvas {\n        display: flex;\n        justify-content: center;\n        align-items: center;\n    }\n\u003C/style>\n```\n\n## Step 7: Handling The File Upload And Clear\n\nNow for the fun part - actually rendering the PDF when the file is uploaded!\nAdd the following functions to the `\u003Cscript>` tag:\n\n```ts\n    async function handle_clear() {\n        _value = null;\n        await tick();\n        gradio.dispatch(\"change\");\n    }\n\n    async function handle_upload({detail}: CustomEvent\u003CFileData>): Promise\u003Cvoid> {\n        value = detail;\n        await tick();\n        gradio.dispatch(\"change\");\n        gradio.dispatch(\"upload\");\n    }\n```\n\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> The `gradio.dispatch` method is actually what is triggering the `change` or `upload` events in the backend. For every event defined in the component's backend, we will explain how to do this in Step 9, there must be at least one `gradio.dispatch(\"\u003Cevent-name>\")` call. These are called `gradio` events and they can be listended from the entire Gradio application. You can dispatch a built-in `svelte` event with the `dispatch` function. These events can only be listened to from the component's direct parent. Learn about svelte events from the [official documentation](https://learn.svelte.dev/tutorial/component-events).\u003C/p>\n\nNow we will run these functions whenever the `Upload` component uploads a file and whenever the `ModifyUpload` component clears the current file. The `\u003CUpload>` component dispatches a `load` event with a payload of type `FileData` corresponding to the uploaded file. The `on:load` syntax tells `Svelte` to automatically run this function in response to the event.\n\n```svelte\n    \u003CModifyUpload i18n={gradio.i18n} on:clear={handle_clear} absolute />\n    \n    ...\n    \n    \u003CUpload\n        on:load={handle_upload}\n        filetype={\"application/pdf\"}\n        file_count=\"single\"\n        {root}\n    >\n        \u003CPdfUploadText/>\n    \u003C/Upload>\n```\n\nCongratulations! You have a working pdf uploader!\n\n![upload-gif](https://gradio-builds.s3.amazonaws.com/assets/pdf-guide/pdf_component_gif_docs.gif)\n\n## Step 8: Adding buttons to navigate pages\n\nIf a user uploads a PDF document with multiple pages, they will only be able to see the first one.\nLet's add some buttons to help them navigate the page.\nWe will use the `BaseButton` from `@gradio/button` so that they look like regular Gradio buttons.\n\nImport the `BaseButton` and add the following functions that will render the next and previous page of the PDF.\n\n```ts\n    import { BaseButton } from \"@gradio/button\";\n\n    ...\n\n    function next_page() {\n        if (currentPage >= numPages) {\n            return;\n        }\n        currentPage++;\n        render_page();\n    }\n\n    function prev_page() {\n        if (currentPage == 1) {\n            return;\n        }\n        currentPage--;\n        render_page();\n    }\n```\n\nNow we will add them underneath the canvas in a separate `\u003Cdiv>`\n\n```svelte\n    ...\n\n    \u003CModifyUpload i18n={gradio.i18n} on:clear={handle_clear} absolute />\n    \u003Cdiv class=\"pdf-canvas\" style=\"height: {height}px\">\n        \u003Ccanvas bind:this={canvasRef}>\u003C/canvas>\n    \u003C/div>\n    \u003Cdiv class=\"button-row\">\n        \u003CBaseButton on:click={prev_page}>\n            ‚¨ÖÔ∏è\n        \u003C/BaseButton>\n        \u003Cspan class=\"page-count\"> {currentPage} / {numPages} \u003C/span>\n        \u003CBaseButton on:click={next_page}>\n            ‚û°Ô∏è\n        \u003C/BaseButton>\n    \u003C/div>\n    \n    ...\n\n\u003Cstyle>\n    .button-row {\n        display: flex;\n        flex-direction: row;\n        width: 100%;\n        justify-content: center;\n        align-items: center;\n    }\n\n    .page-count {\n        margin: 0 10px;\n        font-family: var(--font-mono);\n    }\n```\n\nCongratulations! The frontend is almost complete üéâ\n\n![multipage-pdf-gif](https://gradio-builds.s3.amazonaws.com/assets/pdf-guide/pdf_multipage.gif)\n\n## Step 8.5: The Example view\n\nWe're going to want users of our component to get a preview of the PDF if its used as an `example` in a `gr.Interface` or `gr.Examples`.\n\nTo do so, we're going to add some of the pdf rendering logic in `Index.svelte` to `Example.svelte`.\n\n\n```svelte\n\u003Cscript lang=\"ts\">\n\texport let value: string;\n\texport let type: \"gallery\" | \"table\";\n\texport let selected = false;\n\timport pdfjsLib from \"pdfjs-dist\";\n\tpdfjsLib.GlobalWorkerOptions.workerSrc =  \"https://cdn.bootcss.com/pdf.js/3.11.174/pdf.worker.js\";\n\t\n\tlet pdfDoc;\n\tlet canvasRef;\n\n\tasync function get_doc(url: string) {\n\t\tconst loadingTask = pdfjsLib.getDocument(url);\n\t\tpdfDoc = await loadingTask.promise;\n\t\trenderPage();\n\t\t}\n\n\tfunction renderPage() {\n\t\t// Render a specific page of the PDF onto the canvas\n\t\t\tpdfDoc.getPage(1).then(page => {\n\t\t\t\tconst ctx  = canvasRef.getContext('2d')\n\t\t\t\tctx.clearRect(0, 0, canvasRef.width, canvasRef.height);\n\t\t\t\t\n\t\t\t\tconst viewport = page.getViewport({ scale: 0.2 });\n\t\t\t\t\n\t\t\t\tconst renderContext = {\n\t\t\t\t\tcanvasContext: ctx,\n\t\t\t\t\tviewport\n\t\t\t\t};\n\t\t\t\tcanvasRef.width = viewport.width;\n\t\t\t\tcanvasRef.height = viewport.height;\n\t\t\t\tpage.render(renderContext);\n\t\t\t});\n\t\t}\n\t\n\t$: get_doc(value);\n\u003C/script>\n\n\u003Cdiv\n\tclass:table={type === \"table\"}\n\tclass:gallery={type === \"gallery\"}\n\tclass:selected\n\tstyle=\"justify-content: center; align-items: center; display: flex; flex-direction: column;\"\n>\n\t\u003Ccanvas bind:this={canvasRef}>\u003C/canvas>\n\u003C/div>\n\n\u003Cstyle>\n\t.gallery {\n\t\tpadding: var(--size-1) var(--size-2);\n\t}\n\u003C/style>\n```\n\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> Exercise for the reader - reduce the code duplication between `Index.svelte` and `Example.svelte` üòä\u003C/p>\n\n\nYou will not be able to render examples until we make some changes to the backend code in the next step!\n\n## Step 9: The backend\n\nThe backend changes needed are smaller.\nWe're almost done!\n\nWhat we're going to do is:\n* Add `change` and `upload` events to our component.\n* Add a `height` property to let users control the height of the PDF.\n* Set the `data_model` of our component to be `FileData`. This is so that Gradio can automatically cache and safely serve any files that are processed by our component.\n* Modify the `preprocess` method to return a string corresponding to the path of our uploaded PDF.\n* Modify the `postprocess` to turn a path to a PDF created in an event handler to a `FileData`.\n\nWhen all is said an done, your component's backend code should look like this:\n\n```python\nfrom __future__ import annotations\nfrom typing import Any, Callable\n\nfrom gradio.components.base import Component\nfrom gradio.data_classes import FileData\nfrom gradio import processing_utils\n\nclass PDF(Component):\n\n    EVENTS = [\"change\", \"upload\"]\n\n    data_model = FileData\n\n    def __init__(self, value: Any = None, *,\n                 height: int | None = None,\n                 label: str | None = None, info: str | None = None,\n                 show_label: bool | None = None,\n                 container: bool = True,\n                 scale: int | None = None,\n                 min_width: int | None = None,\n                 interactive: bool | None = None,\n                 visible: bool = True,\n                 elem_id: str | None = None,\n                 elem_classes: list[str] | str | None = None,\n                 render: bool = True,\n                 load_fn: Callable[..., Any] | None = None,\n                 every: float | None = None):\n        super().__init__(value, label=label, info=info,\n                         show_label=show_label, container=container,\n                         scale=scale, min_width=min_width,\n                         interactive=interactive, visible=visible,\n                         elem_id=elem_id, elem_classes=elem_classes,\n                         render=render, load_fn=load_fn, every=every)\n        self.height = height\n\n    def preprocess(self, payload: FileData) -> str:\n        return payload.path\n\n    def postprocess(self, value: str | None) -> FileData:\n        if not value:\n            return None\n        return FileData(path=value)\n\n    def example_payload(self):\n        return \"https://gradio-builds.s3.amazonaws.com/assets/pdf-guide/fw9.pdf\"\n\n    def example_value(self):\n        return \"https://gradio-builds.s3.amazonaws.com/assets/pdf-guide/fw9.pdf\"\n```\n\n## Step 10: Add a demo and publish!\n\nTo test our backend code, let's add a more complex demo that performs Document Question and Answering with huggingface transformers.\n\nIn our `demo` directory, create a `requirements.txt` file with the following packages\n\n```\ntorch\ntransformers\npdf2image\npytesseract\n```\n\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> Remember to install these yourself and restart the dev server! You may need to install extra non-python dependencies for `pdf2image`. See [here](https://pypi.org/project/pdf2image/). Feel free to write your own demo if you have trouble.\u003C/p>\n\n\n```python\nimport gradio as gr\nfrom gradio_pdf import PDF\nfrom pdf2image import convert_from_path\nfrom transformers import pipeline\nfrom pathlib import Path\n\ndir_ = Path(__file__).parent\n\np = pipeline(\n    \"document-question-answering\",\n    model=\"impira/layoutlm-document-qa\",\n)\n\ndef qa(question: str, doc: str) -> str:\n    img = convert_from_path(doc)[0]\n    output = p(img, question)\n    return sorted(output, key=lambda x: x[\"score\"], reverse=True)[0]['answer']\n\n\ndemo = gr.Interface(\n    qa,\n    [gr.Textbox(label=\"Question\"), PDF(label=\"Document\")],\n    gr.Textbox(),\n)\n\ndemo.launch()\n```\n\nSee our demo in action below!\n\n\u003Cvideo autoplay muted loop>\n  \u003Csource src=\"https://gradio-builds.s3.amazonaws.com/assets/pdf-guide/PDFDemo.mov\" type=\"video/mp4\" />\n\u003C/video>\n\nFinally lets build our component with `gradio cc build` and publish it with the `gradio cc publish` command!\nThis will guide you through the process of uploading your component to [PyPi](https://pypi.org/) and [HuggingFace Spaces](https://huggingface.co/spaces).\n\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> You may need to add the following lines to the `Dockerfile` of your HuggingFace Space.\u003C/p>\n\n```Dockerfile\nRUN mkdir -p /tmp/cache/\nRUN chmod a+rwx -R /tmp/cache/\nRUN apt-get update && apt-get install -y poppler-utils tesseract-ocr\n\nENV TRANSFORMERS_CACHE=/tmp/cache/\n```\n\n## Conclusion\n\nIn order to use our new component in **any** gradio 4.0 app, simply install it with pip, e.g. `pip install gradio-pdf`. Then you can use it like the built-in `gr.File()` component (except that it will only accept and display PDF files).\n\nHere is a simple demo with the Blocks api:\n\n```python\nimport gradio as gr\nfrom gradio_pdf import PDF\n\nwith gr.Blocks() as demo:\n    pdf = PDF(label=\"Upload a PDF\", interactive=True)\n    name = gr.Textbox()\n    pdf.upload(lambda f: f, pdf, name)\n\ndemo.launch()\n```\n\n\nI hope you enjoyed this tutorial!\nThe complete source code for our component is [here](https://huggingface.co/spaces/freddyaboulton/gradio_pdf/tree/main/src).\nPlease don't hesitate to reach out to the gradio community on the [HuggingFace Discord](https://discord.gg/hugging-face-879548962464493619) if you get stuck.",tags:[],spaces:[],url:"/guides/pdf-component-example/",contributor:null},{name:"multimodal-chatbot-part1",category:"custom-components",pretty_category:"Custom Components",guide_index:8,absolute_index:31,pretty_name:"Multimodal Chatbot Part1",content:"# Build a Custom Multimodal Chatbot - Part 1\n\nThis is the first in a two part series where we build a custom Multimodal Chatbot component.\nIn part 1, we will modify the Gradio Chatbot component to display text and media files (video, audio, image) in the same message.\nIn part 2, we will build a custom Textbox component that will be able to send multimodal messages (text and media files) to the chatbot.\n\nYou can follow along with the author of this post as he implements the chatbot component in the following YouTube video!\n\n\u003Ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IVJkOHTBPn0?si=bs-sBv43X-RVA8ly\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen>\u003C/iframe>\n\nHere's a preview of what our multimodal chatbot component will look like:\n\n![MultiModal Chatbot](https://gradio-builds.s3.amazonaws.com/assets/MultimodalChatbot.png)\n\n\n## Part 1 - Creating our project\n\nFor this demo we will be tweaking the existing Gradio `Chatbot` component to display text and media files in the same message.\nLet's create a new custom component directory by templating off of the `Chatbot` component source code.\n\n```bash\ngradio cc create MultimodalChatbot --template Chatbot\n```\n\nAnd we're ready to go!\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> Make sure to modify the `Author` key in the `pyproject.toml` file.\u003C/p>\n\n## Part 2a - The backend data_model\n\nOpen up the `multimodalchatbot.py` file in your favorite code editor and let's get started modifying the backend of our component.\n\nThe first thing we will do is create the `data_model` of our component.\nThe `data_model` is the data format that your python component will receive and send to the javascript client running the UI.\nYou can read more about the `data_model` in the [backend guide](./backend).\n\nFor our component, each chatbot message will consist of two keys: a `text` key that displays the text message and an optional list of media files that can be displayed underneath the text.\n\nImport the `FileData` and `GradioModel` classes from `gradio.data_classes` and modify the existing `ChatbotData` class to look like the following:\n\n```python\nclass FileMessage(GradioModel):\n    file: FileData\n    alt_text: Optional[str] = None\n\n\nclass MultimodalMessage(GradioModel):\n    text: Optional[str] = None\n    files: Optional[List[FileMessage]] = None\n\n\nclass ChatbotData(GradioRootModel):\n    root: List[Tuple[Optional[MultimodalMessage], Optional[MultimodalMessage]]]\n\n\nclass MultimodalChatbot(Component):\n    ...\n    data_model = ChatbotData\n```\n\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> The `data_model`s are implemented using `Pydantic V2`. Read the documentation [here](https://docs.pydantic.dev/latest/).\u003C/p>\n\nWe've done the hardest part already!\n\n## Part 2b - The pre and postprocess methods\n\nFor the `preprocess` method, we will keep it simple and pass a list of `MultimodalMessage`s to the python functions that use this component as input. \nThis will let users of our component access the chatbot data with `.text` and `.files` attributes.\nThis is a design choice that you can modify in your implementation!\nWe can return the list of messages with the `root` property of the `ChatbotData` like so:\n\n```python\ndef preprocess(\n    self,\n    payload: ChatbotData | None,\n) -> List[MultimodalMessage] | None:\n    if payload is None:\n        return payload\n    return payload.root\n```\n\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> Learn about the reasoning behind the `preprocess` and `postprocess` methods in the [key concepts guide](./key-component-concepts)\u003C/p>\n\nIn the `postprocess` method we will coerce each message returned by the python function to be a `MultimodalMessage` class. \nWe will also clean up any indentation in the `text` field so that it can be properly displayed as markdown in the frontend.\n\nWe can leave the `postprocess` method as is and modify the `_postprocess_chat_messages`\n\n```python\ndef _postprocess_chat_messages(\n    self, chat_message: MultimodalMessage | dict | None\n) -> MultimodalMessage | None:\n    if chat_message is None:\n        return None\n    if isinstance(chat_message, dict):\n        chat_message = MultimodalMessage(**chat_message)\n    chat_message.text = inspect.cleandoc(chat_message.text or \"\")\n    for file_ in chat_message.files:\n        file_.file.mime_type = client_utils.get_mimetype(file_.file.path)\n    return chat_message\n```\n\nBefore we wrap up with the backend code, let's modify the `example_value` and `example_payload` method to return a valid dictionary representation of the `ChatbotData`:\n\n```python\ndef example_value(self) -> Any:\n    return [[{\"text\": \"Hello!\", \"files\": []}, None]]\n\ndef example_payload(self) -> Any:\n    return [[{\"text\": \"Hello!\", \"files\": []}, None]]\n```\n\nCongrats - the backend is complete!\n\n## Part 3a - The Index.svelte file\n\nThe frontend for the `Chatbot` component is divided into two parts - the `Index.svelte` file and the `shared/Chatbot.svelte` file.\nThe `Index.svelte` file applies some processing to the data received from the server and then delegates the rendering of the conversation to the `shared/Chatbot.svelte` file.\nFirst we will modify the `Index.svelte` file to apply processing to the new data type the backend will return.\n\nLet's begin by porting our custom types  from our python `data_model` to typescript.\nOpen `frontend/shared/utils.ts` and add the following type definitions at the top of the file:\n\n```ts\nexport type FileMessage = {\n\tfile: FileData;\n\talt_text?: string;\n};\n\n\nexport type MultimodalMessage = {\n\ttext: string;\n\tfiles?: FileMessage[];\n}\n```\n\nNow let's import them in `Index.svelte` and modify the type annotations for `value` and `_value`.\n\n```ts\nimport type { FileMessage, MultimodalMessage } from \"./shared/utils\";\n\nexport let value: [\n    MultimodalMessage | null,\n    MultimodalMessage | null\n][] = [];\n\nlet _value: [\n    MultimodalMessage | null,\n    MultimodalMessage | null\n][];\n```\n\nWe need to normalize each message to make sure each file has a proper URL to fetch its contents from.\nWe also need to format any embedded file links in the `text` key.\nLet's add a `process_message` utility function and apply it whenever the `value` changes.\n\n```ts\nfunction process_message(msg: MultimodalMessage | null): MultimodalMessage | null {\n    if (msg === null) {\n        return msg;\n    }\n    msg.text = redirect_src_url(msg.text);\n    msg.files = msg.files.map(normalize_messages);\n    return msg;\n}\n\n$: _value = value\n    ? value.map(([user_msg, bot_msg]) => [\n            process_message(user_msg),\n            process_message(bot_msg)\n        ])\n    : [];\n```\n\n## Part 3b - the Chatbot.svelte file\n\nLet's begin similarly to the `Index.svelte` file and let's first modify the type annotations.\nImport `Mulimodal` message at the top of the `\u003Cscript>` section and use it to type the `value` and `old_value` variables.\n\n```ts\nimport type { MultimodalMessage } from \"./utils\";\n\nexport let value:\n    | [\n            MultimodalMessage | null,\n            MultimodalMessage | null\n        ][]\n    | null;\nlet old_value:\n    | [\n            MultimodalMessage | null,\n            MultimodalMessage | null\n        ][]\n    | null = null;\n```\n\nWe also need to modify the `handle_select` and `handle_like` functions:\n\n```ts\nfunction handle_select(\n    i: number,\n    j: number,\n    message: MultimodalMessage | null\n): void {\n    dispatch(\"select\", {\n        index: [i, j],\n        value: message\n    });\n}\n\nfunction handle_like(\n    i: number,\n    j: number,\n    message: MultimodalMessage | null,\n    liked: boolean\n): void {\n    dispatch(\"like\", {\n        index: [i, j],\n        value: message,\n        liked: liked\n    });\n}\n```\n\nNow for the fun part, actually rendering the text and files in the same message!\n\nYou should see some code like the following that determines whether a file or a markdown message should be displayed depending on the type of the message:\n\n```svelte\n{#if typeof message === \"string\"}\n    \u003CMarkdown\n        {message}\n        {latex_delimiters}\n        {sanitize_html}\n        {render_markdown}\n        {line_breaks}\n        on:load={scroll}\n    />\n{:else if message !== null && message.file?.mime_type?.includes(\"audio\")}\n    \u003Caudio\n        data-testid=\"chatbot-audio\"\n        controls\n        preload=\"metadata\"\n        ...\n```\n\nWe will modify this code to always display the text message and then loop through the files and display all of them that are present:\n\n```svelte\n\u003CMarkdown\n    message={message.text}\n    {latex_delimiters}\n    {sanitize_html}\n    {render_markdown}\n    {line_breaks}\n    on:load={scroll}\n/>\n{#each message.files as file, k}\n    {#if file !== null && file.file.mime_type?.includes(\"audio\")}\n        \u003Caudio\n            data-testid=\"chatbot-audio\"\n            controls\n            preload=\"metadata\"\n            src={file.file?.url}\n            title={file.alt_text}\n            on:play\n            on:pause\n            on:ended\n        />\n    {:else if message !== null && file.file?.mime_type?.includes(\"video\")}\n        \u003Cvideo\n            data-testid=\"chatbot-video\"\n            controls\n            src={file.file?.url}\n            title={file.alt_text}\n            preload=\"auto\"\n            on:play\n            on:pause\n            on:ended\n        >\n            \u003Ctrack kind=\"captions\" />\n        \u003C/video>\n    {:else if message !== null && file.file?.mime_type?.includes(\"image\")}\n        \u003Cimg\n            data-testid=\"chatbot-image\"\n            src={file.file?.url}\n            alt={file.alt_text}\n        />\n    {:else if message !== null && file.file?.url !== null}\n        \u003Ca\n            data-testid=\"chatbot-file\"\n            href={file.file?.url}\n            target=\"_blank\"\n            download={window.__is_colab__\n                ? null\n                : file.file?.orig_name || file.file?.path}\n        >\n            {file.file?.orig_name || file.file?.path}\n        \u003C/a>\n    {:else if pending_message && j === 1}\n        \u003CPending {layout} />\n    {/if}\n{/each}\n```\n\nWe did it! üéâ\n\n## Part 4 - The demo\n\nFor this tutorial, let's keep the demo simple and just display a static conversation between a hypothetical user and a bot.\nThis demo will show how both the user and the bot can send files. \nIn part 2 of this tutorial series we will build a fully functional chatbot demo!\n\nThe demo code will look like the following:\n\n```python\nimport gradio as gr\nfrom gradio_multimodalchatbot import MultimodalChatbot\nfrom gradio.data_classes import FileData\n\nuser_msg1 = {\"text\": \"Hello, what is in this image?\",\n             \"files\": [{\"file\": FileData(path=\"https://gradio-builds.s3.amazonaws.com/diffusion_image/cute_dog.jpg\")}]\n             }\nbot_msg1 = {\"text\": \"It is a very cute dog\",\n            \"files\": []}\n\nuser_msg2 = {\"text\": \"Describe this audio clip please.\",\n             \"files\": [{\"file\": FileData(path=\"cantina.wav\")}]}\nbot_msg2 = {\"text\": \"It is the cantina song from Star Wars\",\n            \"files\": []}\n\nuser_msg3 = {\"text\": \"Give me a video clip please.\",\n             \"files\": []}\nbot_msg3 = {\"text\": \"Here is a video clip of the world\",\n            \"files\": [{\"file\": FileData(path=\"world.mp4\")},\n                      {\"file\": FileData(path=\"cantina.wav\")}]}\n\nconversation = [[user_msg1, bot_msg1], [user_msg2, bot_msg2], [user_msg3, bot_msg3]]\n\nwith gr.Blocks() as demo:\n    MultimodalChatbot(value=conversation, height=800)\n\n\ndemo.launch()\n```\n\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> Change the filepaths so that they correspond to files on your machine. Also, if you are running in development mode, make sure the files are located in the top level of your custom component directory.\u003C/p>\n\n## Part 5 - Deploying and Conclusion\n\nLet's build and deploy our demo with `gradio cc build` and `gradio cc deploy`!\n\nYou can check out our component deployed to [HuggingFace Spaces](https://huggingface.co/spaces/freddyaboulton/gradio_multimodalchatbot) and all of the source code is available [here](https://huggingface.co/spaces/freddyaboulton/gradio_multimodalchatbot/tree/main/src).\n\nSee you in the next installment of this series!",tags:[],spaces:[],url:"/guides/multimodal-chatbot-part1/",contributor:null},{name:"documenting-custom-components",category:"custom-components",pretty_category:"Custom Components",guide_index:9,absolute_index:32,pretty_name:"Documenting Custom Components",content:"# Documenting Custom Components\n\nIn 4.15, we added a  new `gradio cc docs` command to the Gradio CLI to generate rich documentation for your custom component. This command will generate documentation for users automatically, but to get the most out of it, you need to do a few things.\n\n## How do I use it?\n\nThe documentation will be generated when running `gradio cc build`. You can pass the `--no-generate-docs` argument to turn off this behaviour.\n\nThere is also a standalone `docs` command that allows for greater customisation. If you are running this command manually it should be run _after_ the `version` in your `pyproject.toml` has been bumped but before building the component.\n\nAll arguments are optional.\n\n```bash\ngradio cc docs\n  path # The directory of the custom component.\n  --demo-dir # Path to the demo directory.\n  --demo-name # Name of the demo file\n  --space-url # URL of the Hugging Face Space to link to\n  --generate-space # create a documentation space.\n  --no-generate-space # do not create a documentation space\n  --readme-path # Path to the README.md file.\n  --generate-readme # create a REAMDE.md file\n  --no-generate-readme # do not create a README.md file\n  --suppress-demo-check # suppress validation checks and warnings\n```\n\n## What gets generated?\n\nThe `gradio cc docs` command will generate an interactive Gradio app and a static README file with various features. You can see an example here:\n\n- [Gradio app deployed on Hugging Face Spaces]()\n- [README.md rendered by GitHub]()\n\nThe README.md and space both have the following features:\n\n- A description.\n- Installation instructions.\n- A fully functioning code snippet.\n- Optional links to PyPi, GitHub, and Hugging Face Spaces.\n- API documentation including:\n  - An argument table for component initialisation showing types, defaults, and descriptions.\n  - A description of how the component affects the user's predict function.\n  - A table of events and their descriptions.\n  - Any additional interfaces or classes that may be used during initialisation or in the pre- or post- processors.\n\nAdditionally, the Gradio includes:\n\n- A live demo.\n- A richer, interactive version of the parameter tables.\n- Nicer styling!\n\n## What do I need to do?\n\nThe documentation generator uses existing standards to extract the necessary information, namely Type Hints and Docstrings. There are no Gradio-specific APIs for documentation, so following best practices will generally yield the best results.\n\nIf you already use type hints and docstrings in your component source code, you don't need to do much to benefit from this feature, but there are some details that you should be aware of.\n\n### Python version\n\nTo get the best documentation experience, you need to use Python `3.10` or greater when generating documentation. This is because some introspection features used to generate the documentation were only added in `3.10`.\n\n### Type hints\n\nPython type hints are used extensively to provide helpful information for users. \n\n\u003Cdetails> \n\u003Csummary> What are type hints?\u003C/summary>\n\n\nIf you need to become more familiar with type hints in Python, they are a simple way to express what Python types are expected for arguments and return values of functions and methods. They provide a helpful in-editor experience, aid in maintenance, and integrate with various other tools. These types can be simple primitives, like `list` `str` `bool`; they could be more compound types like `list[str]`, `str | None` or `tuple[str, float | int]`; or they can be more complex types using utility classed like [`TypedDict`](https://peps.python.org/pep-0589/#abstract).\n\n[Read more about type hints in Python.](https://realpython.com/lessons/type-hinting/)\n\n\n\u003C/details>\n\n#### What do I need to add hints to?\n\nYou do not need to add type hints to every part of your code. For the documentation to work correctly, you will need to add type hints to the following component methods:\n\n- `__init__` parameters should be typed.\n- `postprocess` parameters and return value should be typed.\n- `preprocess` parameters and return value should be typed.\n\nIf you are using `gradio cc create`, these types should already exist, but you may need to tweak them based on any changes you make.\n\n##### `__init__`\n\nHere, you only need to type the parameters. If you have cloned a template with `gradio` cc create`, these should already be in place. You will only need to add new hints for anything you have added or changed:\n\n```py\ndef __init__(\n  self,\n  value: str | None = None,\n  *,\n  sources: Literal[\"upload\", \"microphone\"] = \"upload,\n  every: float | None = None,\n  ...\n):\n  ...\n```\n\n##### `preprocess` and `postprocess`\n\nThe `preprocess` and `postprocess` methods determine the value passed to the user function and the value that needs to be returned.\n\nEven if the design of your component is primarily as an input or an output, it is worth adding type hints to both the input parameters and the return values because Gradio has no way of limiting how components can be used.\n\nIn this case, we specifically care about:\n\n- The return type of `preprocess`.\n- The input type of `postprocess`.\n\n```py\ndef preprocess(\n  self, payload: FileData | None # input is optional\n) -> tuple[int, str] | str | None:\n\n# user function input  is the preprocess return ‚ñ≤\n# user function output is the postprocess input ‚ñº\n\ndef postprocess(\n  self, value: tuple[int, str] | None\n) -> FileData | bytes | None: # return is optional\n  ...\n```\n\n### Docstrings\n\nDocstrings are also used extensively to extract more meaningful, human-readable descriptions of certain parts of the API.\n\n\u003Cdetails> \n\u003Csummary> What are docstrings?\u003C/summary>\n\n\nIf you need to become more familiar with docstrings in Python, they are a way to annotate parts of your code with human-readable decisions and explanations. They offer a rich in-editor experience like type hints, but unlike type hints, they don't have any specific syntax requirements. They are simple strings and can take almost any form. The only requirement is where they appear. Docstrings should be \"a string literal that occurs as the first statement in a module, function, class, or method definition\".\n\n[Read more about Python docstrings.](https://peps.python.org/pep-0257/#what-is-a-docstring)\n\n\u003C/details>\n\nWhile docstrings don't have any syntax requirements, we need a particular structure for documentation purposes.\n\nAs with type hint, the specific information we care about is as follows:\n\n- `__init__` parameter docstrings.\n- `preprocess` return docstrings.\n- `postprocess` input parameter docstrings.\n\nEverything else is optional.\n\nDocstrings should always take this format to be picked up by the documentation generator:\n\n#### Classes\n\n```py\n\"\"\"\nA description of the class.\n\nThis can span multiple lines and can _contain_ *markdown*.\n\"\"\"\n```\n\n#### Methods and functions \n\nMarkdown in these descriptions will not be converted into formatted text.\n\n```py\n\"\"\"\nParameters:\n    param_one: A description for this parameter.\n    param_two: A description for this parameter.\nReturns:\n    A description for this return value.\n\"\"\"\n```\n\n### Events\n\nIn custom components, events are expressed as a list stored on the `events` field of the component class. While we do not need types for events, we _do_ need a human-readable description so users can understand the behaviour of the event.\n\nTo facilitate this, we must create the event in a specific way.\n\nThere are two ways to add events to a custom component.\n\n#### Built-in events\n\nGradio comes with a variety of built-in events that may be enough for your component. If you are using built-in events, you do not need to do anything as they already have descriptions we can extract:\n\n```py\nfrom gradio.events import Events\n\nclass ParamViewer(Component):\n  ...\n\n  EVENTS = [\n    Events.change,\n    Events.upload,\n  ]\n```\n\n#### Custom events\n\nYou can define a custom event if the built-in events are unsuitable for your use case. This is a straightforward process, but you must create the event in this way for docstrings to work correctly:\n\n```py\nfrom gradio.events import Events, EventListener\n\nclass ParamViewer(Component):\n  ...\n\n  EVENTS = [\n    Events.change,\n    EventListener(\n        \"bingbong\",\n        doc=\"This listener is triggered when the user does a bingbong.\"\n      )\n  ]\n```\n\n### Demo\n\nThe `demo/app.py`, often used for developing the component, generates the live demo and code snippet. The only strict rule here is that the `demo.launch()` command must be contained with a `__name__ == \"__main__\"` conditional as below:\n\n```py\nif __name__ == \"__main__\":\n  demo.launch()\n```\n\nThe documentation generator will scan for such a clause and error if absent. If you are _not_ launching the demo inside the `demo/app.py`, then you can pass `--suppress-demo-check` to turn off this check.\n\n#### Demo recommendations\n\nAlthough there are no additional rules, there are some best practices you should bear in mind to get the best experience from the documentation generator.\n\nThese are only guidelines, and every situation is unique, but they are sound principles to remember.\n\n##### Keep the demo compact\n\nCompact demos look better and make it easier for users to understand what the demo does. Try to remove as many extraneous UI elements as possible to focus the users' attention on the core use case. \n\nSometimes, it might make sense to have a `demo/app.py` just for the docs and an additional, more complex app for your testing purposes. You can also create other spaces, showcasing more complex examples and linking to them from the main class docstring or the `pyproject.toml` description.\n\n#### Keep the code concise\n\nThe 'getting started' snippet utilises the demo code, which should be as short as possible to keep users engaged and avoid confusion.\n\nIt isn't the job of the sample snippet to demonstrate the whole API; this snippet should be the shortest path to success for a new user. It should be easy to type or copy-paste and easy to understand. Explanatory comments should be brief and to the point.\n\n#### Avoid external dependencies\n\nAs mentioned above, users should be able to copy-paste a snippet and have a fully working app. Try to avoid third-party library dependencies to facilitate this.\n\nYou should carefully consider any examples; avoiding examples that require additional files or that make assumptions about the environment is generally a good idea.\n\n#### Ensure the `demo` directory is self-contained\n\nOnly the `demo` directory will be uploaded to Hugging Face spaces in certain instances, as the component will be installed via PyPi if possible. It is essential that this directory is self-contained and any files needed for the correct running of the demo are present.\n\n### Additional URLs\n\nThe documentation generator will generate a few buttons, providing helpful information and links to users. They are obtained automatically in some cases, but some need to be explicitly included in the `pyproject.yaml`. \n\n- PyPi Version and link - This is generated automatically.\n- GitHub Repository - This is populated via the `pyproject.toml`'s `project.urls.repository`.\n- Hugging Face Space - This is populated via the `pyproject.toml`'s `project.urls.space`.\n\nAn example `pyproject.toml` urls section might look like this:\n\n```toml\n[project.urls]\nrepository = \"https://github.com/user/repo-name\"\nspace = \"https://huggingface.co/spaces/user/space-name\"\n```",tags:[],spaces:[],url:"/guides/documenting-custom-components/",contributor:null}]},{category:"Tabular Data Science And Plots",guides:[{name:"connecting-to-a-database",category:"tabular-data-science-and-plots",pretty_category:"Tabular Data Science And Plots",guide_index:1,absolute_index:33,pretty_name:"Connecting To A Database",content:"# Connecting to a Database\n\n\n\n\n## Introduction\n\nThis guide explains how you can use Gradio to connect your app to a database. We will be\nconnecting to a PostgreSQL database hosted on AWS but gradio is completely agnostic to the type of\ndatabase you are connecting to and where it's hosted. So as long as you can write python code to connect\nto your data, you can display it in a web UI with gradio üí™\n\n## Overview\n\nWe will be analyzing bike share data from Chicago. The data is hosted on kaggle [here](https://www.kaggle.com/datasets/evangower/cyclistic-bike-share?select=202203-divvy-tripdata.csv).\nOur goal is to create a dashboard that will enable our business stakeholders to answer the following questions:\n\n1. Are electric bikes more popular than regular bikes?\n2. What are the top 5 most popular departure bike stations?\n\nAt the end of this guide, we will have a functioning application that looks like this:\n\n\u003Cgradio-app space=\"gradio/chicago-bikeshare-dashboard\"> \u003C/gradio-app>\n\n## Step 1 - Creating your database\n\nWe will be storing our data on a PostgreSQL hosted on Amazon's RDS service. Create an AWS account if you don't already have one\nand create a PostgreSQL database on the free tier.\n\n**Important**: If you plan to host this demo on HuggingFace Spaces, make sure database is on port **8080**. Spaces will\nblock all outgoing connections unless they are made to port 80, 443, or 8080 as noted [here](https://huggingface.co/docs/hub/spaces-overview#networking).\nRDS will not let you create a postgreSQL instance on ports 80 or 443.\n\nOnce your database is created, download the dataset from Kaggle and upload it to your database.\nFor the sake of this demo, we will only upload March 2022 data.\n\n## Step 2.a - Write your ETL code\n\nWe will be querying our database for the total count of rides split by the type of bicycle (electric, standard, or docked).\nWe will also query for the total count of rides that depart from each station and take the top 5.\n\nWe will then take the result of our queries and visualize them in with matplotlib.\n\nWe will use the pandas [read_sql](https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html)\nmethod to connect to the database. This requires the `psycopg2` library to be installed.\n\nIn order to connect to our database, we will specify the database username, password, and host as environment variables.\nThis will make our app more secure by avoiding storing sensitive information as plain text in our application files.\n\n```python\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nDB_USER = os.getenv(\"DB_USER\")\nDB_PASSWORD = os.getenv(\"DB_PASSWORD\")\nDB_HOST = os.getenv(\"DB_HOST\")\nPORT = 8080\nDB_NAME = \"bikeshare\"\n\nconnection_string = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}?port={PORT}&dbname={DB_NAME}\"\n\ndef get_count_ride_type():\n    df = pd.read_sql(\n    \"\"\"\n        SELECT COUNT(ride_id) as n, rideable_type\n        FROM rides\n        GROUP BY rideable_type\n        ORDER BY n DESC\n    \"\"\",\n    con=connection_string\n    )\n    fig_m, ax = plt.subplots()\n    ax.bar(x=df['rideable_type'], height=df['n'])\n    ax.set_title(\"Number of rides by bycycle type\")\n    ax.set_ylabel(\"Number of Rides\")\n    ax.set_xlabel(\"Bicycle Type\")\n    return fig_m\n\n\ndef get_most_popular_stations():\n\n    df = pd.read_sql(\n        \"\"\"\n    SELECT COUNT(ride_id) as n, MAX(start_station_name) as station\n    FROM RIDES\n    WHERE start_station_name is NOT NULL\n    GROUP BY start_station_id\n    ORDER BY n DESC\n    LIMIT 5\n    \"\"\",\n    con=connection_string\n    )\n    fig_m, ax = plt.subplots()\n    ax.bar(x=df['station'], height=df['n'])\n    ax.set_title(\"Most popular stations\")\n    ax.set_ylabel(\"Number of Rides\")\n    ax.set_xlabel(\"Station Name\")\n    ax.set_xticklabels(\n        df['station'], rotation=45, ha=\"right\", rotation_mode=\"anchor\"\n    )\n    ax.tick_params(axis=\"x\", labelsize=8)\n    fig_m.tight_layout()\n    return fig_m\n```\n\nIf you were to run our script locally, you could pass in your credentials as environment variables like so\n\n```bash\nDB_USER='username' DB_PASSWORD='password' DB_HOST='host' python app.py\n```\n\n## Step 2.c - Write your gradio app\n\nWe will display or matplotlib plots in two separate `gr.Plot` components displayed side by side using `gr.Row()`.\nBecause we have wrapped our function to fetch the data in a `demo.load()` event trigger,\nour demo will fetch the latest data **dynamically** from the database each time the web page loads. ü™Ñ\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        bike_type = gr.Plot()\n        station = gr.Plot()\n\n    demo.load(get_count_ride_type, inputs=None, outputs=bike_type)\n    demo.load(get_most_popular_stations, inputs=None, outputs=station)\n\ndemo.launch()\n```\n\n## Step 3 - Deployment\n\nIf you run the code above, your app will start running locally.\nYou can even get a temporary shareable link by passing the `share=True` parameter to `launch`.\n\nBut what if you want to a permanent deployment solution?\nLet's deploy our Gradio app to the free HuggingFace Spaces platform.\n\nIf you haven't used Spaces before, follow the previous guide [here](/using_hugging_face_integrations).\nYou will have to add the `DB_USER`, `DB_PASSWORD`, and `DB_HOST` variables as \"Repo Secrets\". You can do this in the \"Settings\" tab.\n\n![secrets](https://github.com/gradio-app/gradio/blob/main/guides/assets/secrets.png?raw=true)\n\n## Conclusion\n\nCongratulations! You know how to connect your gradio app to a database hosted on the cloud! ‚òÅÔ∏è\n\nOur dashboard is now running on [Spaces](https://huggingface.co/spaces/gradio/chicago-bikeshare-dashboard).\nThe complete code is [here](https://huggingface.co/spaces/gradio/chicago-bikeshare-dashboard/blob/main/app.py)\n\nAs you can see, gradio gives you the power to connect to your data wherever it lives and display however you want! üî•\n",tags:["TABULAR","PLOTS"],spaces:["https://huggingface.co/spaces/gradio/chicago-bikeshare-dashboard"],url:"/guides/connecting-to-a-database/",contributor:null},{name:"creating-a-dashboard-from-bigquery-data",category:"tabular-data-science-and-plots",pretty_category:"Tabular Data Science And Plots",guide_index:null,absolute_index:34,pretty_name:"Creating A Dashboard From Bigquery Data",content:"# Creating a Real-Time Dashboard from BigQuery Data\n\n\n\n[Google BigQuery](https://cloud.google.com/bigquery) is a cloud-based service for processing very large data sets. It is a serverless and highly scalable data warehousing solution that enables users to analyze data [using SQL-like queries](https://www.oreilly.com/library/view/google-bigquery-the/9781492044451/ch01.html).\n\nIn this tutorial, we will show you how to query a BigQuery dataset in Python and display the data in a dashboard that updates in real time using `gradio`. The dashboard will look like this:\n\n\u003Cimg src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/bigquery-dashboard.gif\">\n\nWe'll cover the following steps in this Guide:\n\n1. Setting up your BigQuery credentials\n2. Using the BigQuery client\n3. Building the real-time dashboard (in just _7 lines of Python_)\n\nWe'll be working with the [New York Times' COVID dataset](https://www.nytimes.com/interactive/2021/us/covid-cases.html) that is available as a public dataset on BigQuery. The dataset, named `covid19_nyt.us_counties` contains the latest information about the number of confirmed cases and deaths from COVID across US counties.\n\n**Prerequisites**: This Guide uses [Gradio Blocks](/guides/quickstart/#blocks-more-flexibility-and-control), so make your are familiar with the Blocks class.\n\n## Setting up your BigQuery Credentials\n\nTo use Gradio with BigQuery, you will need to obtain your BigQuery credentials and use them with the [BigQuery Python client](https://pypi.org/project/google-cloud-bigquery/). If you already have BigQuery credentials (as a `.json` file), you can skip this section. If not, you can do this for free in just a couple of minutes.\n\n1. First, log in to your Google Cloud account and go to the Google Cloud Console (https://console.cloud.google.com/)\n\n2. In the Cloud Console, click on the hamburger menu in the top-left corner and select \"APIs & Services\" from the menu. If you do not have an existing project, you will need to create one.\n\n3. Then, click the \"+ Enabled APIs & services\" button, which allows you to enable specific services for your project. Search for \"BigQuery API\", click on it, and click the \"Enable\" button. If you see the \"Manage\" button, then the BigQuery is already enabled, and you're all set.\n\n4. In the APIs & Services menu, click on the \"Credentials\" tab and then click on the \"Create credentials\" button.\n\n5. In the \"Create credentials\" dialog, select \"Service account key\" as the type of credentials to create, and give it a name. Also grant the service account permissions by giving it a role such as \"BigQuery User\", which will allow you to run queries.\n\n6. After selecting the service account, select the \"JSON\" key type and then click on the \"Create\" button. This will download the JSON key file containing your credentials to your computer. It will look something like this:\n\n```json\n{\n\t\"type\": \"service_account\",\n\t\"project_id\": \"your project\",\n\t\"private_key_id\": \"your private key id\",\n\t\"private_key\": \"private key\",\n\t\"client_email\": \"email\",\n\t\"client_id\": \"client id\",\n\t\"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n\t\"token_uri\": \"https://accounts.google.com/o/oauth2/token\",\n\t\"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n\t\"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/email_id\"\n}\n```\n\n## Using the BigQuery Client\n\nOnce you have the credentials, you will need to use the BigQuery Python client to authenticate using your credentials. To do this, you will need to install the BigQuery Python client by running the following command in the terminal:\n\n```bash\npip install google-cloud-bigquery[pandas]\n```\n\nYou'll notice that we've installed the pandas add-on, which will be helpful for processing the BigQuery dataset as a pandas dataframe. Once the client is installed, you can authenticate using your credentials by running the following code:\n\n```py\nfrom google.cloud import bigquery\n\nclient = bigquery.Client.from_service_account_json(\"path/to/key.json\")\n```\n\nWith your credentials authenticated, you can now use the BigQuery Python client to interact with your BigQuery datasets.\n\nHere is an example of a function which queries the `covid19_nyt.us_counties` dataset in BigQuery to show the top 20 counties with the most confirmed cases as of the current day:\n\n```py\nimport numpy as np\n\nQUERY = (\n    'SELECT * FROM `bigquery-public-data.covid19_nyt.us_counties` '\n    'ORDER BY date DESC,confirmed_cases DESC '\n    'LIMIT 20')\n\ndef run_query():\n    query_job = client.query(QUERY)\n    query_result = query_job.result()\n    df = query_result.to_dataframe()\n    # Select a subset of columns\n    df = df[[\"confirmed_cases\", \"deaths\", \"county\", \"state_name\"]]\n    # Convert numeric columns to standard numpy types\n    df = df.astype({\"deaths\": np.int64, \"confirmed_cases\": np.int64})\n    return df\n```\n\n## Building the Real-Time Dashboard\n\nOnce you have a function to query the data, you can use the `gr.DataFrame` component from the Gradio library to display the results in a tabular format. This is a useful way to inspect the data and make sure that it has been queried correctly.\n\nHere is an example of how to use the `gr.DataFrame` component to display the results. By passing in the `run_query` function to `gr.DataFrame`, we instruct Gradio to run the function as soon as the page loads and show the results. In addition, you also pass in the keyword `every` to tell the dashboard to refresh every hour (60\\*60 seconds).\n\n```py\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.DataFrame(run_query, every=60*60)\n\ndemo.queue().launch()  # Run the demo using queuing\n```\n\nPerhaps you'd like to add a visualization to our dashboard. You can use the `gr.ScatterPlot()` component to visualize the data in a scatter plot. This allows you to see the relationship between different variables such as case count and case deaths in the dataset and can be useful for exploring the data and gaining insights. Again, we can do this in real-time\nby passing in the `every` parameter.\n\nHere is a complete example showing how to use the `gr.ScatterPlot` to visualize in addition to displaying data with the `gr.DataFrame`\n\n```py\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# üíâ Covid Dashboard (Updated Hourly)\")\n    with gr.Row():\n        gr.DataFrame(run_query, every=60*60)\n        gr.ScatterPlot(run_query, every=60*60, x=\"confirmed_cases\",\n                        y=\"deaths\", tooltip=\"county\", width=500, height=500)\n\ndemo.queue().launch()  # Run the demo with queuing enabled\n```\n",tags:["TABULAR","DASHBOARD","PLOTS"],spaces:[],url:"/guides/creating-a-dashboard-from-bigquery-data/",contributor:null},{name:"creating-a-dashboard-from-supabase-data",category:"tabular-data-science-and-plots",pretty_category:"Tabular Data Science And Plots",guide_index:null,absolute_index:35,pretty_name:"Creating A Dashboard From Supabase Data",content:"# Create a Dashboard from Supabase Data\n\n\n\n[Supabase](https://supabase.com/) is a cloud-based open-source backend that provides a PostgreSQL database, authentication, and other useful features for building web and mobile applications. In this tutorial, you will learn how to read data from Supabase and plot it in **real-time** on a Gradio Dashboard.\n\n**Prerequisites:** To start, you will need a free Supabase account, which you can sign up for here: [https://app.supabase.com/](https://app.supabase.com/)\n\nIn this end-to-end guide, you will learn how to:\n\n- Create tables in Supabase\n- Write data to Supabase using the Supabase Python Client\n- Visualize the data in a real-time dashboard using Gradio\n\nIf you already have data on Supabase that you'd like to visualize in a dashboard, you can skip the first two sections and go directly to [visualizing the data](#visualize-the-data-in-a-real-time-gradio-dashboard)!\n\n## Create a table in Supabase\n\nFirst of all, we need some data to visualize. Following this [excellent guide](https://supabase.com/blog/loading-data-supabase-python), we'll create fake commerce data and put it in Supabase.\n\n1\\. Start by creating a new project in Supabase. Once you're logged in, click the \"New Project\" button\n\n2\\. Give your project a name and database password. You can also choose a pricing plan (for our purposes, the Free Tier is sufficient!)\n\n3\\. You'll be presented with your API keys while the database spins up (can take up to 2 minutes).\n\n4\\. Click on \"Table Editor\" (the table icon) in the left pane to create a new table. We'll create a single table called `Product`, with the following schema:\n\n\u003Ccenter>\n\u003Ctable>\n\u003Ctr>\u003Ctd>product_id\u003C/td>\u003Ctd>int8\u003C/td>\u003C/tr>\n\u003Ctr>\u003Ctd>inventory_count\u003C/td>\u003Ctd>int8\u003C/td>\u003C/tr>\n\u003Ctr>\u003Ctd>price\u003C/td>\u003Ctd>float8\u003C/td>\u003C/tr>\n\u003Ctr>\u003Ctd>product_name\u003C/td>\u003Ctd>varchar\u003C/td>\u003C/tr>\n\u003C/table>\n\u003C/center>\n\n5\\. Click Save to save the table schema.\n\nOur table is now ready!\n\n## Write data to Supabase\n\nThe next step is to write data to a Supabase dataset. We will use the Supabase Python library to do this.\n\n6\\. Install `supabase` by running the following command in your terminal:\n\n```bash\npip install supabase\n```\n\n7\\. Get your project URL and API key. Click the Settings (gear icon) on the left pane and click 'API'. The URL is listed in the Project URL box, while the API key is listed in Project API keys (with the tags `service_role`, `secret`)\n\n8\\. Now, run the following Python script to write some fake data to the table (note you have to put the values of `SUPABASE_URL` and `SUPABASE_SECRET_KEY` from step 7):\n\n```python\nimport supabase\n\n# Initialize the Supabase client\nclient = supabase.create_client('SUPABASE_URL', 'SUPABASE_SECRET_KEY')\n\n# Define the data to write\nimport random\n\nmain_list = []\nfor i in range(10):\n    value = {'product_id': i,\n             'product_name': f\"Item {i}\",\n             'inventory_count': random.randint(1, 100),\n             'price': random.random()*100\n            }\n    main_list.append(value)\n\n# Write the data to the table\ndata = client.table('Product').insert(main_list).execute()\n```\n\nReturn to your Supabase dashboard and refresh the page, you should now see 10 rows populated in the `Product` table!\n\n## Visualize the Data in a Real-Time Gradio Dashboard\n\nFinally, we will read the data from the Supabase dataset using the same `supabase` Python library and create a realtime dashboard using `gradio`.\n\nNote: We repeat certain steps in this section (like creating the Supabase client) in case you did not go through the previous sections. As described in Step 7, you will need the project URL and API Key for your database.\n\n9\\. Write a function that loads the data from the `Product` table and returns it as a pandas Dataframe:\n\n```python\nimport supabase\nimport pandas as pd\n\nclient = supabase.create_client('SUPABASE_URL', 'SUPABASE_SECRET_KEY')\n\ndef read_data():\n    response = client.table('Product').select(\"*\").execute()\n    df = pd.DataFrame(response.data)\n    return df\n```\n\n10\\. Create a small Gradio Dashboard with 2 Barplots that plots the prices and inventories of all of the items every minute and updates in real-time:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as dashboard:\n    with gr.Row():\n        gr.BarPlot(read_data, x=\"product_id\", y=\"price\", title=\"Prices\", every=60)\n        gr.BarPlot(read_data, x=\"product_id\", y=\"inventory_count\", title=\"Inventory\", every=60)\n\ndashboard.queue().launch()\n```\n\nNotice that by passing in a function to `gr.BarPlot()`, we have the BarPlot query the database as soon as the web app loads (and then again every 60 seconds because of the `every` parameter). Your final dashboard should look something like this:\n\n\u003Cgradio-app space=\"abidlabs/supabase\">\u003C/gradio-app>\n\n## Conclusion\n\nThat's it! In this tutorial, you learned how to write data to a Supabase dataset, and then read that data and plot the results as bar plots. If you update the data in the Supabase database, you'll notice that the Gradio dashboard will update within a minute.\n\nTry adding more plots and visualizations to this example (or with a different dataset) to build a more complex dashboard!\n",tags:["TABULAR","DASHBOARD","PLOTS"],spaces:[],url:"/guides/creating-a-dashboard-from-supabase-data/",contributor:null},{name:"creating-a-realtime-dashboard-from-google-sheets",category:"tabular-data-science-and-plots",pretty_category:"Tabular Data Science And Plots",guide_index:null,absolute_index:36,pretty_name:"Creating A Realtime Dashboard From Google Sheets",content:"# Creating a Real-Time Dashboard from Google Sheets\n\n\n\n[Google Sheets](https://www.google.com/sheets/about/) are an easy way to store tabular data in the form of spreadsheets. With Gradio and pandas, it's easy to read data from public or private Google Sheets and then display the data or plot it. In this blog post, we'll build a small _real-time_ dashboard, one that updates when the data in the Google Sheets updates.\n\nBuilding the dashboard itself will just be 9 lines of Python code using Gradio, and our final dashboard will look like this:\n\n\u003Cgradio-app space=\"gradio/line-plot\">\u003C/gradio-app>\n\n**Prerequisites**: This Guide uses [Gradio Blocks](/guides/quickstart/#blocks-more-flexibility-and-control), so make you are familiar with the Blocks class.\n\nThe process is a little different depending on if you are working with a publicly accessible or a private Google Sheet. We'll cover both, so let's get started!\n\n## Public Google Sheets\n\nBuilding a dashboard from a public Google Sheet is very easy, thanks to the [`pandas` library](https://pandas.pydata.org/):\n\n1\\. Get the URL of the Google Sheets that you want to use. To do this, simply go to the Google Sheets, click on the \"Share\" button in the top-right corner, and then click on the \"Get shareable link\" button. This will give you a URL that looks something like this:\n\n```html\nhttps://docs.google.com/spreadsheets/d/1UoKzzRzOCt-FXLLqDKLbryEKEgllGAQUEJ5qtmmQwpU/edit#gid=0\n```\n\n2\\. Now, let's modify this URL and then use it to read the data from the Google Sheets into a Pandas DataFrame. (In the code below, replace the `URL` variable with the URL of your public Google Sheet):\n\n```python\nimport pandas as pd\n\nURL = \"https://docs.google.com/spreadsheets/d/1UoKzzRzOCt-FXLLqDKLbryEKEgllGAQUEJ5qtmmQwpU/edit#gid=0\"\ncsv_url = URL.replace('/edit#gid=', '/export?format=csv&gid=')\n\ndef get_data():\n    return pd.read_csv(csv_url)\n```\n\n3\\. The data query is a function, which means that it's easy to display it real-time using the `gr.DataFrame` component, or plot it real-time using the `gr.LinePlot` component (of course, depending on the data, a different plot may be appropriate). To do this, just pass the function into the respective components, and set the `every` parameter based on how frequently (in seconds) you would like the component to refresh. Here's the Gradio code:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# üìà Real-Time Line Plot\")\n    with gr.Row():\n        with gr.Column():\n            gr.DataFrame(get_data, every=5)\n        with gr.Column():\n            gr.LinePlot(get_data, every=5, x=\"Date\", y=\"Sales\", y_title=\"Sales ($ millions)\", overlay_point=True, width=500, height=500)\n\ndemo.queue().launch()  # Run the demo with queuing enabled\n```\n\nAnd that's it! You have a dashboard that refreshes every 5 seconds, pulling the data from your Google Sheet.\n\n## Private Google Sheets\n\nFor private Google Sheets, the process requires a little more work, but not that much! The key difference is that now, you must authenticate yourself to authorize access to the private Google Sheets.\n\n### Authentication\n\nTo authenticate yourself, obtain credentials from Google Cloud. Here's [how to set up google cloud credentials](https://developers.google.com/workspace/guides/create-credentials):\n\n1\\. First, log in to your Google Cloud account and go to the Google Cloud Console (https://console.cloud.google.com/)\n\n2\\. In the Cloud Console, click on the hamburger menu in the top-left corner and select \"APIs & Services\" from the menu. If you do not have an existing project, you will need to create one.\n\n3\\. Then, click the \"+ Enabled APIs & services\" button, which allows you to enable specific services for your project. Search for \"Google Sheets API\", click on it, and click the \"Enable\" button. If you see the \"Manage\" button, then Google Sheets is already enabled, and you're all set.\n\n4\\. In the APIs & Services menu, click on the \"Credentials\" tab and then click on the \"Create credentials\" button.\n\n5\\. In the \"Create credentials\" dialog, select \"Service account key\" as the type of credentials to create, and give it a name. **Note down the email of the service account**\n\n6\\. After selecting the service account, select the \"JSON\" key type and then click on the \"Create\" button. This will download the JSON key file containing your credentials to your computer. It will look something like this:\n\n```json\n{\n\t\"type\": \"service_account\",\n\t\"project_id\": \"your project\",\n\t\"private_key_id\": \"your private key id\",\n\t\"private_key\": \"private key\",\n\t\"client_email\": \"email\",\n\t\"client_id\": \"client id\",\n\t\"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n\t\"token_uri\": \"https://accounts.google.com/o/oauth2/token\",\n\t\"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n\t\"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/email_id\"\n}\n```\n\n### Querying\n\nOnce you have the credentials `.json` file, you can use the following steps to query your Google Sheet:\n\n1\\. Click on the \"Share\" button in the top-right corner of the Google Sheet. Share the Google Sheets with the email address of the service from Step 5 of authentication subsection (this step is important!). Then click on the \"Get shareable link\" button. This will give you a URL that looks something like this:\n\n```html\nhttps://docs.google.com/spreadsheets/d/1UoKzzRzOCt-FXLLqDKLbryEKEgllGAQUEJ5qtmmQwpU/edit#gid=0\n```\n\n2\\. Install the [`gspread` library](https://docs.gspread.org/en/v5.7.0/), which makes it easy to work with the [Google Sheets API](https://developers.google.com/sheets/api/guides/concepts) in Python by running in the terminal: `pip install gspread`\n\n3\\. Write a function to load the data from the Google Sheet, like this (replace the `URL` variable with the URL of your private Google Sheet):\n\n```python\nimport gspread\nimport pandas as pd\n\n# Authenticate with Google and get the sheet\nURL = 'https://docs.google.com/spreadsheets/d/1_91Vps76SKOdDQ8cFxZQdgjTJiz23375sAT7vPvaj4k/edit#gid=0'\n\ngc = gspread.service_account(\"path/to/key.json\")\nsh = gc.open_by_url(URL)\nworksheet = sh.sheet1\n\ndef get_data():\n    values = worksheet.get_all_values()\n    df = pd.DataFrame(values[1:], columns=values[0])\n    return df\n\n```\n\n4\\. The data query is a function, which means that it's easy to display it real-time using the `gr.DataFrame` component, or plot it real-time using the `gr.LinePlot` component (of course, depending on the data, a different plot may be appropriate). To do this, we just pass the function into the respective components, and set the `every` parameter based on how frequently (in seconds) we would like the component to refresh. Here's the Gradio code:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# üìà Real-Time Line Plot\")\n    with gr.Row():\n        with gr.Column():\n            gr.DataFrame(get_data, every=5)\n        with gr.Column():\n            gr.LinePlot(get_data, every=5, x=\"Date\", y=\"Sales\", y_title=\"Sales ($ millions)\", overlay_point=True, width=500, height=500)\n\ndemo.queue().launch()  # Run the demo with queuing enabled\n```\n\nYou now have a Dashboard that refreshes every 5 seconds, pulling the data from your Google Sheet.\n\n## Conclusion\n\nAnd that's all there is to it! With just a few lines of code, you can use `gradio` and other libraries to read data from a public or private Google Sheet and then display and plot the data in a real-time dashboard.\n",tags:["TABULAR","DASHBOARD","PLOTS"],spaces:[],url:"/guides/creating-a-realtime-dashboard-from-google-sheets/",contributor:null},{name:"plot-component-for-maps",category:"tabular-data-science-and-plots",pretty_category:"Tabular Data Science And Plots",guide_index:null,absolute_index:37,pretty_name:"Plot Component For Maps",content:"# How to Use the Plot Component for Maps\n\n\n\n## Introduction\n\nThis guide explains how you can use Gradio to plot geographical data on a map using the `gradio.Plot` component. The Gradio `Plot` component works with Matplotlib, Bokeh and Plotly. Plotly is what we will be working with in this guide. Plotly allows developers to easily create all sorts of maps with their geographical data. Take a look [here](https://plotly.com/python/maps/) for some examples.\n\n## Overview\n\nWe will be using the New York City Airbnb dataset, which is hosted on kaggle [here](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data). I've uploaded it to the Hugging Face Hub as a dataset [here](https://huggingface.co/datasets/gradio/NYC-Airbnb-Open-Data) for easier use and download. Using this data we will plot Airbnb locations on a map output and allow filtering based on price and location. Below is the demo that we will be building. ‚ö°Ô∏è\n\n\u003Cgradio-app space='gradio/map_airbnb'>\u003C/gradio-app>\n\n## Step 1 - Loading CSV data üíæ\n\nLet's start by loading the Airbnb NYC data from the Hugging Face Hub.\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"gradio/NYC-Airbnb-Open-Data\", split=\"train\")\ndf = dataset.to_pandas()\n\ndef filter_map(min_price, max_price, boroughs):\n    new_df = df[(df['neighbourhood_group'].isin(boroughs)) &\n            (df['price'] > min_price) & (df['price'] \u003C max_price)]\n    names = new_df[\"name\"].tolist()\n    prices = new_df[\"price\"].tolist()\n    text_list = [(names[i], prices[i]) for i in range(0, len(names))]\n```\n\nIn the code above, we first load the csv data into a pandas dataframe. Let's begin by defining a function that we will use as the prediction function for the gradio app. This function will accept the minimum price and maximum price range as well as the list of boroughs to filter the resulting map. We can use the passed in values (`min_price`, `max_price`, and list of `boroughs`) to filter the dataframe and create `new_df`. Next we will create `text_list` of the names and prices of each Airbnb to use as labels on the map.\n\n## Step 2 - Map Figure üåê\n\nPlotly makes it easy to work with maps. Let's take a look below how we can create a map figure.\n\n```python\nimport plotly.graph_objects as go\n\nfig = go.Figure(go.Scattermapbox(\n            customdata=text_list,\n            lat=new_df['latitude'].tolist(),\n            lon=new_df['longitude'].tolist(),\n            mode='markers',\n            marker=go.scattermapbox.Marker(\n                size=6\n            ),\n            hoverinfo=\"text\",\n            hovertemplate='\u003Cb>Name\u003C/b>: %{customdata[0]}\u003Cbr>\u003Cb>Price\u003C/b>: $%{customdata[1]}'\n        ))\n\nfig.update_layout(\n    mapbox_style=\"open-street-map\",\n    hovermode='closest',\n    mapbox=dict(\n        bearing=0,\n        center=go.layout.mapbox.Center(\n            lat=40.67,\n            lon=-73.90\n        ),\n        pitch=0,\n        zoom=9\n    ),\n)\n```\n\nAbove, we create a scatter plot on mapbox by passing it our list of latitudes and longitudes to plot markers. We also pass in our custom data of names and prices for additional info to appear on every marker we hover over. Next we use `update_layout` to specify other map settings such as zoom, and centering.\n\nMore info [here](https://plotly.com/python/scattermapbox/) on scatter plots using Mapbox and Plotly.\n\n## Step 3 - Gradio App ‚ö°Ô∏è\n\nWe will use two `gr.Number` components and a `gr.CheckboxGroup` to allow users of our app to specify price ranges and borough locations. We will then use the `gr.Plot` component as an output for our Plotly + Mapbox map we created earlier.\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Column():\n        with gr.Row():\n            min_price = gr.Number(value=250, label=\"Minimum Price\")\n            max_price = gr.Number(value=1000, label=\"Maximum Price\")\n        boroughs = gr.CheckboxGroup(choices=[\"Queens\", \"Brooklyn\", \"Manhattan\", \"Bronx\", \"Staten Island\"], value=[\"Queens\", \"Brooklyn\"], label=\"Select Boroughs:\")\n        btn = gr.Button(value=\"Update Filter\")\n        map = gr.Plot()\n    demo.load(filter_map, [min_price, max_price, boroughs], map)\n    btn.click(filter_map, [min_price, max_price, boroughs], map)\n```\n\nWe layout these components using the `gr.Column` and `gr.Row` and we'll also add event triggers for when the demo first loads and when our \"Update Filter\" button is clicked in order to trigger the map to update with our new filters.\n\nThis is what the full demo code looks like:\n\n```python\nimport gradio as gr\nimport plotly.graph_objects as go\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"gradio/NYC-Airbnb-Open-Data\", split=\"train\")\ndf = dataset.to_pandas()\n\ndef filter_map(min_price, max_price, boroughs):\n\n    filtered_df = df[(df['neighbourhood_group'].isin(boroughs)) & \n          (df['price'] > min_price) & (df['price'] \u003C max_price)]\n    names = filtered_df[\"name\"].tolist()\n    prices = filtered_df[\"price\"].tolist()\n    text_list = [(names[i], prices[i]) for i in range(0, len(names))]\n    fig = go.Figure(go.Scattermapbox(\n            customdata=text_list,\n            lat=filtered_df['latitude'].tolist(),\n            lon=filtered_df['longitude'].tolist(),\n            mode='markers',\n            marker=go.scattermapbox.Marker(\n                size=6\n            ),\n            hoverinfo=\"text\",\n            hovertemplate='\u003Cb>Name\u003C/b>: %{customdata[0]}\u003Cbr>\u003Cb>Price\u003C/b>: $%{customdata[1]}'\n        ))\n\n    fig.update_layout(\n        mapbox_style=\"open-street-map\",\n        hovermode='closest',\n        mapbox=dict(\n            bearing=0,\n            center=go.layout.mapbox.Center(\n                lat=40.67,\n                lon=-73.90\n            ),\n            pitch=0,\n            zoom=9\n        ),\n    )\n\n    return fig\n\nwith gr.Blocks() as demo:\n    with gr.Column():\n        with gr.Row():\n            min_price = gr.Number(value=250, label=\"Minimum Price\")\n            max_price = gr.Number(value=1000, label=\"Maximum Price\")\n        boroughs = gr.CheckboxGroup(choices=[\"Queens\", \"Brooklyn\", \"Manhattan\", \"Bronx\", \"Staten Island\"], value=[\"Queens\", \"Brooklyn\"], label=\"Select Boroughs:\")\n        btn = gr.Button(value=\"Update Filter\")\n        map = gr.Plot()\n    demo.load(filter_map, [min_price, max_price, boroughs], map)\n    btn.click(filter_map, [min_price, max_price, boroughs], map)\n\ndemo.launch()\n```\n\n## Step 4 - Deployment ü§ó\n\nIf you run the code above, your app will start running locally.\nYou can even get a temporary shareable link by passing the `share=True` parameter to `launch`.\n\nBut what if you want to a permanent deployment solution?\nLet's deploy our Gradio app to the free HuggingFace Spaces platform.\n\nIf you haven't used Spaces before, follow the previous guide [here](/using_hugging_face_integrations).\n\n## Conclusion üéâ\n\nAnd you're all done! That's all the code you need to build a map demo.\n\nHere's a link to the demo [Map demo](https://huggingface.co/spaces/gradio/map_airbnb) and [complete code](https://huggingface.co/spaces/gradio/map_airbnb/blob/main/run.py) (on Hugging Face Spaces)\n",tags:["PLOTS","MAPS"],spaces:[],url:"/guides/plot-component-for-maps/",contributor:null},{name:"styling-the-gradio-dataframe",category:"tabular-data-science-and-plots",pretty_category:"Tabular Data Science And Plots",guide_index:null,absolute_index:38,pretty_name:"Styling The Gradio Dataframe",content:"# How to Style the Gradio Dataframe\n\n\n\n## Introduction\n\nData visualization is a crucial aspect of data analysis and machine learning. The Gradio `DataFrame` component is a popular way to display tabular data (particularly data in the form of a `pandas` `DataFrame` object) within a web application. \n\nThis post will explore the recent enhancements in Gradio that allow users to integrate the styling options of pandas, e.g. adding colors to the DataFrame component, or setting the display precision of numbers. \n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/df-highlight.png)\n\nLet's dive in!\n\n**Prerequisites**: We'll be using the `gradio.Blocks` class in our examples.\nYou can [read the Guide to Blocks first](https://gradio.app/blocks-and-event-listeners) if you are not already familiar with it. Also please make sure you are using the **latest version** version of Gradio: `pip install --upgrade gradio`.\n\n\n## Overview\n\nThe Gradio `DataFrame` component now supports values of the type `Styler` from the `pandas` class. This allows us to reuse the rich existing API and documentation of the `Styler` class instead of inventing a new style format on our own. Here's a complete example of how it looks:\n\n```python\nimport pandas as pd \nimport gradio as gr\n\n# Creating a sample dataframe\ndf = pd.DataFrame({\n    \"A\" : [14, 4, 5, 4, 1], \n    \"B\" : [5, 2, 54, 3, 2], \n    \"C\" : [20, 20, 7, 3, 8], \n    \"D\" : [14, 3, 6, 2, 6], \n    \"E\" : [23, 45, 64, 32, 23]\n}) \n\n# Applying style to highlight the maximum value in each row\nstyler = df.style.highlight_max(color = 'lightgreen', axis = 0)\n\n# Displaying the styled dataframe in Gradio\nwith gr.Blocks() as demo:\n    gr.DataFrame(styler)\n    \ndemo.launch()\n```\n\nThe Styler class can be used to apply conditional formatting and styling to dataframes, making them more visually appealing and interpretable. You can highlight certain values, apply gradients, or even use custom CSS to style the DataFrame. The Styler object is applied to a DataFrame and it returns a new object with the relevant styling properties, which can then be previewed directly, or rendered dynamically in a Gradio interface.\n\nTo read more about the Styler object, read the official `pandas` documentation at: https://pandas.pydata.org/docs/user_guide/style.html\n\nBelow, we'll explore a few examples:\n\n## Highlighting Cells\n\nOk, so let's revisit the previous example. We start by creating a `pd.DataFrame` object and then highlight the highest value in each row with a light green color:\n\n```python\nimport pandas as pd \n\n# Creating a sample dataframe\ndf = pd.DataFrame({\n    \"A\" : [14, 4, 5, 4, 1], \n    \"B\" : [5, 2, 54, 3, 2], \n    \"C\" : [20, 20, 7, 3, 8], \n    \"D\" : [14, 3, 6, 2, 6], \n    \"E\" : [23, 45, 64, 32, 23]\n}) \n\n# Applying style to highlight the maximum value in each row\nstyler = df.style.highlight_max(color = 'lightgreen', axis = 0)\n```\n\nNow, we simply pass this object into the Gradio `DataFrame` and we can visualize our colorful table of data in 4 lines of python:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Dataframe(styler)\n    \ndemo.launch()\n```\n\nHere's how it looks:\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/df-highlight.png)\n\n## Font Colors\n\nApart from highlighting cells, you might want to color specific text within the cells. Here's how you can change text colors for certain columns:\n\n```python\nimport pandas as pd \nimport gradio as gr\n\n# Creating a sample dataframe\ndf = pd.DataFrame({\n    \"A\" : [14, 4, 5, 4, 1], \n    \"B\" : [5, 2, 54, 3, 2], \n    \"C\" : [20, 20, 7, 3, 8], \n    \"D\" : [14, 3, 6, 2, 6], \n    \"E\" : [23, 45, 64, 32, 23]\n}) \n\n# Function to apply text color\ndef highlight_cols(x): \n    df = x.copy() \n    df.loc[:, :] = 'color: purple'\n    df[['B', 'C', 'E']] = 'color: green'\n    return df \n\n# Applying the style function\ns = df.style.apply(highlight_cols, axis = None)\n\n# Displaying the styled dataframe in Gradio\nwith gr.Blocks() as demo:\n    gr.DataFrame(s)\n    \ndemo.launch()\n```\n\nIn this script, we define a custom function highlight_cols that changes the text color to purple for all cells, but overrides this for columns B, C, and E with green. Here's how it looks:\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/df-color.png)\n\n## Display Precision \n\nSometimes, the data you are dealing with might have long floating numbers, and you may want to display only a fixed number of decimals for simplicity. The pandas Styler object allows you to format the precision of numbers displayed. Here's how you can do this:\n\n```python\nimport pandas as pd\nimport gradio as gr\n\n# Creating a sample dataframe with floating numbers\ndf = pd.DataFrame({\n    \"A\" : [14.12345, 4.23456, 5.34567, 4.45678, 1.56789], \n    \"B\" : [5.67891, 2.78912, 54.89123, 3.91234, 2.12345], \n    # ... other columns\n}) \n\n# Setting the precision of numbers to 2 decimal places\ns = df.style.format(\"{:.2f}\")\n\n# Displaying the styled dataframe in Gradio\nwith gr.Blocks() as demo:\n    gr.DataFrame(s)\n    \ndemo.launch()\n```\n\nIn this script, the format method of the Styler object is used to set the precision of numbers to two decimal places. Much cleaner now:\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/df-precision.png)\n\n\n## Note about Interactivity\n\nOne thing to keep in mind is that the gradio `DataFrame` component only accepts `Styler` objects when it is non-interactive (i.e. in \"static\" mode). If the `DataFrame` component is interactive, then the styling information is ignored and instead the raw table values are shown instead. \n\nThe `DataFrame` component is by default non-interactive, unless it is used as an input to an event. In which case, you can force the component to be non-interactive by setting the `interactive` prop like this:\n\n```python\nc = gr.DataFrame(styler, interactive=False)\n```\n\n## Conclusion üéâ\n\nThis is just a taste of what's possible using the `gradio.DataFrame` component with the `Styler` class from `pandas`. Try it out and let us know what you think!",tags:["DATAFRAME","STYLE","COLOR"],spaces:[],url:"/guides/styling-the-gradio-dataframe/",contributor:null},{name:"using-gradio-for-tabular-workflows",category:"tabular-data-science-and-plots",pretty_category:"Tabular Data Science And Plots",guide_index:null,absolute_index:39,pretty_name:"Using Gradio For Tabular Workflows",content:"# Using Gradio for Tabular Data Science Workflows\n\n\n\n## Introduction\n\nTabular data science is the most widely used domain of machine learning, with problems ranging from customer segmentation to churn prediction. Throughout various stages of the tabular data science workflow, communicating your work to stakeholders or clients can be cumbersome; which prevents data scientists from focusing on what matters, such as data analysis and model building. Data scientists can end up spending hours building a dashboard that takes in dataframe and returning plots, or returning a prediction or plot of clusters in a dataset. In this guide, we'll go through how to use `gradio` to improve your data science workflows. We will also talk about how to use `gradio` and [skops](https://skops.readthedocs.io/en/stable/) to build interfaces with only one line of code!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started).\n\n## Let's Create a Simple Interface!\n\nWe will take a look at how we can create a simple UI that predicts failures based on product information.\n\n```python\nimport gradio as gr\nimport pandas as pd\nimport joblib\nimport datasets\n\n\ninputs = [gr.Dataframe(row_count = (2, \"dynamic\"), col_count=(4,\"dynamic\"), label=\"Input Data\", interactive=1)]\n\noutputs = [gr.Dataframe(row_count = (2, \"dynamic\"), col_count=(1, \"fixed\"), label=\"Predictions\", headers=[\"Failures\"])]\n\nmodel = joblib.load(\"model.pkl\")\n\n# we will give our dataframe as example\ndf = datasets.load_dataset(\"merve/supersoaker-failures\")\ndf = df[\"train\"].to_pandas()\n\ndef infer(input_dataframe):\n  return pd.DataFrame(model.predict(input_dataframe))\n\ngr.Interface(fn = infer, inputs = inputs, outputs = outputs, examples = [[df.head(2)]]).launch()\n```\n\nLet's break down above code.\n\n- `fn`: the inference function that takes input dataframe and returns predictions.\n- `inputs`: the component we take our input with. We define our input as dataframe with 2 rows and 4 columns, which initially will look like an empty dataframe with the aforementioned shape. When the `row_count` is set to `dynamic`, you don't have to rely on the dataset you're inputting to pre-defined component.\n- `outputs`: The dataframe component that stores outputs. This UI can take single or multiple samples to infer, and returns 0 or 1 for each sample in one column, so we give `row_count` as 2 and `col_count` as 1 above. `headers` is a list made of header names for dataframe.\n- `examples`: You can either pass the input by dragging and dropping a CSV file, or a pandas DataFrame through examples, which headers will be automatically taken by the interface.\n\nWe will now create an example for a minimal data visualization dashboard. You can find a more comprehensive version in the related Spaces.\n\n\u003Cgradio-app space=\"gradio/tabular-playground\">\u003C/gradio-app>\n\n```python\nimport gradio as gr\nimport pandas as pd\nimport datasets\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = datasets.load_dataset(\"merve/supersoaker-failures\")\ndf = df[\"train\"].to_pandas()\ndf.dropna(axis=0, inplace=True)\n\ndef plot(df):\n  plt.scatter(df.measurement_13, df.measurement_15, c = df.loading,alpha=0.5)\n  plt.savefig(\"scatter.png\")\n  df['failure'].value_counts().plot(kind='bar')\n  plt.savefig(\"bar.png\")\n  sns.heatmap(df.select_dtypes(include=\"number\").corr())\n  plt.savefig(\"corr.png\")\n  plots = [\"corr.png\",\"scatter.png\", \"bar.png\"]\n  return plots\n\ninputs = [gr.Dataframe(label=\"Supersoaker Production Data\")]\noutputs = [gr.Gallery(label=\"Profiling Dashboard\", columns=(1,3))]\n\ngr.Interface(plot, inputs=inputs, outputs=outputs, examples=[df.head(100)], title=\"Supersoaker Failures Analysis Dashboard\").launch()\n```\n\n\u003Cgradio-app space=\"gradio/gradio-analysis-dashboard-minimal\">\u003C/gradio-app>\n\nWe will use the same dataset we used to train our model, but we will make a dashboard to visualize it this time.\n\n- `fn`: The function that will create plots based on data.\n- `inputs`: We use the same `Dataframe` component we used above.\n- `outputs`: The `Gallery` component is used to keep our visualizations.\n- `examples`: We will have the dataset itself as the example.\n\n## Easily load tabular data interfaces with one line of code using skops\n\n`skops` is a library built on top of `huggingface_hub` and `sklearn`. With the recent `gradio` integration of `skops`, you can build tabular data interfaces with one line of code!\n\n```python\nimport gradio as gr\n\n# title and description are optional\ntitle = \"Supersoaker Defective Product Prediction\"\ndescription = \"This model predicts Supersoaker production line failures. Drag and drop any slice from dataset or edit values as you wish in below dataframe component.\"\n\ngr.load(\"huggingface/scikit-learn/tabular-playground\", title=title, description=description).launch()\n```\n\n\u003Cgradio-app space=\"gradio/gradio-skops-integration\">\u003C/gradio-app>\n\n`sklearn` models pushed to Hugging Face Hub using `skops` include a `config.json` file that contains an example input with column names, the task being solved (that can either be `tabular-classification` or `tabular-regression`). From the task type, `gradio` constructs the `Interface` and consumes column names and the example input to build it. You can [refer to skops documentation on hosting models on Hub](https://skops.readthedocs.io/en/latest/auto_examples/plot_hf_hub.html#sphx-glr-auto-examples-plot-hf-hub-py) to learn how to push your models to Hub using `skops`.\n",tags:[],spaces:["https://huggingface.co/spaces/scikit-learn/gradio-skops-integration","https://huggingface.co/spaces/scikit-learn/tabular-playground","https://huggingface.co/spaces/merve/gradio-analysis-dashboard"],url:"/guides/using-gradio-for-tabular-workflows/",contributor:null}]},{category:"Gradio Clients And Lite",guides:[{name:"getting-started-with-the-python-client",category:"gradio-clients-and-lite",pretty_category:"Gradio Clients And Lite",guide_index:1,absolute_index:40,pretty_name:"Getting Started With The Python Client",content:"# Getting Started with the Gradio Python client\n\n\n\nThe Gradio Python client makes it very easy to use any Gradio app as an API. As an example, consider this [Hugging Face Space that transcribes audio files](https://huggingface.co/spaces/abidlabs/whisper) that are recorded from the microphone.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/whisper-screenshot.jpg)\n\nUsing the `gradio_client` library, we can easily use the Gradio as an API to transcribe audio files programmatically.\n\nHere's the entire code to do it:\n\n```python\nfrom gradio_client import Client, file\n\nclient = Client(\"abidlabs/whisper\")\n\nclient.predict(\n    audio=file(\"audio_sample.wav\")\n)\n\n>> \"This is a test of the whisper speech recognition model.\"\n```\n\nThe Gradio client works with any hosted Gradio app! Although the Client is mostly used with apps hosted on [Hugging Face Spaces](https://hf.space), your app can be hosted anywhere, such as your own server.\n\n**Prerequisites**: To use the Gradio client, you do _not_ need to know the `gradio` library in great detail. However, it is helpful to have general familiarity with Gradio's concepts of input and output components.\n\n## Installation\n\nIf you already have a recent version of `gradio`, then the `gradio_client` is included as a dependency. But note that this documentation reflects the latest version of the `gradio_client`, so upgrade if you're not sure!\n\nThe lightweight `gradio_client` package can be installed from pip (or pip3) and is tested to work with **Python versions 3.9 or higher**:\n\n```bash\n$ pip install --upgrade gradio_client\n```\n\n## Connecting to a Gradio App on Hugging Face Spaces\n\nStart by connecting instantiating a `Client` object and connecting it to a Gradio app that is running on Hugging Face Spaces.\n\n```python\nfrom gradio_client import Client\n\nclient = Client(\"abidlabs/en2fr\")  # a Space that translates from English to French\n```\n\nYou can also connect to private Spaces by passing in your HF token with the `hf_token` parameter. You can get your HF token here: https://huggingface.co/settings/tokens\n\n```python\nfrom gradio_client import Client\n\nclient = Client(\"abidlabs/my-private-space\", hf_token=\"...\")\n```\n\n\n## Duplicating a Space for private use\n\nWhile you can use any public Space as an API, you may get rate limited by Hugging Face if you make too many requests. For unlimited usage of a Space, simply duplicate the Space to create a private Space,\nand then use it to make as many requests as you'd like!\n\nThe `gradio_client` includes a class method: `Client.duplicate()` to make this process simple (you'll need to pass in your [Hugging Face token](https://huggingface.co/settings/tokens) or be logged in using the Hugging Face CLI):\n\n```python\nimport os\nfrom gradio_client import Client, file\n\nHF_TOKEN = os.environ.get(\"HF_TOKEN\")\n\nclient = Client.duplicate(\"abidlabs/whisper\", hf_token=HF_TOKEN)\nclient.predict(file(\"audio_sample.wav\"))\n\n>> \"This is a test of the whisper speech recognition model.\"\n```\n\nIf you have previously duplicated a Space, re-running `duplicate()` will _not_ create a new Space. Instead, the Client will attach to the previously-created Space. So it is safe to re-run the `Client.duplicate()` method multiple times.\n\n**Note:** if the original Space uses GPUs, your private Space will as well, and your Hugging Face account will get billed based on the price of the GPU. To minimize charges, your Space will automatically go to sleep after 1 hour of inactivity. You can also set the hardware using the `hardware` parameter of `duplicate()`.\n\n## Connecting a general Gradio app\n\nIf your app is running somewhere else, just provide the full URL instead, including the \"http://\" or \"https://\". Here's an example of making predictions to a Gradio app that is running on a share URL:\n\n```python\nfrom gradio_client import Client\n\nclient = Client(\"https://bec81a83-5b5c-471e.gradio.live\")\n```\n\n## Inspecting the API endpoints\n\nOnce you have connected to a Gradio app, you can view the APIs that are available to you by calling the `Client.view_api()` method. For the Whisper Space, we see the following:\n\n```bash\nClient.predict() Usage Info\n---------------------------\nNamed API endpoints: 1\n\n - predict(audio, api_name=\"/predict\") -> output\n    Parameters:\n     - [Audio] audio: filepath (required)  \n    Returns:\n     - [Textbox] output: str \n```\n\nWe see  that we have 1 API endpoint in this space, and shows us how to use the API endpoint to make a prediction: we should call the `.predict()` method (which we will explore below), providing a parameter `input_audio` of type `str`, which is a `filepath or URL`.\n\nWe should also provide the `api_name='/predict'` argument to the `predict()` method. Although this isn't necessary if a Gradio app has only 1 named endpoint, it does allow us to call different endpoints in a single app if they are available.\n\n## The \"View API\" Page\n\nAs an alternative to running the `.view_api()` method, you can click on the \"Use via API\" link in the footer of the Gradio app, which shows us the same information, along with example usage. \n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/view-api.png)\n\nThe View API page also includes an \"API Recorder\" that lets you interact with the Gradio UI normally and converts your interactions into the corresponding code to run with the Python Client.\n\n## Making a prediction\n\nThe simplest way to make a prediction is simply to call the `.predict()` function with the appropriate arguments:\n\n```python\nfrom gradio_client import Client\n\nclient = Client(\"abidlabs/en2fr\", api_name='/predict')\nclient.predict(\"Hello\")\n\n>> Bonjour\n```\n\nIf there are multiple parameters, then you should pass them as separate arguments to `.predict()`, like this:\n\n```python\nfrom gradio_client import Client\n\nclient = Client(\"gradio/calculator\")\nclient.predict(4, \"add\", 5)\n\n>> 9.0\n```\n\nIt is recommended to provide key-word arguments instead of positional arguments:\n\n\n```python\nfrom gradio_client import Client\n\nclient = Client(\"gradio/calculator\")\nclient.predict(num1=4, operation=\"add\", num2=5)\n\n>> 9.0\n```\n\nThis allows you to take advantage of default arguments. For example, this Space includes the default value for the Slider component so you do not need to provide it when accessing it with the client.\n\n```python\nfrom gradio_client import Client\n\nclient = Client(\"abidlabs/image_generator\")\nclient.predict(text=\"an astronaut riding a camel\")\n```\n\nThe default value is the initial value of the corresponding Gradio component. If the component does not have an initial value, but if the corresponding argument in the predict function has a default value of `None`, then that parameter is also optional in the client. Of course, if you'd like to override it, you can include it as well:\n\n```python\nfrom gradio_client import Client\n\nclient = Client(\"abidlabs/image_generator\")\nclient.predict(text=\"an astronaut riding a camel\", steps=25)\n```\n\nFor providing files or URLs as inputs, you should pass in the filepath or URL to the file enclosed within `gradio_client.file()`. This takes care of uploading the file to the Gradio server and ensures that the file is preprocessed correctly:\n\n```python\nfrom gradio_client import Client, file\n\nclient = Client(\"abidlabs/whisper\")\nclient.predict(\n    audio=file(\"https://audio-samples.github.io/samples/mp3/blizzard_unconditional/sample-0.mp3\")\n)\n\n>> \"My thought I have nobody by a beauty and will as you poured. Mr. Rochester is serve in that so don't find simpus, and devoted abode, to at might in a r‚Äî\"\n```\n\n## Running jobs asynchronously\n\nOe should note that `.predict()` is a _blocking_ operation as it waits for the operation to complete before returning the prediction.\n\nIn many cases, you may be better off letting the job run in the background until you need the results of the prediction. You can do this by creating a `Job` instance using the `.submit()` method, and then later calling `.result()` on the job to get the result. For example:\n\n```python\nfrom gradio_client import Client\n\nclient = Client(space=\"abidlabs/en2fr\")\njob = client.submit(\"Hello\", api_name=\"/predict\")  # This is not blocking\n\n# Do something else\n\njob.result()  # This is blocking\n\n>> Bonjour\n```\n\n## Adding callbacks\n\nAlternatively, one can add one or more callbacks to perform actions after the job has completed running, like this:\n\n```python\nfrom gradio_client import Client\n\ndef print_result(x):\n    print(\"The translated result is: {x}\")\n\nclient = Client(space=\"abidlabs/en2fr\")\n\njob = client.submit(\"Hello\", api_name=\"/predict\", result_callbacks=[print_result])\n\n# Do something else\n\n>> The translated result is: Bonjour\n\n```\n\n## Status\n\nThe `Job` object also allows you to get the status of the running job by calling the `.status()` method. This returns a `StatusUpdate` object with the following attributes: `code` (the status code, one of a set of defined strings representing the status. See the `utils.Status` class), `rank` (the current position of this job in the queue), `queue_size` (the total queue size), `eta` (estimated time this job will complete), `success` (a boolean representing whether the job completed successfully), and `time` (the time that the status was generated).\n\n```py\nfrom gradio_client import Client\n\nclient = Client(src=\"gradio/calculator\")\njob = client.submit(5, \"add\", 4, api_name=\"/predict\")\njob.status()\n\n>> \u003CStatus.STARTING: 'STARTING'>\n```\n\n_Note_: The `Job` class also has a `.done()` instance method which returns a boolean indicating whether the job has completed.\n\n## Cancelling Jobs\n\nThe `Job` class also has a `.cancel()` instance method that cancels jobs that have been queued but not started. For example, if you run:\n\n```py\nclient = Client(\"abidlabs/whisper\")\njob1 = client.submit(file(\"audio_sample1.wav\"))\njob2 = client.submit(file(\"audio_sample2.wav\"))\njob1.cancel()  # will return False, assuming the job has started\njob2.cancel()  # will return True, indicating that the job has been canceled\n```\n\nIf the first job has started processing, then it will not be canceled. If the second job\nhas not yet started, it will be successfully canceled and removed from the queue.\n\n## Generator Endpoints\n\nSome Gradio API endpoints do not return a single value, rather they return a series of values. You can get the series of values that have been returned at any time from such a generator endpoint by running `job.outputs()`:\n\n```py\nfrom gradio_client import Client\n\nclient = Client(src=\"gradio/count_generator\")\njob = client.submit(3, api_name=\"/count\")\nwhile not job.done():\n    time.sleep(0.1)\njob.outputs()\n\n>> ['0', '1', '2']\n```\n\nNote that running `job.result()` on a generator endpoint only gives you the _first_ value returned by the endpoint.\n\nThe `Job` object is also iterable, which means you can use it to display the results of a generator function as they are returned from the endpoint. Here's the equivalent example using the `Job` as a generator:\n\n```py\nfrom gradio_client import Client\n\nclient = Client(src=\"gradio/count_generator\")\njob = client.submit(3, api_name=\"/count\")\n\nfor o in job:\n    print(o)\n\n>> 0\n>> 1\n>> 2\n```\n\nYou can also cancel jobs that that have iterative outputs, in which case the job will finish as soon as the current iteration finishes running.\n\n```py\nfrom gradio_client import Client\nimport time\n\nclient = Client(\"abidlabs/test-yield\")\njob = client.submit(\"abcdef\")\ntime.sleep(3)\njob.cancel()  # job cancels after 2 iterations\n```\n\n## Demos with Session State\n\nGradio demos can include [session state](https://www.gradio.app/guides/state-in-blocks), which provides a way for demos to persist information from user interactions within a page session.\n\nFor example, consider the following demo, which maintains a list of words that a user has submitted in a `gr.State` component. When a user submits a new word, it is added to the state, and the number of previous occurrences of that word is displayed:\n\n```python\nimport gradio as gr\n\ndef count(word, list_of_words):\n    return list_of_words.count(word), list_of_words + [word]\n\nwith gr.Blocks() as demo:\n    words = gr.State([])\n    textbox = gr.Textbox()\n    number = gr.Number()\n    textbox.submit(count, inputs=[textbox, words], outputs=[number, words])\n    \ndemo.launch()\n```\n\nIf you were to connect this this Gradio app using the Python Client, you would notice that the API information only shows a single input and output:\n\n```csv\nClient.predict() Usage Info\n---------------------------\nNamed API endpoints: 1\n\n - predict(word, api_name=\"/count\") -> value_31\n    Parameters:\n     - [Textbox] word: str (required)  \n    Returns:\n     - [Number] value_31: float \n```\n\nThat is because the Python client handles state automatically for you -- as you make a series of requests, the returned state from one request is stored internally and automatically supplied for the subsequent request. If you'd like to reset the state, you can do that by calling `Client.reset_session()`.",tags:["CLIENT","API","SPACES"],spaces:[],url:"/guides/getting-started-with-the-python-client/",contributor:null},{name:"getting-started-with-the-js-client",category:"gradio-clients-and-lite",pretty_category:"Gradio Clients And Lite",guide_index:2,absolute_index:41,pretty_name:"Getting Started With The Js Client",content:"# Getting Started with the Gradio JavaScript Client\n\n\n\nThe Gradio JavaScript Client makes it very easy to use any Gradio app as an API. As an example, consider this [Hugging Face Space that transcribes audio files](https://huggingface.co/spaces/abidlabs/whisper) that are recorded from the microphone.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/whisper-screenshot.jpg)\n\nUsing the `@gradio/client` library, we can easily use the Gradio as an API to transcribe audio files programmatically.\n\nHere's the entire code to do it:\n\n```js\nimport { Client } from \"@gradio/client\";\n\nconst response = await fetch(\n\t\"https://github.com/audio-samples/audio-samples.github.io/raw/master/samples/wav/ted_speakers/SalmanKhan/sample-1.wav\"\n);\nconst audio_file = await response.blob();\n\nconst app = await Client.connect(\"abidlabs/whisper\");\nconst transcription = await app.predict(\"/predict\", [audio_file]);\n\nconsole.log(transcription.data);\n// [ \"I said the same phrase 30 times.\" ]\n```\n\nThe Gradio Client works with any hosted Gradio app, whether it be an image generator, a text summarizer, a stateful chatbot, a tax calculator, or anything else! The Gradio Client is mostly used with apps hosted on [Hugging Face Spaces](https://hf.space), but your app can be hosted anywhere, such as your own server.\n\n**Prequisites**: To use the Gradio client, you do _not_ need to know the `gradio` library in great detail. However, it is helpful to have general familiarity with Gradio's concepts of input and output components.\n\n## Installation via npm\n\nInstall the @gradio/client package to interact with Gradio APIs using Node.js version >=18.0.0 or in browser-based projects. Use npm or any compatible package manager:\n\n```bash\nnpm i @gradio/client\n```\n\nThis command adds @gradio/client to your project dependencies, allowing you to import it in your JavaScript or TypeScript files.\n\n## Installation via CDN\n\nFor quick addition to your web project, you can use the jsDelivr CDN to load the latest version of @gradio/client directly into your HTML:\n\n```bash\n\u003Cscript src=\"https://cdn.jsdelivr.net/npm/@gradio/client/dist/index.min.js\">\u003C/script>\n```\n\nBe sure to add this to the `\u003Chead>` of your HTML. This will install the latest version but we advise hardcoding the version in production. You can find all available versions [here](https://www.jsdelivr.com/package/npm/@gradio/client). This approach is ideal for experimental or prototying purposes, though has some limitations.\n\n## Connecting to a running Gradio App\n\nStart by connecting instantiating a `client` instance and connecting it to a Gradio app that is running on Hugging Face Spaces or generally anywhere on the web.\n\n## Connecting to a Hugging Face Space\n\n```js\nimport { Client } from \"@gradio/client\";\n\nconst app = Client.connect(\"abidlabs/en2fr\"); // a Space that translates from English to French\n```\n\nYou can also connect to private Spaces by passing in your HF token with the `hf_token` property of the options parameter. You can get your HF token here: https://huggingface.co/settings/tokens\n\n```js\nimport { Client } from \"@gradio/client\";\n\nconst app = Client.connect(\"abidlabs/my-private-space\", { hf_token=\"hf_...\" })\n```\n\n## Duplicating a Space for private use\n\nWhile you can use any public Space as an API, you may get rate limited by Hugging Face if you make too many requests. For unlimited usage of a Space, simply duplicate the Space to create a private Space, and then use it to make as many requests as you'd like! You'll need to pass in your [Hugging Face token](https://huggingface.co/settings/tokens)).\n\n`Client.duplicate` is almost identical to `Client.connect`, the only difference is under the hood:\n\n```js\nimport { Client } from \"@gradio/client\";\n\nconst response = await fetch(\n\t\"https://audio-samples.github.io/samples/mp3/blizzard_unconditional/sample-0.mp3\"\n);\nconst audio_file = await response.blob();\n\nconst app = await Client.duplicate(\"abidlabs/whisper\", { hf_token: \"hf_...\" });\nconst transcription = await app.predict(\"/predict\", [audio_file]);\n```\n\nIf you have previously duplicated a Space, re-running `Client.duplicate` will _not_ create a new Space. Instead, the client will attach to the previously-created Space. So it is safe to re-run the `Client.duplicate` method multiple times with the same space.\n\n**Note:** if the original Space uses GPUs, your private Space will as well, and your Hugging Face account will get billed based on the price of the GPU. To minimize charges, your Space will automatically go to sleep after 5 minutes of inactivity. You can also set the hardware using the `hardware` and `timeout` properties of `duplicate`'s options object like this:\n\n```js\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.duplicate(\"abidlabs/whisper\", {\n\thf_token: \"hf_...\",\n\ttimeout: 60,\n\thardware: \"a10g-small\"\n});\n```\n\n## Connecting a general Gradio app\n\nIf your app is running somewhere else, just provide the full URL instead, including the \"http://\" or \"https://\". Here's an example of making predictions to a Gradio app that is running on a share URL:\n\n```js\nimport { Client } from \"@gradio/client\";\n\nconst app = Client.connect(\"https://bec81a83-5b5c-471e.gradio.live\");\n```\n\n## Inspecting the API endpoints\n\nOnce you have connected to a Gradio app, you can view the APIs that are available to you by calling the `Client`'s `view_api` method.\n\nFor the Whisper Space, we can do this:\n\n```js\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"abidlabs/whisper\");\n\nconst app_info = await app.view_api();\n\nconsole.log(app_info);\n```\n\nAnd we will see the following:\n\n```json\n{\n\t\"named_endpoints\": {\n\t\t\"/predict\": {\n\t\t\t\"parameters\": [\n\t\t\t\t{\n\t\t\t\t\t\"label\": \"text\",\n\t\t\t\t\t\"component\": \"Textbox\",\n\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t}\n\t\t\t],\n\t\t\t\"returns\": [\n\t\t\t\t{\n\t\t\t\t\t\"label\": \"output\",\n\t\t\t\t\t\"component\": \"Textbox\",\n\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t}\n\t\t\t]\n\t\t}\n\t},\n\t\"unnamed_endpoints\": {}\n}\n```\n\nThis shows us that we have 1 API endpoint in this space, and shows us how to use the API endpoint to make a prediction: we should call the `.predict()` method (which we will explore below), providing a parameter `input_audio` of type `string`, which is a url to a file.\n\nWe should also provide the `api_name='/predict'` argument to the `predict()` method. Although this isn't necessary if a Gradio app has only 1 named endpoint, it does allow us to call different endpoints in a single app if they are available. If an app has unnamed API endpoints, these can also be displayed by running `.view_api(all_endpoints=True)`.\n\n## The \"View API\" Page\n\nAs an alternative to running the `.view_api()` method, you can click on the \"Use via API\" link in the footer of the Gradio app, which shows us the same information, along with example usage. \n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/view-api.png)\n\nThe View API page also includes an \"API Recorder\" that lets you interact with the Gradio UI normally and converts your interactions into the corresponding code to run with the JS Client.\n\n\n## Making a prediction\n\nThe simplest way to make a prediction is simply to call the `.predict()` method with the appropriate arguments:\n\n```js\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"abidlabs/en2fr\");\nconst result = await app.predict(\"/predict\", [\"Hello\"]);\n```\n\nIf there are multiple parameters, then you should pass them as an array to `.predict()`, like this:\n\n```js\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"gradio/calculator\");\nconst result = await app.predict(\"/predict\", [4, \"add\", 5]);\n```\n\nFor certain inputs, such as images, you should pass in a `Buffer`, `Blob` or `File` depending on what is most convenient. In node, this would be a `Buffer` or `Blob`; in a browser environment, this would be a `Blob` or `File`.\n\n```js\nimport { Client } from \"@gradio/client\";\n\nconst response = await fetch(\n\t\"https://audio-samples.github.io/samples/mp3/blizzard_unconditional/sample-0.mp3\"\n);\nconst audio_file = await response.blob();\n\nconst app = await Client.connect(\"abidlabs/whisper\");\nconst result = await app.predict(\"/predict\", [audio_file]);\n```\n\n## Using events\n\nIf the API you are working with can return results over time, or you wish to access information about the status of a job, you can use the iterable interface for more flexibility. This is especially useful for iterative endpoints or generator endpoints that will produce a series of values over time as discreet responses.\n\n```js\nimport { Client } from \"@gradio/client\";\n\nfunction log_result(payload) {\n\tconst {\n\t\tdata: [translation]\n\t} = payload;\n\n\tconsole.log(`The translated result is: ${translation}`);\n}\n\nconst app = await Client.connect(\"abidlabs/en2fr\");\nconst job = app.submit(\"/predict\", [\"Hello\"]);\n\nfor await (const message of job) {\n\tlog_result(message);\n}\n```\n\n## Status\n\nThe event interface also allows you to get the status of the running job by instantiating the client with the `events` options passing `status` and `data` as an array:\n\n\n```ts\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"abidlabs/en2fr\", {\n\tevents: [\"status\", \"data\"]\n});\n```\n\nThis ensures that status messages are also reported to the client.\n\n`status`es are returned as an object with the following attributes: `status` (a human readbale status of the current job, `\"pending\" | \"generating\" | \"complete\" | \"error\"`), `code` (the detailed gradio code for the job), `position` (the current position of this job in the queue), `queue_size` (the total queue size), `eta` (estimated time this job will complete), `success` (a boolean representing whether the job completed successfully), and `time` ( as `Date` object detailing the time that the status was generated).\n\n```js\nimport { Client } from \"@gradio/client\";\n\nfunction log_status(status) {\n\tconsole.log(\n\t\t`The current status for this job is: ${JSON.stringify(status, null, 2)}.`\n\t);\n}\n\nconst app = await Client.connect(\"abidlabs/en2fr\", {\n\tevents: [\"status\", \"data\"]\n});\nconst job = app.submit(\"/predict\", [\"Hello\"]);\n\nfor await (const message of job) {\n\tif (message.type === \"status\") {\n\t\tlog_status(message);\n\t}\n}\n```\n\n## Cancelling Jobs\n\nThe job instance also has a `.cancel()` method that cancels jobs that have been queued but not started. For example, if you run:\n\n```js\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"abidlabs/en2fr\");\nconst job_one = app.submit(\"/predict\", [\"Hello\"]);\nconst job_two = app.submit(\"/predict\", [\"Friends\"]);\n\njob_one.cancel();\njob_two.cancel();\n```\n\nIf the first job has started processing, then it will not be canceled but the client will no longer listen for updates (throwing away the job). If the second job has not yet started, it will be successfully canceled and removed from the queue.\n\n## Generator Endpoints\n\nSome Gradio API endpoints do not return a single value, rather they return a series of values. You can listen for these values in real time using the iterable interface:\n\n```js\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"gradio/count_generator\");\nconst job = app.submit(0, [9]);\n\nfor await (const message of job) {\n\tconsole.log(message.data);\n}\n```\n\nThis will log out the values as they are generated by the endpoint.\n\nYou can also cancel jobs that that have iterative outputs, in which case the job will finish immediately.\n\n```js\nimport { Client } from \"@gradio/client\";\n\nconst app = await Client.connect(\"gradio/count_generator\");\nconst job = app.submit(0, [9]);\n\nfor await (const message of job) {\n\tconsole.log(message.data);\n}\n\nsetTimeout(() => {\n\tjob.cancel();\n}, 3000);\n```\n",tags:["CLIENT","API","SPACES"],spaces:[],url:"/guides/getting-started-with-the-js-client/",contributor:null},{name:"querying-gradio-apps-with-curl",category:"gradio-clients-and-lite",pretty_category:"Gradio Clients And Lite",guide_index:3,absolute_index:42,pretty_name:"Querying Gradio Apps With Curl",content:"# Querying Gradio Apps with Curl\n\n\n\nIt is possible to use any Gradio app as an API using cURL, the command-line tool that is pre-installed on many operating systems. This is particularly useful if you are trying to query a Gradio app from an environment other than Python or Javascript (since specialized Gradio clients exist for both [Python](/guides/getting-started-with-the-python-client) and [Javascript](/guides/getting-started-with-the-js-client)).\n\nAs an example, consider this Gradio demo that translates text from English to French: https://abidlabs-en2fr.hf.space/.\n\nUsing `curl`, we can translate text programmatically.\n\nHere's the code to do it:\n\n```bash\n$ curl -X POST https://abidlabs-en2fr.hf.space/call/predict -H \"Content-Type: application/json\" -d '{\n  \"data\": [\"Hello, my friend.\"] \n}'\n\n>> {\"event_id\": $EVENT_ID}   \n```\n\n```bash\n$ curl -N https://abidlabs-en2fr.hf.space/call/predict/$EVENT_ID\n\n>> event: complete\n>> data: [\"Bonjour, mon ami.\"]\n```\n\n\nNote: making a prediction and getting a result requires two `curl` requests: a `POST` and a `GET`. The `POST` request returns an `EVENT_ID` and prints  it to the console, which is used in the second `GET` request to fetch the results. You can combine these into a single command using `awk` and `read` to parse the results of the first command and pipe into the second, like this:\n\n```bash\n$ curl -X POST https://abidlabs-en2fr.hf.space/call/predict -H \"Content-Type: application/json\" -d '{\n  \"data\": [\"Hello, my friend.\"] \n}' \\\n  | awk -F'\"' '{ print $4}'  \\\n  | read EVENT_ID; curl -N https://abidlabs-en2fr.hf.space/call/predict/$EVENT_ID\n\n>> event: complete\n>> data: [\"Bonjour, mon ami.\"]\n```\n\nIn the rest of this Guide, we'll explain these two steps in more detail and provide additional examples of querying Gradio apps with `curl`.\n\n\n**Prerequisites**: For this Guide, you do _not_ need to know how to build Gradio apps in great detail. However, it is helpful to have general familiarity with Gradio's concepts of input and output components.\n\n## Installation\n\nYou generally don't need to install cURL, as it comes pre-installed on many operating systems. Run:\n\n```bash\ncurl --version\n```\n\nto confirm that `curl` is installed. If it is not already installed, you can install it by visiting https://curl.se/download.html. \n\n\n## Step 0: Get the URL for your Gradio App \n\nTo query a Gradio app, you'll need its full URL. This is usually just the URL that the Gradio app is hosted on, for example: https://bec81a83-5b5c-471e.gradio.live\n\n\n**Hugging Face Spaces**\n\nHowever, if you are querying a Gradio on Hugging Face Spaces, you will need to use the URL of the embedded Gradio app, not the URL of the Space webpage. For example:\n\n```bash\n‚ùå Space URL: https://huggingface.co/spaces/abidlabs/en2fr\n‚úÖ Gradio app URL: https://abidlabs-en2fr.hf.space/\n```\n\nYou can get the Gradio app URL by clicking the \"view API\" link at the bottom of the page. Or, you can right-click on the page and then click on \"View Frame Source\" or the equivalent in your browser to view the URL of the embedded Gradio app.\n\nWhile you can use any public Space as an API, you may get rate limited by Hugging Face if you make too many requests. For unlimited usage of a Space, simply duplicate the Space to create a private Space,\nand then use it to make as many requests as you'd like!\n\nNote: to query private Spaces, you will need to pass in your Hugging Face (HF) token. You can get your HF token here: https://huggingface.co/settings/tokens. In this case, you will need to include an additional header in both of your `curl` calls that we'll discuss below:\n\n```bash\n-H \"Authorization: Bearer $HF_TOKEN\"\n```\n\nNow, we are ready to make the two `curl` requests.\n\n## Step 1: Make a Prediction (POST)\n\nThe first of the two `curl` requests is `POST` request that submits the input payload to the Gradio app. \n\nThe syntax of the `POST` request is as follows:\n\n```bash\n$ curl -X POST $URL/call/$API_NAME -H \"Content-Type: application/json\" -d '{\n  \"data\": $PAYLOAD\n}'\n```\n\nHere:\n\n* `$URL` is the URL of the Gradio app as obtained in Step 0\n* `$API_NAME` is the name of the API endpoint for the event that you are running. You can get the API endpoint names by clicking the \"view API\" link at the bottom of the page.\n*  `$PAYLOAD` is a valid JSON data list containing the input payload, one element for each input component.\n\nWhen you make this `POST` request successfully, you will get an event id that is printed to the terminal in this format:\n\n```bash\n>> {\"event_id\": $EVENT_ID}   \n```\n\nThis `EVENT_ID` will be needed in the subsequent `curl` request to fetch the results of the prediction. \n\nHere are some examples of how to make the `POST` request\n\n**Basic Example**\n\nRevisiting the example at the beginning of the page, here is how to make the `POST` request for a simple Gradio application that takes in a single input text component:\n\n```bash\n$ curl -X POST https://abidlabs-en2fr.hf.space/call/predict -H \"Content-Type: application/json\" -d '{\n  \"data\": [\"Hello, my friend.\"] \n}'\n```\n\n**Multiple Input Components**\n\nThis [Gradio demo](https://huggingface.co/spaces/gradio/hello_world_3) accepts three inputs: a string corresponding to the `gr.Textbox`, a boolean value corresponding to the `gr.Checkbox`, and a numerical value corresponding to the `gr.Slider`. Here is the `POST` request:\n\n```bash\ncurl -X POST https://gradio-hello-world-3.hf.space/call/predict -H \"Content-Type: application/json\" -d '{\n  \"data\": [\"Hello\", true, 5]\n}'\n```\n\n**Private Spaces**\n\nAs mentioned earlier, if you are making a request to a private Space, you will need to pass in a [Hugging Face token](https://huggingface.co/settings/tokens) that has read access to the Space. The request will look like this:\n\n```bash\n$ curl -X POST https://private-space.hf.space/call/predict -H \"Content-Type: application/json\" -H \"Authorization: Bearer $HF_TOKEN\" -d '{\n  \"data\": [\"Hello, my friend.\"] \n}'\n```\n\n**Files**\n\nIf you are using `curl` to query a Gradio application that requires file inputs, the files *need* to be provided as URLs, and The URL needs to be enclosed in a dictionary in this format:\n\n```bash\n{\"path\": $URL}\n```\n\nHere is an example `POST` request:\n\n```bash\n$ curl -X POST https://gradio-image-mod.hf.space/call/predict -H \"Content-Type: application/json\" -d '{\n  \"data\": [{\"path\": \"https://raw.githubusercontent.com/gradio-app/gradio/main/test/test_files/bus.png\"}] \n}'\n```\n\n\n**Stateful Demos**\n\nIf your Gradio demo [persists user state](/guides/interface-state) across multiple interactions (e.g. is a chatbot), you can pass in a `session_hash` alongside the `data`. Requests with the same `session_hash` are assumed to be part of the same user session. Here's how that might look:\n\n```bash\n# These two requests will share a session\n\ncurl -X POST https://gradio-chatinterface-random-response.hf.space/call/chat -H \"Content-Type: application/json\" -d '{\n  \"data\": [\"Are you sentient?\"],\n  \"session_hash\": \"randomsequence1234\"\n}'\n\ncurl -X POST https://gradio-chatinterface-random-response.hf.space/call/chat -H \"Content-Type: application/json\" -d '{\n  \"data\": [\"Really?\"],\n  \"session_hash\": \"randomsequence1234\"\n}'\n\n# This request will be treated as a new session\n\ncurl -X POST https://gradio-chatinterface-random-response.hf.space/call/chat -H \"Content-Type: application/json\" -d '{\n  \"data\": [\"Are you sentient?\"],\n  \"session_hash\": \"newsequence5678\"\n}'\n```\n\n\n\n## Step 2: GET the result\n\nOnce you have received the `EVENT_ID` corresponding to your prediction, you can stream the results. Gradio stores these results  in a least-recently-used cache in the Gradio app. By default, the cache can store 2,000 results (across all users and endpoints of the app). \n\nTo stream the results for your prediction, make a `GET` request with the following syntax:\n\n```bash\n$ curl -N $URL/call/$API_NAME/$EVENT_ID\n```\n\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> If you are fetching results from a private Space, include a header with your HF token like this: `-H \"Authorization: Bearer $HF_TOKEN\"` in the `GET` request.\u003C/p>\n\nThis should produce a stream of responses in this format:\n\n```bash\nevent: ... \ndata: ...\nevent: ... \ndata: ...\n...\n```\n\nHere: `event` can be one of the following:\n* `generating`: indicating an intermediate result\n* `complete`: indicating that the prediction is complete and the final result \n* `error`: indicating that the prediction was not completed successfully\n* `heartbeat`: sent every 15 seconds to keep the request alive\n\nThe `data` is in the same format as the input payload: valid JSON data list containing the output result, one element for each output component.\n\nHere are some examples of what results you should expect if a request is completed successfully:\n\n**Basic Example**\n\nRevisiting the example at the beginning of the page, we would expect the result to look like this:\n\n```bash\nevent: complete\ndata: [\"Bonjour, mon ami.\"]\n```\n\n**Multiple Outputs**\n\nIf your endpoint returns multiple values, they will appear as elements of the `data` list:\n\n```bash\nevent: complete\ndata: [\"Good morning Hello. It is 5 degrees today\", -15.0]\n```\n\n**Streaming Example**\n\nIf your Gradio app [streams a sequence of values](/guides/streaming-outputs), then they will be streamed directly to your terminal, like this:\n\n```bash\nevent: generating\ndata: [\"Hello, w!\"]\nevent: generating\ndata: [\"Hello, wo!\"]\nevent: generating\ndata: [\"Hello, wor!\"]\nevent: generating\ndata: [\"Hello, worl!\"]\nevent: generating\ndata: [\"Hello, world!\"]\nevent: complete\ndata: [\"Hello, world!\"]\n```\n\n**File Example**\n\nIf your Gradio app returns a file, the file will be represented as a dictionary in this format (including potentially some additional keys):\n\n```python\n{\n    \"orig_name\": \"example.jpg\",\n    \"path\": \"/path/in/server.jpg\",\n    \"url\": \"https:/example.com/example.jpg\",\n    \"meta\": {\"_type\": \"gradio.FileData\"}\n}\n```\n\nIn your terminal, it may appear like this:\n\n```bash\nevent: complete\ndata: [{\"path\": \"/tmp/gradio/359933dc8d6cfe1b022f35e2c639e6e42c97a003/image.webp\", \"url\": \"https://gradio-image-mod.hf.space/c/file=/tmp/gradio/359933dc8d6cfe1b022f35e2c639e6e42c97a003/image.webp\", \"size\": null, \"orig_name\": \"image.webp\", \"mime_type\": null, \"is_stream\": false, \"meta\": {\"_type\": \"gradio.FileData\"}}]\n```",tags:["CURL","API","SPACES"],spaces:[],url:"/guides/querying-gradio-apps-with-curl/",contributor:null},{name:"gradio-and-llm-agents",category:"gradio-clients-and-lite",pretty_category:"Gradio Clients And Lite",guide_index:4,absolute_index:43,pretty_name:"Gradio And Llm Agents",content:"# Gradio & LLM Agents ü§ù\n\nLarge Language Models (LLMs) are very impressive but they can be made even more powerful if we could give them skills to accomplish specialized tasks.\n\nThe [gradio_tools](https://github.com/freddyaboulton/gradio-tools) library can turn any [Gradio](https://github.com/gradio-app/gradio) application into a [tool](https://python.langchain.com/en/latest/modules/agents/tools.html) that an [agent](https://docs.langchain.com/docs/components/agents/agent) can use to complete its task. For example, an LLM could use a Gradio tool to transcribe a voice recording it finds online and then summarize it for you. Or it could use a different Gradio tool to apply OCR to a document on your Google Drive and then answer questions about it.\n\nThis guide will show how you can use `gradio_tools` to grant your LLM Agent access to the cutting edge Gradio applications hosted in the world. Although `gradio_tools` are compatible with more than one agent framework, we will focus on [Langchain Agents](https://docs.langchain.com/docs/components/agents/) in this guide.\n\n## Some background\n\n### What are agents?\n\nA [LangChain agent](https://docs.langchain.com/docs/components/agents/agent) is a Large Language Model (LLM) that takes user input and reports an output based on using one of many tools at its disposal.\n\n### What is Gradio?\n\n[Gradio](https://github.com/gradio-app/gradio) is the defacto standard framework for building Machine Learning Web Applications and sharing them with the world - all with just python! üêç\n\n## gradio_tools - An end-to-end example\n\nTo get started with `gradio_tools`, all you need to do is import and initialize your tools and pass them to the langchain agent!\n\nIn the following example, we import the `StableDiffusionPromptGeneratorTool` to create a good prompt for stable diffusion, the\n`StableDiffusionTool` to create an image with our improved prompt, the `ImageCaptioningTool` to caption the generated image, and\nthe `TextToVideoTool` to create a video from a prompt.\n\nWe then tell our agent to create an image of a dog riding a skateboard, but to please improve our prompt ahead of time. We also ask\nit to caption the generated image and create a video for it. The agent can decide which tool to use without us explicitly telling it.\n\n```python\nimport os\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    raise ValueError(\"OPENAI_API_KEY must be set\")\n\nfrom langchain.agents import initialize_agent\nfrom langchain.llms import OpenAI\nfrom gradio_tools import (StableDiffusionTool, ImageCaptioningTool, StableDiffusionPromptGeneratorTool,\n                          TextToVideoTool)\n\nfrom langchain.memory import ConversationBufferMemory\n\nllm = OpenAI(temperature=0)\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\ntools = [StableDiffusionTool().langchain, ImageCaptioningTool().langchain,\n         StableDiffusionPromptGeneratorTool().langchain, TextToVideoTool().langchain]\n\n\nagent = initialize_agent(tools, llm, memory=memory, agent=\"conversational-react-description\", verbose=True)\noutput = agent.run(input=(\"Please create a photo of a dog riding a skateboard \"\n                          \"but improve my prompt prior to using an image generator.\"\n                          \"Please caption the generated image and create a video for it using the improved prompt.\"))\n```\n\nYou'll note that we are using some pre-built tools that come with `gradio_tools`. Please see this [doc](https://github.com/freddyaboulton/gradio-tools#gradio-tools-gradio--llm-agents) for a complete list of the tools that come with `gradio_tools`.\nIf you would like to use a tool that's not currently in `gradio_tools`, it is very easy to add your own. That's what the next section will cover.\n\n## gradio_tools - creating your own tool\n\nThe core abstraction is the `GradioTool`, which lets you define a new tool for your LLM as long as you implement a standard interface:\n\n```python\nclass GradioTool(BaseTool):\n\n    def __init__(self, name: str, description: str, src: str) -> None:\n\n    @abstractmethod\n    def create_job(self, query: str) -> Job:\n        pass\n\n    @abstractmethod\n    def postprocess(self, output: Tuple[Any] | Any) -> str:\n        pass\n```\n\nThe requirements are:\n\n1. The name for your tool\n2. The description for your tool. This is crucial! Agents decide which tool to use based on their description. Be precise and be sure to include example of what the input and the output of the tool should look like.\n3. The url or space id, e.g. `freddyaboulton/calculator`, of the Gradio application. Based on this value, `gradio_tool` will create a [gradio client](https://github.com/gradio-app/gradio/blob/main/client/python/README.md) instance to query the upstream application via API. Be sure to click the link and learn more about the gradio client library if you are not familiar with it.\n4. create_job - Given a string, this method should parse that string and return a job from the client. Most times, this is as simple as passing the string to the `submit` function of the client. More info on creating jobs [here](https://github.com/gradio-app/gradio/blob/main/client/python/README.md#making-a-prediction)\n5. postprocess - Given the result of the job, convert it to a string the LLM can display to the user.\n6. _Optional_ - Some libraries, e.g. [MiniChain](https://github.com/srush/MiniChain/tree/main), may need some info about the underlying gradio input and output types used by the tool. By default, this will return gr.Textbox() but\n   if you'd like to provide more accurate info, implement the `_block_input(self, gr)` and `_block_output(self, gr)` methods of the tool. The `gr` variable is the gradio module (the result of `import gradio as gr`). It will be\n   automatically imported by the `GradiTool` parent class and passed to the `_block_input` and `_block_output` methods.\n\nAnd that's it!\n\nOnce you have created your tool, open a pull request to the `gradio_tools` repo! We welcome all contributions.\n\n## Example tool - Stable Diffusion\n\nHere is the code for the StableDiffusion tool as an example:\n\n```python\nfrom gradio_tool import GradioTool\nimport os\n\nclass StableDiffusionTool(GradioTool):\n    \"\"\"Tool for calling stable diffusion from llm\"\"\"\n\n    def __init__(\n        self,\n        name=\"StableDiffusion\",\n        description=(\n            \"An image generator. Use this to generate images based on \"\n            \"text input. Input should be a description of what the image should \"\n            \"look like. The output will be a path to an image file.\"\n        ),\n        src=\"gradio-client-demos/stable-diffusion\",\n        hf_token=None,\n    ) -> None:\n        super().__init__(name, description, src, hf_token)\n\n    def create_job(self, query: str) -> Job:\n        return self.client.submit(query, \"\", 9, fn_index=1)\n\n    def postprocess(self, output: str) -> str:\n        return [os.path.join(output, i) for i in os.listdir(output) if not i.endswith(\"json\")][0]\n\n    def _block_input(self, gr) -> \"gr.components.Component\":\n        return gr.Textbox()\n\n    def _block_output(self, gr) -> \"gr.components.Component\":\n        return gr.Image()\n```\n\nSome notes on this implementation:\n\n1. All instances of `GradioTool` have an attribute called `client` that is a pointed to the underlying [gradio client](https://github.com/gradio-app/gradio/tree/main/client/python#gradio_client-use-a-gradio-app-as-an-api----in-3-lines-of-python). That is what you should use\n   in the `create_job` method.\n2. `create_job` just passes the query string to the `submit` function of the client with some other parameters hardcoded, i.e. the negative prompt string and the guidance scale. We could modify our tool to also accept these values from the input string in a subsequent version.\n3. The `postprocess` method simply returns the first image from the gallery of images created by the stable diffusion space. We use the `os` module to get the full path of the image.\n\n## Conclusion\n\nYou now know how to extend the abilities of your LLM with the 1000s of gradio spaces running in the wild!\nAgain, we welcome any contributions to the [gradio_tools](https://github.com/freddyaboulton/gradio-tools) library.\nWe're excited to see the tools you all build!\n",tags:[],spaces:[],url:"/guides/gradio-and-llm-agents/",contributor:null},{name:"gradio-lite",category:"gradio-clients-and-lite",pretty_category:"Gradio Clients And Lite",guide_index:5,absolute_index:44,pretty_name:"Gradio Lite",content:"# Gradio-Lite: Serverless Gradio Running Entirely in Your Browser\n\n\n\nGradio is a popular Python library for creating interactive machine learning apps. Traditionally, Gradio applications have relied on server-side infrastructure to run, which can be a hurdle for developers who need to host their applications.\n\nEnter Gradio-lite (`@gradio/lite`): a library that leverages [Pyodide](https://pyodide.org/en/stable/) to bring Gradio directly to your browser. In this blog post, we'll explore what `@gradio/lite` is, go over example code, and discuss the benefits it offers for running Gradio applications.\n\n## What is `@gradio/lite`?\n\n`@gradio/lite` is a JavaScript library that enables you to run Gradio applications directly within your web browser. It achieves this by utilizing Pyodide, a Python runtime for WebAssembly, which allows Python code to be executed in the browser environment. With `@gradio/lite`, you can **write regular Python code for your Gradio applications**, and they will **run seamlessly in the browser** without the need for server-side infrastructure.\n\n## Getting Started\n\nLet's build a \"Hello World\" Gradio app in `@gradio/lite`\n\n\n### 1. Import JS and CSS\n\nStart by creating a new HTML file, if you don't have one already. Importing the JavaScript and CSS corresponding to the `@gradio/lite` package by using the following code:\n\n\n```html\n\u003Chtml>\n\t\u003Chead>\n\t\t\u003Cscript type=\"module\" crossorigin src=\"https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.js\">\u003C/script>\n\t\t\u003Clink rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.css\" />\n\t\u003C/head>\n\u003C/html>\n```\n\nNote that you should generally use the latest version of `@gradio/lite` that is available. You can see the [versions available here](https://www.jsdelivr.com/package/npm/@gradio/lite?tab=files).\n\n### 2. Create the `\u003Cgradio-lite>` tags\n\nSomewhere in the body of your HTML page (wherever you'd like the Gradio app to be rendered), create opening and closing `\u003Cgradio-lite>` tags.\n\n```html\n\u003Chtml>\n\t\u003Chead>\n\t\t\u003Cscript type=\"module\" crossorigin src=\"https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.js\">\u003C/script>\n\t\t\u003Clink rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.css\" />\n\t\u003C/head>\n\t\u003Cbody>\n\t\t\u003Cgradio-lite>\n\t\t\u003C/gradio-lite>\n\t\u003C/body>\n\u003C/html>\n```\n\nNote: you can add the `theme` attribute to the `\u003Cgradio-lite>` tag to force the theme to be dark or light (by default, it respects the system theme). E.g.\n\n```html\n\u003Cgradio-lite theme=\"dark\">\n...\n\u003C/gradio-lite>\n```\n\n### 3. Write your Gradio app inside of the tags\n\nNow, write your Gradio app as you would normally, in Python! Keep in mind that since this is Python, whitespace and indentations matter.\n\n```html\n\u003Chtml>\n\t\u003Chead>\n\t\t\u003Cscript type=\"module\" crossorigin src=\"https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.js\">\u003C/script>\n\t\t\u003Clink rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.css\" />\n\t\u003C/head>\n\t\u003Cbody>\n\t\t\u003Cgradio-lite>\n\t\timport gradio as gr\n\n\t\tdef greet(name):\n\t\t\treturn \"Hello, \" + name + \"!\"\n\n\t\tgr.Interface(greet, \"textbox\", \"textbox\").launch()\n\t\t\u003C/gradio-lite>\n\t\u003C/body>\n\u003C/html>\n```\n\nAnd that's it! You should now be able to open your HTML page in the browser and see the Gradio app rendered! Note that it may take a little while for the Gradio app to load initially since Pyodide can take a while to install in your browser.\n\n**Note on debugging**: to see any errors in your Gradio-lite application, open the inspector in your web browser. All errors (including Python errors) will be printed there.\n\n## More Examples: Adding Additional Files and Requirements\n\nWhat if you want to create a Gradio app that spans multiple files? Or that has custom Python requirements? Both are possible with `@gradio/lite`!\n\n### Multiple Files\n\nAdding multiple files within a `@gradio/lite` app is very straightforward: use the `\u003Cgradio-file>` tag. You can have as many `\u003Cgradio-file>` tags as you want, but each one needs to have a `name` attribute and the entry point to your Gradio app should have the `entrypoint` attribute.\n\nHere's an example:\n\n```html\n\u003Cgradio-lite>\n\n\u003Cgradio-file name=\"app.py\" entrypoint>\nimport gradio as gr\nfrom utils import add\n\ndemo = gr.Interface(fn=add, inputs=[\"number\", \"number\"], outputs=\"number\")\n\ndemo.launch()\n\u003C/gradio-file>\n\n\u003Cgradio-file name=\"utils.py\" >\ndef add(a, b):\n\treturn a + b\n\u003C/gradio-file>\n\n\u003C/gradio-lite>\n\n```\n\n### Additional Requirements\n\nIf your Gradio app has additional requirements, it is usually possible to [install them in the browser using micropip](https://pyodide.org/en/stable/usage/loading-packages.html#loading-packages). We've created a wrapper to make this paticularly convenient: simply list your requirements in the same syntax as a `requirements.txt` and enclose them with `\u003Cgradio-requirements>` tags.\n\nHere, we install `transformers_js_py` to run a text classification model directly in the browser!\n\n```html\n\u003Cgradio-lite>\n\n\u003Cgradio-requirements>\ntransformers_js_py\n\u003C/gradio-requirements>\n\n\u003Cgradio-file name=\"app.py\" entrypoint>\nfrom transformers_js import import_transformers_js\nimport gradio as gr\n\ntransformers = await import_transformers_js()\npipeline = transformers.pipeline\npipe = await pipeline('sentiment-analysis')\n\nasync def classify(text):\n\treturn await pipe(text)\n\ndemo = gr.Interface(classify, \"textbox\", \"json\")\ndemo.launch()\n\u003C/gradio-file>\n\n\u003C/gradio-lite>\n\n```\n\n**Try it out**: You can see this example running in [this Hugging Face Static Space](https://huggingface.co/spaces/abidlabs/gradio-lite-classify), which lets you host static (serverless) web applications for free. Visit the page and you'll be able to run a machine learning model without internet access!\n\n### SharedWorker mode\n\nBy default, Gradio-Lite executes Python code in a [Web Worker](https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API) with [Pyodide](https://pyodide.org/) runtime, and each Gradio-Lite app has its own worker.\nIt has some benefits such as environment isolation.\n\nHowever, when there are many Gradio-Lite apps in the same page, it may cause performance issues such as high memory usage because each app has its own worker and Pyodide runtime.\nIn such cases, you can use the **SharedWorker mode** to share a single Pyodide runtime in a [SharedWorker](https://developer.mozilla.org/en-US/docs/Web/API/SharedWorker) among multiple Gradio-Lite apps. To enable the SharedWorker mode, set the `shared-worker` attribute to the `\u003Cgradio-lite>` tag.\n\n```html\n\u003C!-- These two Gradio-Lite apps share a single worker -->\n\n\u003Cgradio-lite shared-worker>\nimport gradio as gr\n# ...\n\u003C/gradio-lite>\n\n\u003Cgradio-lite shared-worker>\nimport gradio as gr\n# ...\n\u003C/gradio-lite>\n```\n\nWhen using the SharedWorker mode, you should be aware of the following points:\n* The apps share the same Python environment, which means that they can access the same  modules and objects. If, for example, one app makes changes to some modules, the changes will be visible to other apps.\n* The file system is shared among the apps, while each app's files are mounted in each home directory, so each app can access the files of other apps.\n\n### Code and Demo Playground\n\nIf you'd like to see the code side-by-side with the demo just pass in the `playground` attribute to the gradio-lite element. This will create an interactive playground that allows you to change the code and update the demo! If you're using playground, you can also set layout to either 'vertical' or 'horizontal' which will determine if the code editor and preview are side-by-side or on top of each other (by default it's reposnsive with the width of the page).\n\n```html\n\u003Cgradio-lite playground layout=\"horizontal\">\nimport gradio as gr\n\ngr.Interface(fn=lambda x: x,\n\t\t\tinputs=gr.Textbox(),\n\t\t\toutputs=gr.Textbox()\n\t\t).launch()\n\u003C/gradio-lite>\n```\n\n## Benefits of Using `@gradio/lite`\n\n### 1. Serverless Deployment\nThe primary advantage of @gradio/lite is that it eliminates the need for server infrastructure. This simplifies deployment, reduces server-related costs, and makes it easier to share your Gradio applications with others.\n\n### 2. Low Latency\nBy running in the browser, @gradio/lite offers low-latency interactions for users. There's no need for data to travel to and from a server, resulting in faster responses and a smoother user experience.\n\n### 3. Privacy and Security\nSince all processing occurs within the user's browser, `@gradio/lite` enhances privacy and security. User data remains on their device, providing peace of mind regarding data handling.\n\n### Limitations\n\n* Currently, the biggest limitation in using `@gradio/lite` is that your Gradio apps will generally take more time (usually 5-15 seconds) to load initially in the browser. This is because the browser needs to load the Pyodide runtime before it can render Python code.\n\n* Not every Python package is supported by Pyodide. While `gradio` and many other popular packages (including `numpy`, `scikit-learn`, and `transformers-js`) can be installed in Pyodide, if your app has many dependencies, its worth checking whether whether the dependencies are included in Pyodide, or can be [installed with `micropip`](https://micropip.pyodide.org/en/v0.2.2/project/api.html#micropip.install).\n\n## Try it out!\n\nYou can immediately try out `@gradio/lite` by copying and pasting this code in a local `index.html` file and opening it with your browser:\n\n```html\n\u003Chtml>\n\t\u003Chead>\n\t\t\u003Cscript type=\"module\" crossorigin src=\"https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.js\">\u003C/script>\n\t\t\u003Clink rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.css\" />\n\t\u003C/head>\n\t\u003Cbody>\n\t\t\u003Cgradio-lite>\n\t\timport gradio as gr\n\n\t\tdef greet(name):\n\t\t\treturn \"Hello, \" + name + \"!\"\n\n\t\tgr.Interface(greet, \"textbox\", \"textbox\").launch()\n\t\t\u003C/gradio-lite>\n\t\u003C/body>\n\u003C/html>\n```\n\n\nWe've also created a playground on the Gradio website that allows you to interactively edit code and see the results immediately!\n\nPlayground: https://www.gradio.app/playground\n",tags:["SERVERLESS","BROWSER","PYODIDE"],spaces:[],url:"/guides/gradio-lite/",contributor:null},{name:"gradio-lite-and-transformers-js",category:"gradio-clients-and-lite",pretty_category:"Gradio Clients And Lite",guide_index:6,absolute_index:45,pretty_name:"Gradio Lite And Transformers Js",content:"# Building Serverless Machine Learning Apps with Gradio-Lite and Transformers.js\n\n\n\nGradio and [Transformers](https://huggingface.co/docs/transformers/index) are a powerful combination for building machine learning apps with a web interface. Both libraries have serverless versions that can run entirely in the browser: [Gradio-Lite](./gradio-lite) and [Transformers.js](https://huggingface.co/docs/transformers.js/index).\nIn this document, we will introduce how to create a serverless machine learning application using Gradio-Lite and Transformers.js.\nYou will just write Python code within a static HTML file and host it without setting up a server-side Python runtime.\n\n\n## Libraries Used\n\n### Gradio-Lite\n\nGradio-Lite is the serverless version of Gradio, allowing you to build serverless web UI applications by embedding Python code within HTML. For a detailed introduction to Gradio-Lite itself, please read [this Guide](./gradio-lite).\n\n### Transformers.js and Transformers.js.py\n\nTransformers.js is the JavaScript version of the Transformers library that allows you to run machine learning models entirely in the browser.\nSince Transformers.js is a JavaScript library, it cannot be directly used from the Python code of Gradio-Lite applications. To address this, we use a wrapper library called [Transformers.js.py](https://github.com/whitphx/transformers.js.py).\nThe name Transformers.js.py may sound unusual, but it represents the necessary technology stack for using Transformers.js from Python code within a browser environment. The regular Transformers library is not compatible with browser environments.\n\n## Sample Code\n\nHere's an example of how to use Gradio-Lite and Transformers.js together.\nPlease create an HTML file and paste the following code:\n\n```html\n\u003Chtml>\n\t\u003Chead>\n\t\t\u003Cscript type=\"module\" crossorigin src=\"https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.js\">\u003C/script>\n\t\t\u003Clink rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.css\" />\n\t\u003C/head>\n\t\u003Cbody>\n\t\t\u003Cgradio-lite>\nimport gradio as gr\nfrom transformers_js_py import pipeline\n\npipe = await pipeline('sentiment-analysis')\n\ndemo = gr.Interface.from_pipeline(pipe)\n\ndemo.launch()\n\n\t\t\t\u003Cgradio-requirements>\ntransformers-js-py\n\t\t\t\u003C/gradio-requirements>\n\t\t\u003C/gradio-lite>\n\t\u003C/body>\n\u003C/html>\n```\n\nHere is a running example of the code above (after the app has loaded, you could disconnect your Internet connection and the app will still work since its running entirely in your browser):\n\n\u003Cgradio-lite shared-worker>\nimport gradio as gr\nfrom transformers_js_py import pipeline\n\u003C!-- --->\npipe = await pipeline('sentiment-analysis')\n\u003C!-- --->\ndemo = gr.Interface.from_pipeline(pipe)\n\u003C!-- --->\ndemo.launch()\n\u003Cgradio-requirements>\ntransformers-js-py\n\u003C/gradio-requirements>\n\u003C/gradio-lite>\n\nAnd you you can open your HTML file in a browser to see the Gradio app running!\n\nThe Python code inside the `\u003Cgradio-lite>` tag is the Gradio application code. For more details on this part, please refer to [this article](./gradio-lite).\nThe `\u003Cgradio-requirements>` tag is used to specify packages to be installed in addition to Gradio-Lite and its dependencies. In this case, we are using Transformers.js.py (`transformers-js-py`), so it is specified here.\n\nLet's break down the code:\n\n`pipe = await pipeline('sentiment-analysis')` creates a Transformers.js pipeline.\nIn this example, we create a sentiment analysis pipeline.\nFor more information on the available pipeline types and usage, please refer to the [Transformers.js documentation](https://huggingface.co/docs/transformers.js/index).\n\n`demo = gr.Interface.from_pipeline(pipe)` creates a Gradio app instance. By passing the Transformers.js.py pipeline to `gr.Interface.from_pipeline()`, we can create an interface that utilizes that pipeline with predefined input and output components.\n\nFinally, `demo.launch()` launches the created app.\n\n## Customizing the Model or Pipeline\n\nYou can modify the line `pipe = await pipeline('sentiment-analysis')` in the sample above to try different models or tasks.\n\nFor example, if you change it to `pipe = await pipeline('sentiment-analysis', 'Xenova/bert-base-multilingual-uncased-sentiment')`, you can test the same sentiment analysis task but with a different model. The second argument of the `pipeline` function specifies the model name.\nIf it's not specified like in the first example, the default model is used. For more details on these specs, refer to the [Transformers.js documentation](https://huggingface.co/docs/transformers.js/index).\n\n\u003Cgradio-lite shared-worker>\nimport gradio as gr\nfrom transformers_js_py import pipeline\n\u003C!-- --->\npipe = await pipeline('sentiment-analysis', 'Xenova/bert-base-multilingual-uncased-sentiment')\n\u003C!-- --->\ndemo = gr.Interface.from_pipeline(pipe)\n\u003C!-- --->\ndemo.launch()\n\u003Cgradio-requirements>\ntransformers-js-py\n\u003C/gradio-requirements>\n\u003C/gradio-lite>\n\nAs another example, changing it to `pipe = await pipeline('image-classification')` creates a pipeline for image classification instead of sentiment analysis.\nIn this case, the interface created with `demo = gr.Interface.from_pipeline(pipe)` will have a UI for uploading an image and displaying the classification result. The `gr.Interface.from_pipeline` function automatically creates an appropriate UI based on the type of pipeline.\n\n\u003Cgradio-lite shared-worker>\nimport gradio as gr\nfrom transformers_js_py import pipeline\n\u003C!-- --->\npipe = await pipeline('image-classification')\n\u003C!-- --->\ndemo = gr.Interface.from_pipeline(pipe)\n\u003C!-- --->\ndemo.launch()\n\u003Cgradio-requirements>\ntransformers-js-py\n\u003C/gradio-requirements>\n\u003C/gradio-lite>\n\n\u003Cbr>\n\n**Note**: If you use an audio pipeline, such as `automatic-speech-recognition`, you will need to put `transformers-js-py[audio]` in your `\u003Cgradio-requirements>` as there are additional requirements needed to process audio files.\n\n## Customizing the UI\n\nInstead of using `gr.Interface.from_pipeline()`, you can define the user interface using Gradio's regular API.\nHere's an example where the Python code inside the `\u003Cgradio-lite>` tag has been modified from the previous sample:\n\n```html\n\u003Chtml>\n\t\u003Chead>\n\t\t\u003Cscript type=\"module\" crossorigin src=\"https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.js\">\u003C/script>\n\t\t\u003Clink rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.css\" />\n\t\u003C/head>\n\t\u003Cbody>\n\t\t\u003Cgradio-lite>\nimport gradio as gr\nfrom transformers_js_py import pipeline\n\npipe = await pipeline('sentiment-analysis')\n\nasync def fn(text):\n\tresult = await pipe(text)\n\treturn result\n\ndemo = gr.Interface(\n\tfn=fn,\n\tinputs=gr.Textbox(),\n\toutputs=gr.JSON(),\n)\n\ndemo.launch()\n\n\t\t\t\u003Cgradio-requirements>\ntransformers-js-py\n\t\t\t\u003C/gradio-requirements>\n\t\t\u003C/gradio-lite>\n\t\u003C/body>\n\u003C/html>\n```\n\nIn this example, we modified the code to construct the Gradio user interface manually so that we could output the result as JSON.\n\n\u003Cgradio-lite shared-worker>\nimport gradio as gr\nfrom transformers_js_py import pipeline\n\u003C!-- --->\npipe = await pipeline('sentiment-analysis')\n\u003C!-- --->\nasync def fn(text):\n\tresult = await pipe(text)\n\treturn result\n\u003C!-- --->\ndemo = gr.Interface(\n\tfn=fn,\n\tinputs=gr.Textbox(),\n\toutputs=gr.JSON(),\n)\n\u003C!-- --->\ndemo.launch()\n\u003Cgradio-requirements>\ntransformers-js-py\n\u003C/gradio-requirements>\n\u003C/gradio-lite>\n\n## Conclusion\n\nBy combining Gradio-Lite and Transformers.js (and Transformers.js.py), you can create serverless machine learning applications that run entirely in the browser.\n\nGradio-Lite provides a convenient method to create an interface for a given Transformers.js pipeline, `gr.Interface.from_pipeline()`.\nThis method automatically constructs the interface based on the pipeline's task type.\n\nAlternatively, you can define the interface manually using Gradio's regular API, as shown in the second example.\n\nBy using these libraries, you can build and deploy machine learning applications without the need for server-side Python setup or external dependencies.\n",tags:["SERVERLESS","BROWSER","PYODIDE","TRANSFORMERS"],spaces:[],url:"/guides/gradio-lite-and-transformers-js/",contributor:null},{name:"fastapi-app-with-the-gradio-client",category:"gradio-clients-and-lite",pretty_category:"Gradio Clients And Lite",guide_index:7,absolute_index:46,pretty_name:"Fastapi App With The Gradio Client",content:"# Building a FastAPI App with the Gradio Python Client\n\n\n\nIn this blog post, we will demonstrate how to use the `gradio_client` [Python library](getting-started-with-the-python-client/), which enables developers to make requests to a Gradio app programmatically, by creating an example FastAPI web app. The web app we will be building is called \"Acapellify,\" and it will allow users to upload video files as input and return a version of that video without instrumental music. It will also display a gallery of generated videos.\n\n**Prerequisites**\n\nBefore we begin, make sure you are running Python 3.9 or later, and have the following libraries installed:\n\n- `gradio_client`\n- `fastapi`\n- `uvicorn`\n\nYou can install these libraries from `pip`:\n\n```bash\n$ pip install gradio_client fastapi uvicorn\n```\n\nYou will also need to have ffmpeg installed. You can check to see if you already have ffmpeg by running in your terminal:\n\n```bash\n$ ffmpeg version\n```\n\nOtherwise, install ffmpeg [by following these instructions](https://www.hostinger.com/tutorials/how-to-install-ffmpeg).\n\n## Step 1: Write the Video Processing Function\n\nLet's start with what seems like the most complex bit -- using machine learning to remove the music from a video.\n\nLuckily for us, there's an existing Space we can use to make this process easier: [https://huggingface.co/spaces/abidlabs/music-separation](https://huggingface.co/spaces/abidlabs/music-separation). This Space takes an audio file and produces two separate audio files: one with the instrumental music and one with all other sounds in the original clip. Perfect to use with our client!\n\nOpen a new Python file, say `main.py`, and start by importing the `Client` class from `gradio_client` and connecting it to this Space:\n\n```py\nfrom gradio_client import Client\n\nclient = Client(\"abidlabs/music-separation\")\n\ndef acapellify(audio_path):\n    result = client.predict(audio_path, api_name=\"/predict\")\n    return result[0]\n```\n\nThat's all the code that's needed -- notice that the API endpoints returns two audio files (one without the music, and one with just the music) in a list, and so we just return the first element of the list.\n\n---\n\n**Note**: since this is a public Space, there might be other users using this Space as well, which might result in a slow experience. You can duplicate this Space with your own [Hugging Face token](https://huggingface.co/settings/tokens) and create a private Space that only you have will have access to and bypass the queue. To do that, simply replace the first two lines above with:\n\n```py\nfrom gradio_client import Client\n\nclient = Client.duplicate(\"abidlabs/music-separation\", hf_token=YOUR_HF_TOKEN)\n```\n\nEverything else remains the same!\n\n---\n\nNow, of course, we are working with video files, so we first need to extract the audio from the video files. For this, we will be using the `ffmpeg` library, which does a lot of heavy lifting when it comes to working with audio and video files. The most common way to use `ffmpeg` is through the command line, which we'll call via Python's `subprocess` module:\n\nOur video processing workflow will consist of three steps:\n\n1. First, we start by taking in a video filepath and extracting the audio using `ffmpeg`.\n2. Then, we pass in the audio file through the `acapellify()` function above.\n3. Finally, we combine the new audio with the original video to produce a final acapellified video.\n\nHere's the complete code in Python, which you can add to your `main.py` file:\n\n```python\nimport subprocess\n\ndef process_video(video_path):\n    old_audio = os.path.basename(video_path).split(\".\")[0] + \".m4a\"\n    subprocess.run(['ffmpeg', '-y', '-i', video_path, '-vn', '-acodec', 'copy', old_audio])\n\n    new_audio = acapellify(old_audio)\n\n    new_video = f\"acap_{video_path}\"\n    subprocess.call(['ffmpeg', '-y', '-i', video_path, '-i', new_audio, '-map', '0:v', '-map', '1:a', '-c:v', 'copy', '-c:a', 'aac', '-strict', 'experimental', f\"static/{new_video}\"])\n    return new_video\n```\n\nYou can read up on [ffmpeg documentation](https://ffmpeg.org/ffmpeg.html) if you'd like to understand all of the command line parameters, as they are beyond the scope of this tutorial.\n\n## Step 2: Create a FastAPI app (Backend Routes)\n\nNext up, we'll create a simple FastAPI app. If you haven't used FastAPI before, check out [the great FastAPI docs](https://fastapi.tiangolo.com/). Otherwise, this basic template, which we add to `main.py`, will look pretty familiar:\n\n```python\nimport os\nfrom fastapi import FastAPI, File, UploadFile, Request\nfrom fastapi.responses import HTMLResponse, RedirectResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.templating import Jinja2Templates\n\napp = FastAPI()\nos.makedirs(\"static\", exist_ok=True)\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\ntemplates = Jinja2Templates(directory=\"templates\")\n\nvideos = []\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def home(request: Request):\n    return templates.TemplateResponse(\n        \"home.html\", {\"request\": request, \"videos\": videos})\n\n@app.post(\"/uploadvideo/\")\nasync def upload_video(video: UploadFile = File(...)):\n    new_video = process_video(video.filename)\n    videos.append(new_video)\n    return RedirectResponse(url='/', status_code=303)\n```\n\nIn this example, the FastAPI app has two routes: `/` and `/uploadvideo/`.\n\nThe `/` route returns an HTML template that displays a gallery of all uploaded videos.\n\nThe `/uploadvideo/` route accepts a `POST` request with an `UploadFile` object, which represents the uploaded video file. The video file is \"acapellified\" via the `process_video()` method, and the output video is stored in a list which stores all of the uploaded videos in memory.\n\nNote that this is a very basic example and if this were a production app, you will need to add more logic to handle file storage, user authentication, and security considerations.\n\n## Step 3: Create a FastAPI app (Frontend Template)\n\nFinally, we create the frontend of our web application. First, we create a folder called `templates` in the same directory as `main.py`. We then create a template, `home.html` inside the `templates` folder. Here is the resulting file structure:\n\n```csv\n‚îú‚îÄ‚îÄ main.py\n‚îú‚îÄ‚îÄ templates\n‚îÇ   ‚îî‚îÄ‚îÄ home.html\n```\n\nWrite the following as the contents of `home.html`:\n\n```html\n&lt;!DOCTYPE html> &lt;html> &lt;head> &lt;title>Video Gallery&lt;/title>\n&lt;style> body { font-family: sans-serif; margin: 0; padding: 0;\nbackground-color: #f5f5f5; } h1 { text-align: center; margin-top: 30px;\nmargin-bottom: 20px; } .gallery { display: flex; flex-wrap: wrap;\njustify-content: center; gap: 20px; padding: 20px; } .video { border: 2px solid\n#ccc; box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.2); border-radius: 5px; overflow:\nhidden; width: 300px; margin-bottom: 20px; } .video video { width: 100%; height:\n200px; } .video p { text-align: center; margin: 10px 0; } form { margin-top:\n20px; text-align: center; } input[type=\"file\"] { display: none; } .upload-btn {\ndisplay: inline-block; background-color: #3498db; color: #fff; padding: 10px\n20px; font-size: 16px; border: none; border-radius: 5px; cursor: pointer; }\n.upload-btn:hover { background-color: #2980b9; } .file-name { margin-left: 10px;\n} &lt;/style> &lt;/head> &lt;body> &lt;h1>Video Gallery&lt;/h1> {% if videos %}\n&lt;div class=\"gallery\"> {% for video in videos %} &lt;div class=\"video\">\n&lt;video controls> &lt;source src=\"{{ url_for('static', path=video) }}\"\ntype=\"video/mp4\"> Your browser does not support the video tag. &lt;/video>\n&lt;p>{{ video }}&lt;/p> &lt;/div> {% endfor %} &lt;/div> {% else %} &lt;p>No\nvideos uploaded yet.&lt;/p> {% endif %} &lt;form action=\"/uploadvideo/\"\nmethod=\"post\" enctype=\"multipart/form-data\"> &lt;label for=\"video-upload\"\nclass=\"upload-btn\">Choose video file&lt;/label> &lt;input type=\"file\"\nname=\"video\" id=\"video-upload\"> &lt;span class=\"file-name\">&lt;/span> &lt;button\ntype=\"submit\" class=\"upload-btn\">Upload&lt;/button> &lt;/form> &lt;script> //\nDisplay selected file name in the form const fileUpload =\ndocument.getElementById(\"video-upload\"); const fileName =\ndocument.querySelector(\".file-name\"); fileUpload.addEventListener(\"change\", (e)\n=> { fileName.textContent = e.target.files[0].name; }); &lt;/script> &lt;/body>\n&lt;/html>\n```\n\n## Step 4: Run your FastAPI app\n\nFinally, we are ready to run our FastAPI app, powered by the Gradio Python Client!\n\nOpen up a terminal and navigate to the directory containing `main.py`. Then run the following command in the terminal:\n\n```bash\n$ uvicorn main:app\n```\n\nYou should see an output that looks like this:\n\n```csv\nLoaded as API: https://abidlabs-music-separation.hf.space ‚úî\nINFO:     Started server process [1360]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n```\n\nAnd that's it! Start uploading videos and you'll get some \"acapellified\" videos in response (might take seconds to minutes to process depending on the length of your videos). Here's how the UI looks after uploading two videos:\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/acapellify.png)\n\nIf you'd like to learn more about how to use the Gradio Python Client in your projects, [read the dedicated Guide](/guides/getting-started-with-the-python-client/).\n",tags:["CLIENT","API","WEB APP"],spaces:[],url:"/guides/fastapi-app-with-the-gradio-client/",contributor:null}]},{category:"Other Tutorials",guides:[{name:"using-hugging-face-integrations",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:1,absolute_index:47,pretty_name:"Using Hugging Face Integrations",content:"# Using Hugging Face Integrations\n\n\n\n\n\n\n## Introduction\n\nThe Hugging Face Hub is a central platform that has hundreds of thousands of [models](https://huggingface.co/models), [datasets](https://huggingface.co/datasets) and [demos](https://huggingface.co/spaces) (also known as Spaces). \n\nGradio has multiple features that make it extremely easy to leverage existing models and Spaces on the Hub. This guide walks through these features.\n\n\n## Demos with the Hugging Face Inference Endpoints\n\nHugging Face has a service called [Serverless Inference Endpoints](https://huggingface.co/docs/api-inference/index), which allows you to send HTTP requests to models on the Hub. The API includes a generous free tier, and you can switch to [dedicated Inference Endpoints](https://huggingface.co/inference-endpoints/dedicated) when you want to use it in production. Gradio integrates directly with Serverless Inference Endpoints so that you can create a demo simply by specifying a model's name (e.g. `Helsinki-NLP/opus-mt-en-es`), like this:\n\n```python\nimport gradio as gr\n\ndemo = gr.load(\"Helsinki-NLP/opus-mt-en-es\", src=\"models\")\n\ndemo.launch()\n```\n\nFor any Hugging Face model supported in Inference Endpoints, Gradio automatically infers the expected input and output and make the underlying server calls, so you don't have to worry about defining the prediction function. \n\nNotice that we just put specify the model name and state that the `src` should be `models` (Hugging Face's Model Hub). There is no need to install any dependencies (except `gradio`) since you are not loading the model on your computer.\n\nYou might notice that the first inference takes a little bit longer. This happens since the Inference Endpoints is loading the model in the server. You get some benefits afterward:\n\n- The inference will be much faster.\n- The server caches your requests.\n- You get built-in automatic scaling.\n\n## Hosting your Gradio demos on Spaces\n\n[Hugging Face Spaces](https://hf.co/spaces) allows anyone to host their Gradio demos freely, and uploading your Gradio demos take a couple of minutes. You can head to [hf.co/new-space](https://huggingface.co/new-space), select the Gradio SDK, create an `app.py` file, and voila! You have a demo you can share with anyone else. To learn more, read [this guide how to host on Hugging Face Spaces using the website](https://huggingface.co/blog/gradio-spaces).\n\nAlternatively, you can create a Space programmatically, making use of the [huggingface_hub client library](https://huggingface.co/docs/huggingface_hub/index) library. Here's an example:\n\n```python\nfrom huggingface_hub import (\n    create_repo,\n    get_full_repo_name,\n    upload_file,\n)\ncreate_repo(name=target_space_name, token=hf_token, repo_type=\"space\", space_sdk=\"gradio\")\nrepo_name = get_full_repo_name(model_id=target_space_name, token=hf_token)\nfile_url = upload_file(\n    path_or_fileobj=\"file.txt\",\n    path_in_repo=\"app.py\",\n    repo_id=repo_name,\n    repo_type=\"space\",\n    token=hf_token,\n)\n```\n\nHere, `create_repo` creates a gradio repo with the target name under a specific account using that account's Write Token. `repo_name` gets the full repo name of the related repo. Finally `upload_file` uploads a file inside the repo with the name `app.py`.\n\n\n## Loading demos from Spaces\n\nYou can also use and remix existing Gradio demos on Hugging Face Spaces. For example, you could take two existing Gradio demos on Spaces and put them as separate tabs and create a new demo. You can run this new demo locally, or upload it to Spaces, allowing endless possibilities to remix and create new demos!\n\nHere's an example that does exactly that:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n  with gr.Tab(\"Translate to Spanish\"):\n    gr.load(\"gradio/en2es\", src=\"spaces\")\n  with gr.Tab(\"Translate to French\"):\n    gr.load(\"abidlabs/en2fr\", src=\"spaces\")\n\ndemo.launch()\n```\n\nNotice that we use `gr.load()`, the same method we used to load models using Inference Endpoints. However, here we specify that the `src` is `spaces` (Hugging Face Spaces). \n\nNote: loading a Space in this way may result in slight differences from the original Space. In particular, any attributes that apply to the entire Blocks, such as the theme or custom CSS/JS, will not be loaded. You can copy these properties from the Space you are loading into your own `Blocks` object. \n\n## Demos with the `Pipeline` in `transformers`\n\nHugging Face's popular `transformers` library has a very easy-to-use abstraction, [`pipeline()`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/pipelines#transformers.pipeline) that handles most of the complex code to offer a simple API for common tasks. By specifying the task and an (optional) model, you can build a demo around an existing model with few lines of Python:\n\n```python\nimport gradio as gr\n\nfrom transformers import pipeline\n\npipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n\ndef predict(text):\n  return pipe(text)[0][\"translation_text\"]\n\ndemo = gr.Interface(\n  fn=predict,\n  inputs='text',\n  outputs='text',\n)\n\ndemo.launch()\n```\n\nBut `gradio` actually makes it even easier to convert a `pipeline` to a demo, simply by using the `gradio.Interface.from_pipeline` methods, which skips the need to specify the input and output components:\n\n```python\nfrom transformers import pipeline\nimport gradio as gr\n\npipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n\ndemo = gr.Interface.from_pipeline(pipe)\ndemo.launch()\n```\n\nThe previous code produces the following interface, which you can try right here in your browser:\n\n\u003Cgradio-app space=\"gradio/en2es\">\u003C/gradio-app>\n\n\n## Recap\n\nThat's it! Let's recap the various ways Gradio and Hugging Face work together:\n\n1. You can build a demo around Inference Endpoints without having to load the model, by using `gr.load()`.\n2. You host your Gradio demo on Hugging Face Spaces, either using the GUI or entirely in Python.\n3. You can load demos from Hugging Face Spaces to remix and create new Gradio demos using `gr.load()`.\n4. You can convert a `transformers` pipeline into a Gradio demo using `from_pipeline()`.\n\nü§ó\n",tags:["HUB","SPACES","EMBED"],spaces:["https://huggingface.co/spaces/gradio/en2es"],url:"/guides/using-hugging-face-integrations/",contributor:"\u003Ca href=\"https://huggingface.co/osanseviero\">Omar Sanseviero\u003C/a> ü¶ô"},{name:"Gradio-and-Comet",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:48,pretty_name:"Gradio And Comet",content:"# Using Gradio and Comet\n\n\n\n\n## Introduction\n\nIn this guide we will demonstrate some of the ways you can use Gradio with Comet. We will cover the basics of using Comet with Gradio and show you some of the ways that you can leverage Gradio's advanced features such as [Embedding with iFrames](https://www.gradio.app/guides/sharing-your-app/#embedding-with-iframes) and [State](https://www.gradio.app/docs/#state) to build some amazing model evaluation workflows.\n\nHere is a list of the topics covered in this guide.\n\n1. Logging Gradio UI's to your Comet Experiments\n2. Embedding Gradio Applications directly into your Comet Projects\n3. Embedding Hugging Face Spaces directly into your Comet Projects\n4. Logging Model Inferences from your Gradio Application to Comet\n\n## What is Comet?\n\n[Comet](https://www.comet.com?utm_source=gradio&utm_medium=referral&utm_campaign=gradio-integration&utm_content=gradio-docs) is an MLOps Platform that is designed to help Data Scientists and Teams build better models faster! Comet provides tooling to Track, Explain, Manage, and Monitor your models in a single place! It works with Jupyter Notebooks and Scripts and most importantly it's 100% free!\n\n## Setup\n\nFirst, install the dependencies needed to run these examples\n\n```shell\npip install comet_ml torch torchvision transformers gradio shap requests Pillow\n```\n\nNext, you will need to [sign up for a Comet Account](https://www.comet.com/signup?utm_source=gradio&utm_medium=referral&utm_campaign=gradio-integration&utm_content=gradio-docs). Once you have your account set up, [grab your API Key](https://www.comet.com/docs/v2/guides/getting-started/quickstart/#get-an-api-key?utm_source=gradio&utm_medium=referral&utm_campaign=gradio-integration&utm_content=gradio-docs) and configure your Comet credentials\n\nIf you're running these examples as a script, you can either export your credentials as environment variables\n\n```shell\nexport COMET_API_KEY=\"\u003CYour API Key>\"\nexport COMET_WORKSPACE=\"\u003CYour Workspace Name>\"\nexport COMET_PROJECT_NAME=\"\u003CYour Project Name>\"\n```\n\nor set them in a `.comet.config` file in your working directory. You file should be formatted in the following way.\n\n```shell\n[comet]\napi_key=\u003CYour API Key>\nworkspace=\u003CYour Workspace Name>\nproject_name=\u003CYour Project Name>\n```\n\nIf you are using the provided Colab Notebooks to run these examples, please run the cell with the following snippet before starting the Gradio UI. Running this cell allows you to interactively add your API key to the notebook.\n\n```python\nimport comet_ml\ncomet_ml.init()\n```\n\n## 1. Logging Gradio UI's to your Comet Experiments\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/comet-examples/blob/master/integrations/model-evaluation/gradio/notebooks/Gradio_and_Comet.ipynb)\n\nIn this example, we will go over how to log your Gradio Applications to Comet and interact with them using the Gradio Custom Panel.\n\nLet's start by building a simple Image Classification example using `resnet18`.\n\n```python\nimport comet_ml\n\nimport requests\nimport torch\nfrom PIL import Image\nfrom torchvision import transforms\n\ntorch.hub.download_url_to_file(\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n\nif torch.cuda.is_available():\n    device = \"cuda\"\nelse:\n    device = \"cpu\"\n\nmodel = torch.hub.load(\"pytorch/vision:v0.6.0\", \"resnet18\", pretrained=True).eval()\nmodel = model.to(device)\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\n\ndef predict(inp):\n    inp = Image.fromarray(inp.astype(\"uint8\"), \"RGB\")\n    inp = transforms.ToTensor()(inp).unsqueeze(0)\n    with torch.no_grad():\n        prediction = torch.nn.functional.softmax(model(inp.to(device))[0], dim=0)\n    return {labels[i]: float(prediction[i]) for i in range(1000)}\n\n\ninputs = gr.Image()\noutputs = gr.Label(num_top_classes=3)\n\nio = gr.Interface(\n    fn=predict, inputs=inputs, outputs=outputs, examples=[\"dog.jpg\"]\n)\nio.launch(inline=False, share=True)\n\nexperiment = comet_ml.Experiment()\nexperiment.add_tag(\"image-classifier\")\n\nio.integrate(comet_ml=experiment)\n```\n\nThe last line in this snippet will log the URL of the Gradio Application to your Comet Experiment. You can find the URL in the Text Tab of your Experiment.\n\n\u003Cvideo width=\"560\" height=\"315\" controls>\n    \u003Csource src=\"https://user-images.githubusercontent.com/7529846/214328034-09369d4d-8b94-4c4a-aa3c-25e3ed8394c4.mp4\">\u003C/source>\n\u003C/video>\n\nAdd the Gradio Panel to your Experiment to interact with your application.\n\n\u003Cvideo width=\"560\" height=\"315\" controls>\n    \u003Csource src=\"https://user-images.githubusercontent.com/7529846/214328194-95987f83-c180-4929-9bed-c8a0d3563ed7.mp4\">\u003C/source>\n\u003C/video>\n\n## 2. Embedding Gradio Applications directly into your Comet Projects\n\n\u003Ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KZnpH7msPq0?start=9\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen>\u003C/iframe>\n\nIf you are permanently hosting your Gradio application, you can embed the UI using the Gradio Panel Extended custom Panel.\n\nGo to your Comet Project page, and head over to the Panels tab. Click the `+ Add` button to bring up the Panels search page.\n\n\u003Cimg width=\"560\" alt=\"adding-panels\" src=\"https://user-images.githubusercontent.com/7529846/214329314-70a3ff3d-27fb-408c-a4d1-4b58892a3854.jpeg\">\n\nNext, search for Gradio Panel Extended in the Public Panels section and click `Add`.\n\n\u003Cimg width=\"560\" alt=\"gradio-panel-extended\" src=\"https://user-images.githubusercontent.com/7529846/214325577-43226119-0292-46be-a62a-0c7a80646ebb.png\">\n\nOnce you have added your Panel, click `Edit` to access to the Panel Options page and paste in the URL of your Gradio application.\n\n![Edit-Gradio-Panel-Options](https://user-images.githubusercontent.com/7529846/214573001-23814b5a-ca65-4ace-a8a5-b27cdda70f7a.gif)\n\n\u003Cimg width=\"560\" alt=\"Edit-Gradio-Panel-URL\" src=\"https://user-images.githubusercontent.com/7529846/214334843-870fe726-0aa1-4b21-bbc6-0c48f56c48d8.png\">\n\n## 3. Embedding Hugging Face Spaces directly into your Comet Projects\n\n\u003Ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KZnpH7msPq0?start=107\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen>\u003C/iframe>\n\nYou can also embed Gradio Applications that are hosted on Hugging Faces Spaces into your Comet Projects using the Hugging Face Spaces Panel.\n\nGo to your Comet Project page, and head over to the Panels tab. Click the `+ Add` button to bring up the Panels search page. Next, search for the Hugging Face Spaces Panel in the Public Panels section and click `Add`.\n\n\u003Cimg width=\"560\" height=\"315\" alt=\"huggingface-spaces-panel\" src=\"https://user-images.githubusercontent.com/7529846/214325606-99aa3af3-b284-4026-b423-d3d238797e12.png\">\n\nOnce you have added your Panel, click Edit to access to the Panel Options page and paste in the path of your Hugging Face Space e.g. `pytorch/ResNet`\n\n\u003Cimg width=\"560\" height=\"315\" alt=\"Edit-HF-Space\" src=\"https://user-images.githubusercontent.com/7529846/214335868-c6f25dee-13db-4388-bcf5-65194f850b02.png\">\n\n## 4. Logging Model Inferences to Comet\n\n\u003Ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KZnpH7msPq0?start=176\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen>\u003C/iframe>\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/comet-examples/blob/master/integrations/model-evaluation/gradio/notebooks/Logging_Model_Inferences_with_Comet_and_Gradio.ipynb)\n\nIn the previous examples, we demonstrated the various ways in which you can interact with a Gradio application through the Comet UI. Additionally, you can also log model inferences, such as SHAP plots, from your Gradio application to Comet.\n\nIn the following snippet, we're going to log inferences from a Text Generation model. We can persist an Experiment across multiple inference calls using Gradio's [State](https://www.gradio.app/docs/#state) object. This will allow you to log multiple inferences from a model to a single Experiment.\n\n```python\nimport comet_ml\nimport gradio as gr\nimport shap\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nif torch.cuda.is_available():\n    device = \"cuda\"\nelse:\n    device = \"cpu\"\n\nMODEL_NAME = \"gpt2\"\n\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n\n# set model decoder to true\nmodel.config.is_decoder = True\n# set text-generation params under task_specific_params\nmodel.config.task_specific_params[\"text-generation\"] = {\n    \"do_sample\": True,\n    \"max_length\": 50,\n    \"temperature\": 0.7,\n    \"top_k\": 50,\n    \"no_repeat_ngram_size\": 2,\n}\nmodel = model.to(device)\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nexplainer = shap.Explainer(model, tokenizer)\n\n\ndef start_experiment():\n    \"\"\"Returns an APIExperiment object that is thread safe\n    and can be used to log inferences to a single Experiment\n    \"\"\"\n    try:\n        api = comet_ml.API()\n        workspace = api.get_default_workspace()\n        project_name = comet_ml.config.get_config()[\"comet.project_name\"]\n\n        experiment = comet_ml.APIExperiment(\n            workspace=workspace, project_name=project_name\n        )\n        experiment.log_other(\"Created from\", \"gradio-inference\")\n\n        message = f\"Started Experiment: [{experiment.name}]({experiment.url})\"\n\n        return (experiment, message)\n\n    except Exception as e:\n        return None, None\n\n\ndef predict(text, state, message):\n    experiment = state\n\n    shap_values = explainer([text])\n    plot = shap.plots.text(shap_values, display=False)\n\n    if experiment is not None:\n        experiment.log_other(\"message\", message)\n        experiment.log_html(plot)\n\n    return plot\n\n\nwith gr.Blocks() as demo:\n    start_experiment_btn = gr.Button(\"Start New Experiment\")\n    experiment_status = gr.Markdown()\n\n    # Log a message to the Experiment to provide more context\n    experiment_message = gr.Textbox(label=\"Experiment Message\")\n    experiment = gr.State()\n\n    input_text = gr.Textbox(label=\"Input Text\", lines=5, interactive=True)\n    submit_btn = gr.Button(\"Submit\")\n\n    output = gr.HTML(interactive=True)\n\n    start_experiment_btn.click(\n        start_experiment, outputs=[experiment, experiment_status]\n    )\n    submit_btn.click(\n        predict, inputs=[input_text, experiment, experiment_message], outputs=[output]\n    )\n```\n\nInferences from this snippet will be saved in the HTML tab of your experiment.\n\n\u003Cvideo width=\"560\" height=\"315\" controls>\n    \u003Csource src=\"https://user-images.githubusercontent.com/7529846/214328610-466e5c81-4814-49b9-887c-065aca14dd30.mp4\">\u003C/source>\n\u003C/video>\n\n## Conclusion\n\nWe hope you found this guide useful and that it provides some inspiration to help you build awesome model evaluation workflows with Comet and Gradio.\n\n## How to contribute Gradio demos on HF spaces on the Comet organization\n\n- Create an account on Hugging Face [here](https://huggingface.co/join).\n- Add Gradio Demo under your username, see this [course](https://huggingface.co/course/chapter9/4?fw=pt) for setting up Gradio Demo on Hugging Face.\n- Request to join the Comet organization [here](https://huggingface.co/Comet).\n\n## Additional Resources\n\n- [Comet Documentation](https://www.comet.com/docs/v2/?utm_source=gradio&utm_medium=referral&utm_campaign=gradio-integration&utm_content=gradio-docs)\n",tags:["COMET","SPACES"],spaces:[],url:"/guides/Gradio-and-Comet/",contributor:"the Comet team"},{name:"Gradio-and-ONNX-on-Hugging-Face",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:49,pretty_name:"Gradio And ONNX On Hugging Face",content:"# Gradio and ONNX on Hugging Face\n\n\n\n\n\n## Introduction\n\nIn this Guide, we'll walk you through:\n\n- Introduction of ONNX, ONNX model zoo, Gradio, and Hugging Face Spaces\n- How to setup a Gradio demo for EfficientNet-Lite4\n- How to contribute your own Gradio demos for the ONNX organization on Hugging Face\n\nHere's an [example](https://onnx-efficientnet-lite4.hf.space/) of an ONNX model.\n\n## What is the ONNX Model Zoo?\n\nOpen Neural Network Exchange ([ONNX](https://onnx.ai/)) is an open standard format for representing machine learning models. ONNX is supported by a community of partners who have implemented it in many frameworks and tools. For example, if you have trained a model in TensorFlow or PyTorch, you can convert it to ONNX easily, and from there run it on a variety of devices using an engine/compiler like ONNX Runtime.\n\nThe [ONNX Model Zoo](https://github.com/onnx/models) is a collection of pre-trained, state-of-the-art models in the ONNX format contributed by community members. Accompanying each model are Jupyter notebooks for model training and running inference with the trained model. The notebooks are written in Python and include links to the training dataset as well as references to the original paper that describes the model architecture.\n\n## What are Hugging Face Spaces & Gradio?\n\n### Gradio\n\nGradio lets users demo their machine learning models as a web app all in python code. Gradio wraps a python function into a user interface and the demos can be launched inside jupyter notebooks, colab notebooks, as well as embedded in your own website and hosted on Hugging Face Spaces for free.\n\nGet started [here](https://gradio.app/getting_started)\n\n### Hugging Face Spaces\n\nHugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces [here](https://huggingface.co/spaces/launch).\n\n### Hugging Face Models\n\nHugging Face Model Hub also supports ONNX models and ONNX models can be filtered through the [ONNX tag](https://huggingface.co/models?library=onnx&sort=downloads)\n\n## How did Hugging Face help the ONNX Model Zoo?\n\nThere are a lot of Jupyter notebooks in the ONNX Model Zoo for users to test models. Previously, users needed to download the models themselves and run those notebooks locally for testing. With Hugging Face, the testing process can be much simpler and more user-friendly. Users can easily try certain ONNX Model Zoo model on Hugging Face Spaces and run a quick demo powered by Gradio with ONNX Runtime, all on cloud without downloading anything locally. Note, there are various runtimes for ONNX, e.g., [ONNX Runtime](https://github.com/microsoft/onnxruntime), [MXNet](https://github.com/apache/incubator-mxnet).\n\n## What is the role of ONNX Runtime?\n\nONNX Runtime is a cross-platform inference and training machine-learning accelerator. It makes live Gradio demos with ONNX Model Zoo model on Hugging Face possible.\n\nONNX Runtime inference can enable faster customer experiences and lower costs, supporting models from deep learning frameworks such as PyTorch and TensorFlow/Keras as well as classical machine learning libraries such as scikit-learn, LightGBM, XGBoost, etc. ONNX Runtime is compatible with different hardware, drivers, and operating systems, and provides optimal performance by leveraging hardware accelerators where applicable alongside graph optimizations and transforms. For more information please see the [official website](https://onnxruntime.ai/).\n\n## Setting up a Gradio Demo for EfficientNet-Lite4\n\nEfficientNet-Lite 4 is the largest variant and most accurate of the set of EfficientNet-Lite models. It is an integer-only quantized model that produces the highest accuracy of all of the EfficientNet models. It achieves 80.4% ImageNet top-1 accuracy, while still running in real-time (e.g. 30ms/image) on a Pixel 4 CPU. To learn more read the [model card](https://github.com/onnx/models/tree/main/vision/classification/efficientnet-lite4)\n\nHere we walk through setting up a example demo for EfficientNet-Lite4 using Gradio\n\nFirst we import our dependencies and download and load the efficientnet-lite4 model from the onnx model zoo. Then load the labels from the labels_map.txt file. We then setup our preprocessing functions, load the model for inference, and setup the inference function. Finally, the inference function is wrapped into a gradio interface for a user to interact with. See the full code below.\n\n```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport cv2\nimport json\nimport gradio as gr\nfrom huggingface_hub import hf_hub_download\nfrom onnx import hub\nimport onnxruntime as ort\n\n# loads ONNX model from ONNX Model Zoo\nmodel = hub.load(\"efficientnet-lite4\")\n# loads the labels text file\nlabels = json.load(open(\"labels_map.txt\", \"r\"))\n\n# sets image file dimensions to 224x224 by resizing and cropping image from center\ndef pre_process_edgetpu(img, dims):\n    output_height, output_width, _ = dims\n    img = resize_with_aspectratio(img, output_height, output_width, inter_pol=cv2.INTER_LINEAR)\n    img = center_crop(img, output_height, output_width)\n    img = np.asarray(img, dtype='float32')\n    # converts jpg pixel value from [0 - 255] to float array [-1.0 - 1.0]\n    img -= [127.0, 127.0, 127.0]\n    img /= [128.0, 128.0, 128.0]\n    return img\n\n# resizes the image with a proportional scale\ndef resize_with_aspectratio(img, out_height, out_width, scale=87.5, inter_pol=cv2.INTER_LINEAR):\n    height, width, _ = img.shape\n    new_height = int(100. * out_height / scale)\n    new_width = int(100. * out_width / scale)\n    if height > width:\n        w = new_width\n        h = int(new_height * height / width)\n    else:\n        h = new_height\n        w = int(new_width * width / height)\n    img = cv2.resize(img, (w, h), interpolation=inter_pol)\n    return img\n\n# crops the image around the center based on given height and width\ndef center_crop(img, out_height, out_width):\n    height, width, _ = img.shape\n    left = int((width - out_width) / 2)\n    right = int((width + out_width) / 2)\n    top = int((height - out_height) / 2)\n    bottom = int((height + out_height) / 2)\n    img = img[top:bottom, left:right]\n    return img\n\n\nsess = ort.InferenceSession(model)\n\ndef inference(img):\n  img = cv2.imread(img)\n  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n  img = pre_process_edgetpu(img, (224, 224, 3))\n\n  img_batch = np.expand_dims(img, axis=0)\n\n  results = sess.run([\"Softmax:0\"], {\"images:0\": img_batch})[0]\n  result = reversed(results[0].argsort()[-5:])\n  resultdic = {}\n  for r in result:\n      resultdic[labels[str(r)]] = float(results[0][r])\n  return resultdic\n\ntitle = \"EfficientNet-Lite4\"\ndescription = \"EfficientNet-Lite 4 is the largest variant and most accurate of the set of EfficientNet-Lite model. It is an integer-only quantized model that produces the highest accuracy of all of the EfficientNet models. It achieves 80.4% ImageNet top-1 accuracy, while still running in real-time (e.g. 30ms/image) on a Pixel 4 CPU.\"\nexamples = [['catonnx.jpg']]\ngr.Interface(inference, gr.Image(type=\"filepath\"), \"label\", title=title, description=description, examples=examples).launch()\n```\n\n## How to contribute Gradio demos on HF spaces using ONNX models\n\n- Add model to the [onnx model zoo](https://github.com/onnx/models/blob/main/.github/PULL_REQUEST_TEMPLATE.md)\n- Create an account on Hugging Face [here](https://huggingface.co/join).\n- See list of models left to add to ONNX organization, please refer to the table with the [Models list](https://github.com/onnx/models#models)\n- Add Gradio Demo under your username, see this [blog post](https://huggingface.co/blog/gradio-spaces) for setting up Gradio Demo on Hugging Face.\n- Request to join ONNX Organization [here](https://huggingface.co/onnx).\n- Once approved transfer model from your username to ONNX organization\n- Add a badge for model in model table, see examples in [Models list](https://github.com/onnx/models#models)\n",tags:["ONNX","SPACES"],spaces:["https://huggingface.co/spaces/onnx/EfficientNet-Lite4"],url:"/guides/Gradio-and-ONNX-on-Hugging-Face/",contributor:"Gradio and the \u003Ca href=\"https://onnx.ai/\">ONNX\u003C/a> team"},{name:"Gradio-and-Wandb-Integration",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:50,pretty_name:"Gradio And Wandb Integration",content:"# Gradio and W&B Integration\n\n\n\n\n\n## Introduction\n\nIn this Guide, we'll walk you through:\n\n- Introduction of Gradio, and Hugging Face Spaces, and Wandb\n- How to setup a Gradio demo using the Wandb integration for JoJoGAN\n- How to contribute your own Gradio demos after tracking your experiments on wandb to the Wandb organization on Hugging Face\n\n\n## What is Wandb?\n\nWeights and Biases (W&B) allows data scientists and machine learning scientists to track their machine learning experiments at every stage, from training to production. Any metric can be aggregated over samples and shown in panels in a customizable and searchable dashboard, like below:\n\n\u003Cimg alt=\"Screen Shot 2022-08-01 at 5 54 59 PM\" src=\"https://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png\">\n\n## What are Hugging Face Spaces & Gradio?\n\n### Gradio\n\nGradio lets users demo their machine learning models as a web app, all in a few lines of Python. Gradio wraps any Python function (such as a machine learning model's inference function) into a user interface and the demos can be launched inside jupyter notebooks, colab notebooks, as well as embedded in your own website and hosted on Hugging Face Spaces for free.\n\nGet started [here](https://gradio.app/getting_started)\n\n### Hugging Face Spaces\n\nHugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces [here](https://huggingface.co/spaces/launch).\n\n## Setting up a Gradio Demo for JoJoGAN\n\nNow, let's walk you through how to do this on your own. We'll make the assumption that you're new to W&B and Gradio for the purposes of this tutorial.\n\nLet's get started!\n\n1. Create a W&B account\n\n   Follow [these quick instructions](https://app.wandb.ai/login) to create your free account if you don‚Äôt have one already. It shouldn't take more than a couple minutes. Once you're done (or if you've already got an account), next, we'll run a quick colab.\n\n2. Open Colab Install Gradio and W&B\n\n   We'll be following along with the colab provided in the JoJoGAN repo with some minor modifications to use Wandb and Gradio more effectively.\n\n   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mchong6/JoJoGAN/blob/main/stylize.ipynb)\n\n   Install Gradio and Wandb at the top:\n\n```sh\n\npip install gradio wandb\n```\n\n3. Finetune StyleGAN and W&B experiment tracking\n\n   This next step will open a W&B dashboard to track your experiments and a gradio panel showing pretrained models to choose from a drop down menu from a Gradio Demo hosted on Huggingface Spaces. Here's the code you need for that:\n\n   ```python\n\n   alpha =  1.0\n   alpha = 1-alpha\n\n   preserve_color = True\n   num_iter = 100\n   log_interval = 50\n\n\n   samples = []\n   column_names = [\"Reference (y)\", \"Style Code(w)\", \"Real Face Image(x)\"]\n\n   wandb.init(project=\"JoJoGAN\")\n   config = wandb.config\n   config.num_iter = num_iter\n   config.preserve_color = preserve_color\n   wandb.log(\n   {\"Style reference\": [wandb.Image(transforms.ToPILImage()(target_im))]},\n   step=0)\n\n   # load discriminator for perceptual loss\n   discriminator = Discriminator(1024, 2).eval().to(device)\n   ckpt = torch.load('models/stylegan2-ffhq-config-f.pt', map_location=lambda storage, loc: storage)\n   discriminator.load_state_dict(ckpt[\"d\"], strict=False)\n\n   # reset generator\n   del generator\n   generator = deepcopy(original_generator)\n\n   g_optim = optim.Adam(generator.parameters(), lr=2e-3, betas=(0, 0.99))\n\n   # Which layers to swap for generating a family of plausible real images -> fake image\n   if preserve_color:\n       id_swap = [9,11,15,16,17]\n   else:\n       id_swap = list(range(7, generator.n_latent))\n\n   for idx in tqdm(range(num_iter)):\n       mean_w = generator.get_latent(torch.randn([latents.size(0), latent_dim]).to(device)).unsqueeze(1).repeat(1, generator.n_latent, 1)\n       in_latent = latents.clone()\n       in_latent[:, id_swap] = alpha*latents[:, id_swap] + (1-alpha)*mean_w[:, id_swap]\n\n       img = generator(in_latent, input_is_latent=True)\n\n       with torch.no_grad():\n           real_feat = discriminator(targets)\n       fake_feat = discriminator(img)\n\n       loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)\n\n\n       wandb.log({\"loss\": loss}, step=idx)\n       if idx % log_interval == 0:\n           generator.eval()\n           my_sample = generator(my_w, input_is_latent=True)\n           generator.train()\n           my_sample = transforms.ToPILImage()(utils.make_grid(my_sample, normalize=True, range=(-1, 1)))\n           wandb.log(\n           {\"Current stylization\": [wandb.Image(my_sample)]},\n           step=idx)\n       table_data = [\n               wandb.Image(transforms.ToPILImage()(target_im)),\n               wandb.Image(img),\n               wandb.Image(my_sample),\n           ]\n       samples.append(table_data)\n\n       g_optim.zero_grad()\n       loss.backward()\n       g_optim.step()\n\n   out_table = wandb.Table(data=samples, columns=column_names)\n   wandb.log({\"Current Samples\": out_table})\n   ```\n\nalpha = 1.0\nalpha = 1-alpha\n\npreserve_color = True\nnum_iter = 100\nlog_interval = 50\n\nsamples = []\ncolumn_names = [\"Referece (y)\", \"Style Code(w)\", \"Real Face Image(x)\"]\n\nwandb.init(project=\"JoJoGAN\")\nconfig = wandb.config\nconfig.num_iter = num_iter\nconfig.preserve_color = preserve_color\nwandb.log(\n{\"Style reference\": [wandb.Image(transforms.ToPILImage()(target_im))]},\nstep=0)\n\n# load discriminator for perceptual loss\n\ndiscriminator = Discriminator(1024, 2).eval().to(device)\nckpt = torch.load('models/stylegan2-ffhq-config-f.pt', map_location=lambda storage, loc: storage)\ndiscriminator.load_state_dict(ckpt[\"d\"], strict=False)\n\n# reset generator\n\ndel generator\ngenerator = deepcopy(original_generator)\n\ng_optim = optim.Adam(generator.parameters(), lr=2e-3, betas=(0, 0.99))\n\n# Which layers to swap for generating a family of plausible real images -> fake image\n\nif preserve_color:\nid_swap = [9,11,15,16,17]\nelse:\nid_swap = list(range(7, generator.n_latent))\n\nfor idx in tqdm(range(num_iter)):\nmean_w = generator.get_latent(torch.randn([latents.size(0), latent_dim]).to(device)).unsqueeze(1).repeat(1, generator.n_latent, 1)\nin_latent = latents.clone()\nin_latent[:, id_swap] = alpha*latents[:, id_swap] + (1-alpha)*mean_w[:, id_swap]\n\n    img = generator(in_latent, input_is_latent=True)\n\n    with torch.no_grad():\n        real_feat = discriminator(targets)\n    fake_feat = discriminator(img)\n\n    loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)\n\n\n    wandb.log({\"loss\": loss}, step=idx)\n    if idx % log_interval == 0:\n        generator.eval()\n        my_sample = generator(my_w, input_is_latent=True)\n        generator.train()\n        my_sample = transforms.ToPILImage()(utils.make_grid(my_sample, normalize=True, range=(-1, 1)))\n        wandb.log(\n        {\"Current stylization\": [wandb.Image(my_sample)]},\n        step=idx)\n    table_data = [\n            wandb.Image(transforms.ToPILImage()(target_im)),\n            wandb.Image(img),\n            wandb.Image(my_sample),\n        ]\n    samples.append(table_data)\n\n    g_optim.zero_grad()\n    loss.backward()\n    g_optim.step()\n\nout_table = wandb.Table(data=samples, columns=column_names)\nwandb.log({\"Current Samples\": out_table})\n\n````\n\n4. Save, Download, and Load Model\n\n    Here's how to save and download your model.\n\n```python\n\nfrom PIL import Image\nimport torch\ntorch.backends.cudnn.benchmark = True\nfrom torchvision import transforms, utils\nfrom util import *\nimport math\nimport random\nimport numpy as np\nfrom torch import nn, autograd, optim\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\nimport lpips\nfrom model import *\nfrom e4e_projection import projection as e4e_projection\n\nfrom copy import deepcopy\nimport imageio\n\nimport os\nimport sys\nimport torchvision.transforms as transforms\nfrom argparse import Namespace\nfrom e4e.models.psp import pSp\nfrom util import *\nfrom huggingface_hub import hf_hub_download\nfrom google.colab import files\n\ntorch.save({\"g\": generator.state_dict()}, \"your-model-name.pt\")\n\nfiles.download('your-model-name.pt')\n\nlatent_dim = 512\ndevice=\"cuda\"\nmodel_path_s = hf_hub_download(repo_id=\"akhaliq/jojogan-stylegan2-ffhq-config-f\", filename=\"stylegan2-ffhq-config-f.pt\")\noriginal_generator = Generator(1024, latent_dim, 8, 2).to(device)\nckpt = torch.load(model_path_s, map_location=lambda storage, loc: storage)\noriginal_generator.load_state_dict(ckpt[\"g_ema\"], strict=False)\nmean_latent = original_generator.mean_latent(10000)\n\ngenerator = deepcopy(original_generator)\n\nckpt = torch.load(\"/content/JoJoGAN/your-model-name.pt\", map_location=lambda storage, loc: storage)\ngenerator.load_state_dict(ckpt[\"g\"], strict=False)\ngenerator.eval()\n\nplt.rcParams['figure.dpi'] = 150\n\n\n\ntransform = transforms.Compose(\n    [\n        transforms.Resize((1024, 1024)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ]\n)\n\n\ndef inference(img):\n    img.save('out.jpg')\n    aligned_face = align_face('out.jpg')\n\n    my_w = e4e_projection(aligned_face, \"out.pt\", device).unsqueeze(0)\n    with torch.no_grad():\n        my_sample = generator(my_w, input_is_latent=True)\n\n\n    npimage = my_sample[0].cpu().permute(1, 2, 0).detach().numpy()\n    imageio.imwrite('filename.jpeg', npimage)\n    return 'filename.jpeg'\n````\n\n5. Build a Gradio Demo\n\n```python\n\nimport gradio as gr\n\ntitle = \"JoJoGAN\"\ndescription = \"Gradio Demo for JoJoGAN: One Shot Face Stylization. To use it, simply upload your image, or click one of the examples to load them. Read more at the links below.\"\n\ndemo = gr.Interface(\n    inference,\n    gr.Image(type=\"pil\"),\n    gr.Image(type=\"file\"),\n    title=title,\n    description=description\n)\n\ndemo.launch(share=True)\n```\n\n6. Integrate Gradio into your W&B Dashboard\n\n   The last step‚Äîintegrating your Gradio demo with your W&B dashboard‚Äîis just one extra line:\n\n```python\n\ndemo.integrate(wandb=wandb)\n```\n\n    Once you call integrate, a demo will be created and you can integrate it into your dashboard or report\n\n    Outside of W&B with Web components, using the gradio-app tags allows anyone can embed Gradio demos on HF spaces directly into their blogs, websites, documentation, etc.:\n\n```html\n\u003Cgradio-app space=\"akhaliq/JoJoGAN\"> \u003C/gradio-app>\n```\n\n7. (Optional) Embed W&B plots in your Gradio App\n\n   It's also possible to embed W&B plots within Gradio apps. To do so, you can create a W&B Report of your plots and\n   embed them within your Gradio app within a `gr.HTML` block.\n\n   The Report will need to be public and you will need to wrap the URL within an iFrame like this:\n\n```python\n\nimport gradio as gr\n\ndef wandb_report(url):\n    iframe = f'\u003Ciframe src={url} style=\"border:none;height:1024px;width:100%\">'\n    return gr.HTML(iframe)\n\nwith gr.Blocks() as demo:\n    report_url = 'https://wandb.ai/_scott/pytorch-sweeps-demo/reports/loss-22-10-07-16-00-17---VmlldzoyNzU2NzAx'\n    report = wandb_report(report_url)\n\ndemo.launch(share=True)\n```\n\n## Conclusion\n\nWe hope you enjoyed this brief demo of embedding a Gradio demo to a W&B report! Thanks for making it to the end. To recap:\n\n- Only one single reference image is needed for fine-tuning JoJoGAN which usually takes about 1 minute on a GPU in colab. After training, style can be applied to any input image. Read more in the paper.\n\n- W&B tracks experiments with just a few lines of code added to a colab and you can visualize, sort, and understand your experiments in a single, centralized dashboard.\n\n- Gradio, meanwhile, demos the model in a user friendly interface to share anywhere on the web.\n\n## How to contribute Gradio demos on HF spaces on the Wandb organization\n\n- Create an account on Hugging Face [here](https://huggingface.co/join).\n- Add Gradio Demo under your username, see this [course](https://huggingface.co/course/chapter9/4?fw=pt) for setting up Gradio Demo on Hugging Face.\n- Request to join wandb organization [here](https://huggingface.co/wandb).\n- Once approved transfer model from your username to Wandb organization\n",tags:["WANDB","SPACES"],spaces:["https://huggingface.co/spaces/akhaliq/JoJoGAN"],url:"/guides/Gradio-and-Wandb-Integration/",contributor:"Gradio team"},{name:"create-your-own-friends-with-a-gan",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:51,pretty_name:"Create Your Own Friends With A Gan",content:"# Create Your Own Friends with a GAN\n\n\n\n\n\n\n## Introduction\n\nIt seems that cryptocurrencies, [NFTs](https://www.nytimes.com/interactive/2022/03/18/technology/nft-guide.html), and the web3 movement are all the rage these days! Digital assets are being listed on marketplaces for astounding amounts of money, and just about every celebrity is debuting their own NFT collection. While your crypto assets [may be taxable, such as in Canada](https://www.canada.ca/en/revenue-agency/programs/about-canada-revenue-agency-cra/compliance/digital-currency/cryptocurrency-guide.html), today we'll explore some fun and tax-free ways to generate your own assortment of procedurally generated [CryptoPunks](https://www.larvalabs.com/cryptopunks).\n\nGenerative Adversarial Networks, often known just as _GANs_, are a specific class of deep-learning models that are designed to learn from an input dataset to create (_generate!_) new material that is convincingly similar to elements of the original training set. Famously, the website [thispersondoesnotexist.com](https://thispersondoesnotexist.com/) went viral with lifelike, yet synthetic, images of people generated with a model called StyleGAN2. GANs have gained traction in the machine learning world, and are now being used to generate all sorts of images, text, and even [music](https://salu133445.github.io/musegan/)!\n\nToday we'll briefly look at the high-level intuition behind GANs, and then we'll build a small demo around a pre-trained GAN to see what all the fuss is about. Here's a [peek](https://nimaboscarino-cryptopunks.hf.space) at what we're going to be putting together.\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). To use the pretrained model, also install `torch` and `torchvision`.\n\n## GANs: a very brief introduction\n\nOriginally proposed in [Goodfellow et al. 2014](https://arxiv.org/abs/1406.2661), GANs are made up of neural networks which compete with the intention of outsmarting each other. One network, known as the _generator_, is responsible for generating images. The other network, the _discriminator_, receives an image at a time from the generator along with a **real** image from the training data set. The discriminator then has to guess: which image is the fake?\n\nThe generator is constantly training to create images which are trickier for the discriminator to identify, while the discriminator raises the bar for the generator every time it correctly detects a fake. As the networks engage in this competitive (_adversarial!_) relationship, the images that get generated improve to the point where they become indistinguishable to human eyes!\n\nFor a more in-depth look at GANs, you can take a look at [this excellent post on Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/06/a-detailed-explanation-of-gan-with-implementation-using-tensorflow-and-keras/) or this [PyTorch tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html). For now, though, we'll dive into a demo!\n\n## Step 1 ‚Äî Create the Generator model\n\nTo generate new images with a GAN, you only need the generator model. There are many different architectures that the generator could use, but for this demo we'll use a pretrained GAN generator model with the following architecture:\n\n```python\nfrom torch import nn\n\nclass Generator(nn.Module):\n    # Refer to the link below for explanations about nc, nz, and ngf\n    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs\n    def __init__(self, nc=4, nz=100, ngf=64):\n        super(Generator, self).__init__()\n        self.network = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, input):\n        output = self.network(input)\n        return output\n```\n\nWe're taking the generator from [this repo by @teddykoker](https://github.com/teddykoker/cryptopunks-gan/blob/main/train.py#L90), where you can also see the original discriminator model structure.\n\nAfter instantiating the model, we'll load in the weights from the Hugging Face Hub, stored at [nateraw/cryptopunks-gan](https://huggingface.co/nateraw/cryptopunks-gan):\n\n```python\nfrom huggingface_hub import hf_hub_download\nimport torch\n\nmodel = Generator()\nweights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')\nmodel.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available\n```\n\n## Step 2 ‚Äî Defining a `predict` function\n\nThe `predict` function is the key to making Gradio work! Whatever inputs we choose through the Gradio interface will get passed through our `predict` function, which should operate on the inputs and generate outputs that we can display with Gradio output components. For GANs it's common to pass random noise into our model as the input, so we'll generate a tensor of random numbers and pass that through the model. We can then use `torchvision`'s `save_image` function to save the output of the model as a `png` file, and return the file name:\n\n```python\nfrom torchvision.utils import save_image\n\ndef predict(seed):\n    num_punks = 4\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n```\n\nWe're giving our `predict` function a `seed` parameter, so that we can fix the random tensor generation with a seed. We'll then be able to reproduce punks if we want to see them again by passing in the same seed.\n\n_Note!_ Our model needs an input tensor of dimensions 100x1x1 to do a single inference, or (BatchSize)x100x1x1 for generating a batch of images. In this demo we'll start by generating 4 punks at a time.\n\n## Step 3 ‚Äî Creating a Gradio interface\n\nAt this point you can even run the code you have with `predict(\u003CSOME_NUMBER>)`, and you'll find your freshly generated punks in your file system at `./punks.png`. To make a truly interactive demo, though, we'll build out a simple interface with Gradio. Our goals here are to:\n\n- Set a slider input so users can choose the \"seed\" value\n- Use an image component for our output to showcase the generated punks\n- Use our `predict()` to take the seed and generate the images\n\nWith `gr.Interface()`, we can define all of that with a single function call:\n\n```python\nimport gradio as gr\n\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n    ],\n    outputs=\"image\",\n).launch()\n```\n\n\n## Step 4 ‚Äî Even more punks!\n\nGenerating 4 punks at a time is a good start, but maybe we'd like to control how many we want to make each time. Adding more inputs to our Gradio interface is as simple as adding another item to the `inputs` list that we pass to `gr.Interface`:\n\n```python\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n        gr.Slider(4, 64, label='Number of Punks', step=1, default=10), # Adding another slider!\n    ],\n    outputs=\"image\",\n).launch()\n```\n\nThe new input will be passed to our `predict()` function, so we have to make some changes to that function to accept a new parameter:\n\n```python\ndef predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n```\n\nWhen you relaunch your interface, you should see a second slider that'll let you control the number of punks!\n\n## Step 5 - Polishing it up\n\nYour Gradio app is pretty much good to go, but you can add a few extra things to really make it ready for the spotlight ‚ú®\n\nWe can add some examples that users can easily try out by adding this to the `gr.Interface`:\n\n```python\ngr.Interface(\n    # ...\n    # keep everything as it is, and then add\n    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],\n).launch(cache_examples=True) # cache_examples is optional\n```\n\nThe `examples` parameter takes a list of lists, where each item in the sublists is ordered in the same order that we've listed the `inputs`. So in our case, `[seed, num_punks]`. Give it a try!\n\nYou can also try adding a `title`, `description`, and `article` to the `gr.Interface`. Each of those parameters accepts a string, so try it out and see what happens üëÄ `article` will also accept HTML, as [explored in a previous guide](/guides/key-features/#descriptive-content)!\n\nWhen you're all done, you may end up with something like [this](https://nimaboscarino-cryptopunks.hf.space).\n\nFor reference, here is our full code:\n\n```python\nimport torch\nfrom torch import nn\nfrom huggingface_hub import hf_hub_download\nfrom torchvision.utils import save_image\nimport gradio as gr\n\nclass Generator(nn.Module):\n    # Refer to the link below for explanations about nc, nz, and ngf\n    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs\n    def __init__(self, nc=4, nz=100, ngf=64):\n        super(Generator, self).__init__()\n        self.network = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, input):\n        output = self.network(input)\n        return output\n\nmodel = Generator()\nweights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')\nmodel.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available\n\ndef predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n        gr.Slider(4, 64, label='Number of Punks', step=1, default=10),\n    ],\n    outputs=\"image\",\n    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],\n).launch(cache_examples=True)\n```\n\n---\n\nCongratulations! You've built out your very own GAN-powered CryptoPunks generator, with a fancy Gradio interface that makes it easy for anyone to use. Now you can [scour the Hub for more GANs](https://huggingface.co/models?other=gan) (or train your own) and continue making even more awesome demos ü§ó\n",tags:["GAN","IMAGE","HUB"],spaces:["https://huggingface.co/spaces/NimaBoscarino/cryptopunks","https://huggingface.co/spaces/nateraw/cryptopunks-generator"],url:"/guides/create-your-own-friends-with-a-gan/",contributor:"\u003Ca href=\"https://huggingface.co/NimaBoscarino\">Nima Boscarino\u003C/a> and \u003Ca href=\"https://huggingface.co/nateraw\">Nate Raw\u003C/a>"},{name:"deploying-gradio-with-docker",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:52,pretty_name:"Deploying Gradio With Docker",content:"# Deploying a Gradio app with Docker\n\n\n\n\n### Introduction\n\nGradio is a powerful and intuitive Python library designed for creating web apps that showcase machine learning models. These web apps can be run locally, or [deployed on Hugging Face Spaces ](https://huggingface.co/spaces)for free. Or, you can deploy them on your servers in Docker containers. Dockerizing Gradio apps offers several benefits:\n\n- **Consistency**: Docker ensures that your Gradio app runs the same way, irrespective of where it is deployed, by packaging the application and its environment together.\n- **Portability**: Containers can be easily moved across different systems or cloud environments.\n- **Scalability**: Docker works well with orchestration systems like Kubernetes, allowing your app to scale up or down based on demand.\n\n## How to Dockerize a Gradio App\n\nLet's go through a simple example to understand how to containerize a Gradio app using Docker.\n\n#### Step 1: Create Your Gradio App\n\nFirst, we need a simple Gradio app. Let's create a Python file named `app.py` with the following content:\n\n```python\nimport gradio as gr\n\ndef greet(name):\n    return f\"Hello {name}!\"\n\niface = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\").launch()\n```\n\nThis app creates a simple interface that greets the user by name.\n\n#### Step 2: Create a Dockerfile\n\nNext, we'll create a Dockerfile to specify how our app should be built and run in a Docker container. Create a file named `Dockerfile` in the same directory as your app with the following content:\n\n```dockerfile\nFROM python:3.8-slim\n\nWORKDIR /usr/src/app\nCOPY . .\nRUN pip install --no-cache-dir gradio\nEXPOSE 7860\nENV GRADIO_SERVER_NAME=\"0.0.0.0\"\n\nCMD [\"python\", \"app.py\"]\n```\n\nThis Dockerfile performs the following steps:\n- Starts from a Python 3.8 slim image.\n- Sets the working directory and copies the app into the container.\n- Installs Gradio (you should install all other requirements as well).\n- Exposes port 7860 (Gradio's default port).\n- Sets the `GRADIO_SERVER_NAME` environment variable to ensure Gradio listens on all network interfaces.\n- Specifies the command to run the app.\n\n#### Step 3: Build and Run Your Docker Container\n\nWith the Dockerfile in place, you can build and run your container:\n\n```bash\ndocker build -t gradio-app .\ndocker run -p 7860:7860 gradio-app\n```\n\nYour Gradio app should now be accessible at `http://localhost:7860`.\n\n## Important Considerations\n\nWhen running Gradio applications in Docker, there are a few important things to keep in mind:\n\n#### Running the Gradio app on `\"0.0.0.0\"` and exposing port 7860\n\nIn the Docker environment, setting `GRADIO_SERVER_NAME=\"0.0.0.0\"` as an environment variable (or directly in your Gradio app's `launch()` function) is crucial for allowing connections from outside the container. And the `EXPOSE 7860` directive in the Dockerfile tells Docker to expose Gradio's default port on the container to enable external access to the Gradio app. \n\n#### Enable Stickiness for Multiple Replicas\n\nWhen deploying Gradio apps with multiple replicas, such as on AWS ECS, it's important to enable stickiness with `sessionAffinity: ClientIP`. This ensures that all requests from the same user are routed to the same instance. This is important because Gradio's communication protocol requires multiple separate connections from the frontend to the backend in order for events to be processed correctly. (If you use Terraform, you'll want to add a [stickiness block](https://registry.terraform.io/providers/hashicorp/aws/3.14.1/docs/resources/lb_target_group#stickiness) into your target group definition.)\n\n#### Deploying Behind a Proxy\n\nIf you're deploying your Gradio app behind a proxy, like Nginx, it's essential to configure the proxy correctly. Gradio provides a [Guide that walks through the necessary steps](https://www.gradio.app/guides/running-gradio-on-your-web-server-with-nginx). This setup ensures your app is accessible and performs well in production environments.\n\n",tags:["DEPLOYMENT","DOCKER"],spaces:[],url:"/guides/deploying-gradio-with-docker/",contributor:null},{name:"developing-faster-with-reload-mode",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:53,pretty_name:"Developing Faster With Reload Mode",content:"# Developing Faster with Auto-Reloading\n\n**Prerequisite**: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/blocks-and-event-listeners).\n\nThis guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks.\n\n## Why Auto-Reloading?\n\nWhen you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes.\n\nTo make it faster and more convenient to write your code, we've made it easier to \"reload\" your Gradio apps instantly when you are developing in a **Python IDE** (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We've also developed an analogous \"magic command\" that allows you to re-run cells faster if you use **Jupyter Notebooks** (or any similar environment like Colab).\n\nThis short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to build Gradio apps faster.\n\n## Python IDE Reload üî•\n\nIf you are building Gradio Blocks using a Python IDE, your file of code (let's name it `run.py`) might look something like this:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# Greetings from Gradio!\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`.\n\nInstead of doing this, you can run your code in **reload mode** by changing 1 word: `python` to `gradio`:\n\nIn the terminal, run `gradio run.py`. That's it!\n\nNow, you'll see that after you'll see something like this:\n\n```bash\nWatching: '/Users/freddy/sources/gradio/gradio', '/Users/freddy/sources/gradio/demo/'\n\nRunning on local URL:  http://127.0.0.1:7860\n```\n\nThe important part here is the line that says `Watching...` What's happening here is that Gradio will be observing the directory where `run.py` file lives, and if the file changes, it will automatically rerun the file for you. So you can focus on writing your code, and your Gradio demo will refresh automatically ü•≥\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> the `gradio` command does not detect the parameters passed to the `launch()` methods because the `launch()` method is never called in reload mode. For example, setting `auth`, or `show_error` in `launch()` will not be reflected in the app.\u003C/p>\n\nThere is one important thing to keep in mind when using the reload mode: Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. So if your `run.py` file looked like this:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as my_demo:\n    gr.Markdown(\"# Greetings from Gradio!\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == \"__main__\":\n    my_demo.launch()\n```\n\nThen you would launch it in reload mode like this: `gradio run.py --demo-name=my_demo`.\n\nBy default, the Gradio use UTF-8 encoding for scripts. **For reload mode**, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:\n\n1. Configure encoding declaration of python script, for example: `# -*- coding: cp1252 -*-`\n2. Confirm that your code editor has identified that encoding format. \n3. Run like this: `gradio run.py --encoding cp1252`\n\nüî• If your application accepts command line arguments, you can pass them in as well. Here's an example:\n\n```python\nimport gradio as gr\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--name\", type=str, default=\"User\")\nargs, unknown = parser.parse_known_args()\n\nwith gr.Blocks() as demo:\n    gr.Markdown(f\"# Greetings {args.name}!\")\n    inp = gr.Textbox()\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: x, inputs=inp, outputs=out)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\nWhich you could run like this: `gradio run.py --name Gretel`\n\nAs a small aside, this auto-reloading happens if you change your `run.py` source code or the Gradio source code. Meaning that this can be useful if you decide to [contribute to Gradio itself](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md) ‚úÖ\n\n\n## Controlling the Reload üéõÔ∏è\n\nBy default, reload mode will re-run your entire script for every change you make.\nBut there are some cases where this is not desirable.\nFor example, loading a machine learning model should probably only happen once to save time. There are also some Python libraries that use C or Rust extensions that throw errors when they are reloaded, like `numpy` and `tiktoken`.\n\nIn these situations, you can place code that you do not want to be re-run inside an `if gr.NO_RELOAD:`  codeblock. Here's an example of how you can use it to only load a transformers model once during the development process.\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> The value of `gr.NO_RELOAD` is `True`. So you don't have to change your script when you are done developing and want to run it in production. Simply run the file with `python` instead of `gradio`.\u003C/p>\n\n```python\nimport gradio as gr\n\nif gr.NO_RELOAD:\n\tfrom transformers import pipeline\n\tpipe = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n\ndemo = gr.Interface(lambda s: pipe(s), gr.Textbox(), gr.Label())\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\n\n## Jupyter Notebook Magic üîÆ\n\nWhat about if you use Jupyter Notebooks (or Colab Notebooks, etc.) to develop code? We got something for you too!\n\nWe've developed a **magic command** that will create and run a Blocks demo for you. To use this, load the gradio extension at the top of your notebook:\n\n`%load_ext gradio`\n\nThen, in the cell that you are developing your Gradio demo, simply write the magic command **`%%blocks`** at the top, and then write the layout and components like you would normally:\n\n```py\n%%blocks\n\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(f\"# Greetings {args.name}!\")\n    inp = gr.Textbox()\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: x, inputs=inp, outputs=out)\n```\n\nNotice that:\n\n- You do not need to launch your demo ‚Äî Gradio does that for you automatically!\n\n- Every time you rerun the cell, Gradio will re-render your app on the same port and using the same underlying web server. This means you'll see your changes _much, much faster_ than if you were rerunning the cell normally.\n\nHere's what it looks like in a jupyter notebook:\n\n![](https://gradio-builds.s3.amazonaws.com/demo-files/jupyter_reload.gif)\n\nü™Ñ This works in colab notebooks too! [Here's a colab notebook](https://colab.research.google.com/drive/1zAuWoiTIb3O2oitbtVb2_ekv1K6ggtC1?usp=sharing) where you can see the Blocks magic in action. Try making some changes and re-running the cell with the Gradio code!\n\nThe Notebook Magic is now the author's preferred way of building Gradio demos. Regardless of how you write Python code, we hope either of these methods will give you a much better development experience using Gradio.\n\n---\n\n## Next Steps\n\nNow that you know how to develop quickly using Gradio, start building your own!\n\nIf you are looking for inspiration, try exploring demos other people have built with Gradio, [browse public Hugging Face Spaces](http://hf.space/) ü§ó\n",tags:[],spaces:[],url:"/guides/developing-faster-with-reload-mode/",contributor:null},{name:"how-to-use-3D-model-component",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:54,pretty_name:"How To Use 3D Model Component",content:"# How to Use the 3D Model Component\n\n\n\n\n## Introduction\n\n3D models are becoming more popular in machine learning and make for some of the most fun demos to experiment with. Using `gradio`, you can easily build a demo of your 3D image model and share it with anyone. The Gradio 3D Model component accepts 3 file types including: _.obj_, _.glb_, & _.gltf_.\n\nThis guide will show you how to build a demo for your 3D image model in a few lines of code; like the one below. Play around with 3D object by clicking around, dragging and zooming:\n\n\u003Cgradio-app space=\"gradio/Model3D\"> \u003C/gradio-app>\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](https://gradio.app/guides/quickstart).\n\n## Taking a Look at the Code\n\nLet's take a look at how to create the minimal interface above. The prediction function in this case will just return the original 3D model mesh, but you can change this function to run inference on your machine learning model. We'll take a look at more complex examples below.\n\n```python\nimport gradio as gr\nimport os\n\n\ndef load_mesh(mesh_file_name):\n    return mesh_file_name\n\n\ndemo = gr.Interface(\n    fn=load_mesh,\n    inputs=gr.Model3D(),\n    outputs=gr.Model3D(\n            clear_color=[0.0, 0.0, 0.0, 0.0],  label=\"3D Model\"),\n    examples=[\n        [os.path.join(os.path.dirname(__file__), \"files/Bunny.obj\")],\n        [os.path.join(os.path.dirname(__file__), \"files/Duck.glb\")],\n        [os.path.join(os.path.dirname(__file__), \"files/Fox.gltf\")],\n        [os.path.join(os.path.dirname(__file__), \"files/face.obj\")],\n    ],\n)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\nLet's break down the code above:\n\n`load_mesh`: This is our 'prediction' function and for simplicity, this function will take in the 3D model mesh and return it.\n\nCreating the Interface:\n\n- `fn`: the prediction function that is used when the user clicks submit. In our case this is the `load_mesh` function.\n- `inputs`: create a model3D input component. The input expects an uploaded file as a {str} filepath.\n- `outputs`: create a model3D output component. The output component also expects a file as a {str} filepath.\n  - `clear_color`: this is the background color of the 3D model canvas. Expects RGBa values.\n  - `label`: the label that appears on the top left of the component.\n- `examples`: list of 3D model files. The 3D model component can accept _.obj_, _.glb_, & _.gltf_ file types.\n- `cache_examples`: saves the predicted output for the examples, to save time on inference.\n\n## Exploring a more complex Model3D Demo:\n\nBelow is a demo that uses the DPT model to predict the depth of an image and then uses 3D Point Cloud to create a 3D object. Take a look at the [app.py](https://huggingface.co/spaces/gradio/dpt-depth-estimation-3d-obj/blob/main/app.py) file for a peek into the code and the model prediction function.\n\u003Cgradio-app space=\"gradio/dpt-depth-estimation-3d-obj\"> \u003C/gradio-app>\n\n---\n\nAnd you're done! That's all the code you need to build an interface for your Model3D model. Here are some references that you may find useful:\n\n- Gradio's [\"Getting Started\" guide](https://gradio.app/getting_started/)\n- The first [3D Model Demo](https://huggingface.co/spaces/gradio/Model3D) and [complete code](https://huggingface.co/spaces/gradio/Model3D/tree/main) (on Hugging Face Spaces)\n",tags:["VISION","IMAGE"],spaces:["https://huggingface.co/spaces/gradio/Model3D","https://huggingface.co/spaces/gradio/PIFu-Clothed-Human-Digitization","https://huggingface.co/spaces/gradio/dpt-depth-estimation-3d-obj"],url:"/guides/how-to-use-3D-model-component/",contributor:null},{name:"image-classification-in-pytorch",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:55,pretty_name:"Image Classification In Pytorch",content:"# Image Classification in PyTorch\n\n\n\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from autonomous vehicles to medical imaging.\n\nSuch models are perfect to use with Gradio's _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like the demo on the bottom of the page.\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). We will be using a pretrained image classification model, so you should also have `torch` installed.\n\n## Step 1 ‚Äî Setting up the Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a pretrained Resnet-18 model, as it is easily downloadable from [PyTorch Hub](https://pytorch.org/hub/pytorch_vision_resnet/). You can use a different pretrained model or train your own.\n\n```python\nimport torch\n\nmodel = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()\n```\n\nBecause we will be using the model for inference, we have called the `.eval()` method.\n\n## Step 2 ‚Äî Defining a `predict` function\n\nNext, we will need to define a function that takes in the _user input_, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this [text file](https://git.io/JJkYN).\n\nIn the case of our pretrained model, it will look like this:\n\n```python\nimport requests\nfrom PIL import Image\nfrom torchvision import transforms\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef predict(inp):\n  inp = transforms.ToTensor()(inp).unsqueeze(0)\n  with torch.no_grad():\n    prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n    confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n  return confidences\n```\n\nLet's break this down. The function takes one parameter:\n\n- `inp`: the input image as a `PIL` image\n\nThen, the function converts the image to a PIL Image and then eventually a PyTorch `tensor`, passes it through the model, and returns:\n\n- `confidences`: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities\n\n## Step 3 ‚Äî Creating a Gradio Interface\n\nNow that we have our predictive function set up, we can create a Gradio Interface around it.\n\nIn this case, the input component is a drag-and-drop image component. To create this input, we use `Image(type=\"pil\")` which creates the component and handles the preprocessing to convert that to a `PIL` image.\n\nThe output component will be a `Label`, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images by constructing it as `Label(num_top_classes=3)`.\n\nFinally, we'll add one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:\n\n```python\nimport gradio as gr\n\ngr.Interface(fn=predict,\n             inputs=gr.Image(type=\"pil\"),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"lion.jpg\", \"cheetah.jpg\"]).launch()\n```\n\nThis produces the following interface, which you can try right here in your browser (try uploading your own examples!):\n\n\u003Cgradio-app space=\"gradio/pytorch-image-classifier\">\n\n\n---\n\nAnd you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n",tags:["VISION","RESNET","PYTORCH"],spaces:["https://huggingface.co/spaces/abidlabs/pytorch-image-classifier","https://huggingface.co/spaces/pytorch/ResNet","https://huggingface.co/spaces/pytorch/ResNext","https://huggingface.co/spaces/pytorch/SqueezeNet"],url:"/guides/image-classification-in-pytorch/",contributor:null},{name:"image-classification-in-tensorflow",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:56,pretty_name:"Image Classification In Tensorflow",content:"# Image Classification in TensorFlow and Keras\n\n\n\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from traffic control systems to satellite imaging.\n\nSuch models are perfect to use with Gradio's _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like the demo on the bottom of the page.\n\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). We will be using a pretrained Keras image classification model, so you should also have `tensorflow` installed.\n\n## Step 1 ‚Äî Setting up the Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a pretrained Mobile Net model, as it is easily downloadable from [Keras](https://keras.io/api/applications/mobilenet/). You can use a different pretrained model or train your own.\n\n```python\nimport tensorflow as tf\n\ninception_net = tf.keras.applications.MobileNetV2()\n```\n\nThis line automatically downloads the MobileNet model and weights using the Keras library.\n\n## Step 2 ‚Äî Defining a `predict` function\n\nNext, we will need to define a function that takes in the _user input_, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this [text file](https://git.io/JJkYN).\n\nIn the case of our pretrained model, it will look like this:\n\n```python\nimport requests\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef classify_image(inp):\n  inp = inp.reshape((-1, 224, 224, 3))\n  inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n  prediction = inception_net.predict(inp).flatten()\n  confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n  return confidences\n```\n\nLet's break this down. The function takes one parameter:\n\n- `inp`: the input image as a `numpy` array\n\nThen, the function adds a batch dimension, passes it through the model, and returns:\n\n- `confidences`: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities\n\n## Step 3 ‚Äî Creating a Gradio Interface\n\nNow that we have our predictive function set up, we can create a Gradio Interface around it.\n\nIn this case, the input component is a drag-and-drop image component. To create this input, we can use the `\"gradio.inputs.Image\"` class, which creates the component and handles the preprocessing to convert that to a numpy array. We will instantiate the class with a parameter that automatically preprocesses the input image to be 224 pixels by 224 pixels, which is the size that MobileNet expects.\n\nThe output component will be a `\"label\"`, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images.\n\nFinally, we'll add one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:\n\n```python\nimport gradio as gr\n\ngr.Interface(fn=classify_image,\n             inputs=gr.Image(shape=(224, 224)),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"banana.jpg\", \"car.jpg\"]).launch()\n```\n\nThis produces the following interface, which you can try right here in your browser (try uploading your own examples!):\n\n\u003Cgradio-app space=\"gradio/keras-image-classifier\">\n\n---\n\nAnd you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n",tags:["VISION","MOBILENET","TENSORFLOW"],spaces:["https://huggingface.co/spaces/abidlabs/keras-image-classifier"],url:"/guides/image-classification-in-tensorflow/",contributor:null},{name:"image-classification-with-vision-transformers",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:57,pretty_name:"Image Classification With Vision Transformers",content:"# Image Classification with Vision Transformers\n\n\n\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from facial recognition to manufacturing quality control.\n\nState-of-the-art image classifiers are based on the _transformers_ architectures, originally popularized for NLP tasks. Such architectures are typically called vision transformers (ViT). Such models are perfect to use with Gradio's _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in a **single line of Python**, and it will look like the demo on the bottom of the page.\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started).\n\n## Step 1 ‚Äî Choosing a Vision Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a model from the [Hugging Face Model Hub](https://huggingface.co/models?pipeline_tag=image-classification). The Hub contains thousands of models covering dozens of different machine learning tasks.\n\nExpand the Tasks category on the left sidebar and select \"Image Classification\" as our task of interest. You will then see all of the models on the Hub that are designed to classify images.\n\nAt the time of writing, the most popular one is `google/vit-base-patch16-224`, which has been trained on ImageNet images at a resolution of 224x224 pixels. We will use this model for our demo.\n\n## Step 2 ‚Äî Loading the Vision Transformer Model with Gradio\n\nWhen using a model from the Hugging Face Hub, we do not need to define the input or output components for the demo. Similarly, we do not need to be concerned with the details of preprocessing or postprocessing.\nAll of these are automatically inferred from the model tags.\n\nBesides the import statement, it only takes a single line of Python to load and launch the demo.\n\nWe use the `gr.Interface.load()` method and pass in the path to the model including the `huggingface/` to designate that it is from the Hugging Face Hub.\n\n```python\nimport gradio as gr\n\ngr.Interface.load(\n             \"huggingface/google/vit-base-patch16-224\",\n             examples=[\"alligator.jpg\", \"laptop.jpg\"]).launch()\n```\n\nNotice that we have added one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples.\n\nThis produces the following interface, which you can try right here in your browser. When you input an image, it is automatically preprocessed and sent to the Hugging Face Hub API, where it is passed through the model and returned as a human-interpretable prediction. Try uploading your own image!\n\n\u003Cgradio-app space=\"gradio/vision-transformer\">\n\n---\n\nAnd you're done! In one line of code, you have built a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n",tags:["VISION","TRANSFORMERS","HUB"],spaces:["https://huggingface.co/spaces/abidlabs/vision-transformer"],url:"/guides/image-classification-with-vision-transformers/",contributor:null},{name:"installing-gradio-in-a-virtual-environment",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:58,pretty_name:"Installing Gradio In A Virtual Environment",content:"# Installing Gradio in a Virtual Environment\n\n\n\nIn this guide, we will describe step-by-step how to install `gradio` within a virtual environment. This guide will cover both Windows and MacOS/Linux systems.\n\n## Virtual Environments\n\nA virtual environment in Python is a self-contained directory that holds a Python installation for a particular version of Python, along with a number of additional packages. This environment is isolated from the main Python installation and other virtual environments. Each environment can have its own independent set of installed Python packages, which allows you to maintain different versions of libraries for different projects without conflicts.\n\n\nUsing virtual environments ensures that you can work on multiple Python projects on the same machine without any conflicts. This is particularly useful when different projects require different versions of the same library. It also simplifies dependency management and enhances reproducibility, as you can easily share the requirements of your project with others.\n\n\n## Installing Gradio on Windows\n\nTo install Gradio on a Windows system in a virtual environment, follow these steps:\n\n1. **Install Python**: Ensure you have Python 3.8 or higher installed. You can download it from [python.org](https://www.python.org/). You can verify the installation by running `python --version` or `python3 --version` in Command Prompt.\n\n\n2. **Create a Virtual Environment**:\n   Open Command Prompt and navigate to your project directory. Then create a virtual environment using the following command:\n\n   ```bash\n   python -m venv gradio-env\n   ```\n\n   This command creates a new directory `gradio-env` in your project folder, containing a fresh Python installation.\n\n3. **Activate the Virtual Environment**:\n   To activate the virtual environment, run:\n\n   ```bash\n   .\\gradio-env\\Scripts\\activate\n   ```\n\n   Your command prompt should now indicate that you are working inside `gradio-env`. Note: you can choose a different name than `gradio-env` for your virtual environment in this step.\n\n\n4. **Install Gradio**:\n   Now, you can install Gradio using pip:\n\n   ```bash\n   pip install gradio\n   ```\n\n5. **Verification**:\n   To verify the installation, run `python` and then type:\n\n   ```python\n   import gradio as gr\n   print(gr.__version__)\n   ```\n\n   This will display the installed version of Gradio.\n\n## Installing Gradio on MacOS/Linux\n\nThe installation steps on MacOS and Linux are similar to Windows but with some differences in commands.\n\n1. **Install Python**:\n   Python usually comes pre-installed on MacOS and most Linux distributions. You can verify the installation by running `python --version` in the terminal (note that depending on how Python is installed, you might have to use `python3` instead of `python` throughout these steps). \n   \n   Ensure you have Python 3.8 or higher installed. If you do not have it installed, you can download it from [python.org](https://www.python.org/). \n\n2. **Create a Virtual Environment**:\n   Open Terminal and navigate to your project directory. Then create a virtual environment using:\n\n   ```bash\n   python -m venv gradio-env\n   ```\n\n   Note: you can choose a different name than `gradio-env` for your virtual environment in this step.\n\n3. **Activate the Virtual Environment**:\n   To activate the virtual environment on MacOS/Linux, use:\n\n   ```bash\n   source gradio-env/bin/activate\n   ```\n\n4. **Install Gradio**:\n   With the virtual environment activated, install Gradio using pip:\n\n   ```bash\n   pip install gradio\n   ```\n\n5. **Verification**:\n   To verify the installation, run `python` and then type:\n\n   ```python\n   import gradio as gr\n   print(gr.__version__)\n   ```\n\n   This will display the installed version of Gradio.\n\nBy following these steps, you can successfully install Gradio in a virtual environment on your operating system, ensuring a clean and managed workspace for your Python projects.",tags:["INSTALLATION"],spaces:[],url:"/guides/installing-gradio-in-a-virtual-environment/",contributor:null},{name:"named-entity-recognition",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:59,pretty_name:"Named Entity Recognition",content:"# Named-Entity Recognition\n\n\n\n\n## Introduction\n\nNamed-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or \"token\") into different categories, such as names of people or names of locations, or different parts of speech.\n\nFor example, given the sentence:\n\n> Does Chicago have any Pakistani restaurants?\n\nA named-entity recognition algorithm may identify:\n\n- \"Chicago\" as a **location**\n- \"Pakistani\" as an **ethnicity**\n\nand so on.\n\nUsing `gradio` (specifically the `HighlightedText` component), you can easily build a web demo of your NER model and share that with the rest of your team.\n\nHere is an example of a demo that you'll be able to build:\n\n\u003Cgradio-app space='gradio/ner_pipeline'>\u003C/gradio-app>\n\nThis tutorial will show how to take a pretrained NER model and deploy it with a Gradio interface. We will show two different ways to use the `HighlightedText` component -- depending on your NER model, either of these two ways may be easier to learn!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). You will also need a pretrained named-entity recognition model. You can use your own, while in this tutorial, we will use one from the `transformers` library.\n\n### Approach 1: List of Entity Dictionaries\n\nMany named-entity recognition models output a list of dictionaries. Each dictionary consists of an _entity_, a \"start\" index, and an \"end\" index. This is, for example, how NER models in the `transformers` library operate:\n\n```py\nfrom transformers import pipeline\nner_pipeline = pipeline(\"ner\")\nner_pipeline(\"Does Chicago have any Pakistani restaurants\")\n```\n\nOutput:\n\n```bash\n[{'entity': 'I-LOC',\n  'score': 0.9988978,\n  'index': 2,\n  'word': 'Chicago',\n  'start': 5,\n  'end': 12},\n {'entity': 'I-MISC',\n  'score': 0.9958592,\n  'index': 5,\n  'word': 'Pakistani',\n  'start': 22,\n  'end': 31}]\n```\n\nIf you have such a model, it is very easy to hook it up to Gradio's `HighlightedText` component. All you need to do is pass in this **list of entities**, along with the **original text** to the model, together as dictionary, with the keys being `\"entities\"` and `\"text\"` respectively.\n\nHere is a complete example:\n\n```python\nfrom transformers import pipeline\n\nimport gradio as gr\n\nner_pipeline = pipeline(\"ner\")\n\nexamples = [\n    \"Does Chicago have any stores and does Joe live here?\",\n]\n\ndef ner(text):\n    output = ner_pipeline(text)\n    return {\"text\": text, \"entities\": output}    \n\ndemo = gr.Interface(ner,\n             gr.Textbox(placeholder=\"Enter sentence here...\"), \n             gr.HighlightedText(),\n             examples=examples)\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/ner_pipeline'>\u003C/gradio-app>\n\n### Approach 2: List of Tuples\n\nAn alternative way to pass data into the `HighlightedText` component is a list of tuples. The first element of each tuple should be the word or words that are being classified into a particular entity. The second element should be the entity label (or `None` if they should be unlabeled). The `HighlightedText` component automatically strings together the words and labels to display the entities.\n\nIn some cases, this can be easier than the first approach. Here is a demo showing this approach using Spacy's parts-of-speech tagger:\n\n```python\nimport gradio as gr\nimport os\nos.system('python -m spacy download en_core_web_sm')\nimport spacy\nfrom spacy import displacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef text_analysis(text):\n    doc = nlp(text)\n    html = displacy.render(doc, style=\"dep\", page=True)\n    html = (\n        \"\u003Cdiv style='max-width:100%; max-height:360px; overflow:auto'>\"\n        + html\n        + \"\u003C/div>\"\n    )\n    pos_count = {\n        \"char_count\": len(text),\n        \"token_count\": 0,\n    }\n    pos_tokens = []\n\n    for token in doc:\n        pos_tokens.extend([(token.text, token.pos_), (\" \", None)])\n\n    return pos_tokens, pos_count, html\n\ndemo = gr.Interface(\n    text_analysis,\n    gr.Textbox(placeholder=\"Enter sentence here...\"),\n    [\"highlight\", \"json\", \"html\"],\n    examples=[\n        [\"What a beautiful morning for a walk!\"],\n        [\"It was the best of times, it was the worst of times.\"],\n    ],\n)\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/text_analysis'>\u003C/gradio-app>\n\n---\n\nAnd you're done! That's all you need to know to build a web-based GUI for your NER model.\n\nFun tip: you can share your NER demo instantly with others simply by setting `share=True` in `launch()`.\n",tags:["NER","TEXT","HIGHLIGHT"],spaces:["https://huggingface.co/spaces/rajistics/biobert_ner_demo","https://huggingface.co/spaces/abidlabs/ner","https://huggingface.co/spaces/rajistics/Financial_Analyst_AI"],url:"/guides/named-entity-recognition/",contributor:null},{name:"real-time-speech-recognition",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:60,pretty_name:"Real Time Speech Recognition",content:"# Real Time Speech Recognition\n\n\n\n## Introduction\n\nAutomatic speech recognition (ASR), the conversion of spoken speech to text, is a very important and thriving area of machine learning. ASR algorithms run on practically every smartphone, and are becoming increasingly embedded in professional workflows, such as digital assistants for nurses and doctors. Because ASR algorithms are designed to be used directly by customers and end users, it is important to validate that they are behaving as expected when confronted with a wide variety of speech patterns (different accents, pitches, and background audio conditions).\n\nUsing `gradio`, you can easily build a demo of your ASR model and share that with a testing team, or test it yourself by speaking through the microphone on your device.\n\nThis tutorial will show how to take a pretrained speech-to-text model and deploy it with a Gradio interface. We will start with a **_full-context_** model, in which the user speaks the entire audio before the prediction runs. Then we will adapt the demo to make it **_streaming_**, meaning that the audio model will convert speech as you speak. \n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). You will also need a pretrained speech recognition model. In this tutorial, we will build demos from 2 ASR libraries:\n\n- Transformers (for this, `pip install transformers` and `pip install torch`)\n\nMake sure you have at least one of these installed so that you can follow along the tutorial. You will also need `ffmpeg` [installed on your system](https://www.ffmpeg.org/download.html), if you do not already have it, to process files from the microphone.\n\nHere's how to build a real time speech recognition (ASR) app:\n\n1. [Set up the Transformers ASR Model](#1-set-up-the-transformers-asr-model)\n2. [Create a Full-Context ASR Demo with Transformers](#2-create-a-full-context-asr-demo-with-transformers)\n3. [Create a Streaming ASR Demo with Transformers](#3-create-a-streaming-asr-demo-with-transformers)\n\n## 1. Set up the Transformers ASR Model\n\nFirst, you will need to have an ASR model that you have either trained yourself or you will need to download a pretrained model. In this tutorial, we will start by using a pretrained ASR model from the model, `whisper`.\n\nHere is the code to load `whisper` from Hugging Face `transformers`.\n\n```python\nfrom transformers import pipeline\n\np = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n```\n\nThat's it!\n\n## 2. Create a Full-Context ASR Demo with Transformers\n\nWe will start by creating a _full-context_ ASR demo, in which the user speaks the full audio before using the ASR model to run inference. This is very easy with Gradio -- we simply create a function around the `pipeline` object above.\n\nWe will use `gradio`'s built in `Audio` component, configured to take input from the user's microphone and return a filepath for the recorded audio. The output component will be a plain `Textbox`.\n\n```python\nimport gradio as gr\nfrom transformers import pipeline\nimport numpy as np\n\ntranscriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n\ndef transcribe(audio):\n    sr, y = audio\n    y = y.astype(np.float32)\n    y /= np.max(np.abs(y))\n\n    return transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]\n\n\ndemo = gr.Interface(\n    transcribe,\n    gr.Audio(sources=[\"microphone\"]),\n    \"text\",\n)\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/asr'>\u003C/gradio-app>\n\nThe `transcribe` function takes a single parameter, `audio`, which is a numpy array of the audio the user recorded. The `pipeline` object expects this in float32 format, so we convert it first to float32, and then extract the transcribed text.\n\n## 3. Create a Streaming ASR Demo with Transformers\n\nTo make this a *streaming* demo, we need to make these changes:\n\n1. Set `streaming=True` in the `Audio` component\n2. Set `live=True` in the `Interface`\n3. Add a `state` to the interface to store the recorded audio of a user\n\nTake a look below.\n\n```python\nimport gradio as gr\nfrom transformers import pipeline\nimport numpy as np\n\ntranscriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n\ndef transcribe(stream, new_chunk):\n    sr, y = new_chunk\n    y = y.astype(np.float32)\n    y /= np.max(np.abs(y))\n\n    if stream is not None:\n        stream = np.concatenate([stream, y])\n    else:\n        stream = y\n    return stream, transcriber({\"sampling_rate\": sr, \"raw\": stream})[\"text\"]\n\n\ndemo = gr.Interface(\n    transcribe,\n    [\"state\", gr.Audio(sources=[\"microphone\"], streaming=True)],\n    [\"state\", \"text\"],\n    live=True,\n)\n\ndemo.launch()\n\n```\n\nNotice now we have a state variable now, because we need to track all the audio history. `transcribe` gets called whenever there is a new small chunk of audio, but we also need to keep track of all the audio that has been spoken so far in state. \nAs the interface runs, the `transcribe` function gets called, with a record of all the previously spoken audio in `stream`, as well as the new chunk of audio as `new_chunk`. We return the new full audio so that can be stored back in state, and we also return the transcription.\nHere we naively append the audio together and simply call the `transcriber` object on the entire audio. You can imagine more efficient ways of handling this, such as re-processing only the last 5 seconds of audio whenever a new chunk of audio received. \n\n\u003Cgradio-app space='gradio/stream_asr'>\u003C/gradio-app>\n\nNow the ASR model will run inference as you speak! ",tags:["ASR","SPEECH","STREAMING"],spaces:[],url:"/guides/real-time-speech-recognition/",contributor:null},{name:"running-background-tasks",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:61,pretty_name:"Running Background Tasks",content:"# Running Background Tasks\n\n\n\n\n## Introduction\n\nThis guide explains how you can run background tasks from your gradio app.\nBackground tasks are operations that you'd like to perform outside the request-response\nlifecycle of your app either once or on a periodic schedule.\nExamples of background tasks include periodically synchronizing data to an external database or\nsending a report of model predictions via email.\n\n## Overview\n\nWe will be creating a simple \"Google-forms-style\" application to gather feedback from users of the gradio library.\nWe will use a local sqlite database to store our data, but we will periodically synchronize the state of the database\nwith a [HuggingFace Dataset](https://huggingface.co/datasets) so that our user reviews are always backed up.\nThe synchronization will happen in a background task running every 60 seconds.\n\nAt the end of the demo, you'll have a fully working application like this one:\n\n\u003Cgradio-app space=\"freddyaboulton/gradio-google-forms\"> \u003C/gradio-app>\n\n## Step 1 - Write your database logic üíæ\n\nOur application will store the name of the reviewer, their rating of gradio on a scale of 1 to 5, as well as\nany comments they want to share about the library. Let's write some code that creates a database table to\nstore this data. We'll also write some functions to insert a review into that table and fetch the latest 10 reviews.\n\nWe're going to use the `sqlite3` library to connect to our sqlite database but gradio will work with any library.\n\nThe code will look like this:\n\n```python\nDB_FILE = \"./reviews.db\"\ndb = sqlite3.connect(DB_FILE)\n\n# Create table if it doesn't already exist\ntry:\n    db.execute(\"SELECT * FROM reviews\").fetchall()\n    db.close()\nexcept sqlite3.OperationalError:\n    db.execute(\n        '''\n        CREATE TABLE reviews (id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n                              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP NOT NULL,\n                              name TEXT, review INTEGER, comments TEXT)\n        ''')\n    db.commit()\n    db.close()\n\ndef get_latest_reviews(db: sqlite3.Connection):\n    reviews = db.execute(\"SELECT * FROM reviews ORDER BY id DESC limit 10\").fetchall()\n    total_reviews = db.execute(\"Select COUNT(id) from reviews\").fetchone()[0]\n    reviews = pd.DataFrame(reviews, columns=[\"id\", \"date_created\", \"name\", \"review\", \"comments\"])\n    return reviews, total_reviews\n\n\ndef add_review(name: str, review: int, comments: str):\n    db = sqlite3.connect(DB_FILE)\n    cursor = db.cursor()\n    cursor.execute(\"INSERT INTO reviews(name, review, comments) VALUES(?,?,?)\", [name, review, comments])\n    db.commit()\n    reviews, total_reviews = get_latest_reviews(db)\n    db.close()\n    return reviews, total_reviews\n```\n\nLet's also write a function to load the latest reviews when the gradio application loads:\n\n```python\ndef load_data():\n    db = sqlite3.connect(DB_FILE)\n    reviews, total_reviews = get_latest_reviews(db)\n    db.close()\n    return reviews, total_reviews\n```\n\n## Step 2 - Create a gradio app ‚ö°\n\nNow that we have our database logic defined, we can use gradio create a dynamic web page to ask our users for feedback!\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            name = gr.Textbox(label=\"Name\", placeholder=\"What is your name?\")\n            review = gr.Radio(label=\"How satisfied are you with using gradio?\", choices=[1, 2, 3, 4, 5])\n            comments = gr.Textbox(label=\"Comments\", lines=10, placeholder=\"Do you have any feedback on gradio?\")\n            submit = gr.Button(value=\"Submit Feedback\")\n        with gr.Column():\n            data = gr.Dataframe(label=\"Most recently created 10 rows\")\n            count = gr.Number(label=\"Total number of reviews\")\n    submit.click(add_review, [name, review, comments], [data, count])\n    demo.load(load_data, None, [data, count])\n```\n\n## Step 3 - Synchronize with HuggingFace Datasets ü§ó\n\nWe could call `demo.launch()` after step 2 and have a fully functioning application. However,\nour data would be stored locally on our machine. If the sqlite file were accidentally deleted, we'd lose all of our reviews!\nLet's back up our data to a dataset on the HuggingFace hub.\n\nCreate a dataset [here](https://huggingface.co/datasets) before proceeding.\n\nNow at the **top** of our script, we'll use the [huggingface hub client library](https://huggingface.co/docs/huggingface_hub/index)\nto connect to our dataset and pull the latest backup.\n\n```python\nTOKEN = os.environ.get('HUB_TOKEN')\nrepo = huggingface_hub.Repository(\n    local_dir=\"data\",\n    repo_type=\"dataset\",\n    clone_from=\"\u003Cname-of-your-dataset>\",\n    use_auth_token=TOKEN\n)\nrepo.git_pull()\n\nshutil.copyfile(\"./data/reviews.db\", DB_FILE)\n```\n\nNote that you'll have to get an access token from the \"Settings\" tab of your HuggingFace for the above code to work.\nIn the script, the token is securely accessed via an environment variable.\n\n![access_token](https://github.com/gradio-app/gradio/blob/main/guides/assets/access_token.png?raw=true)\n\nNow we will create a background task to synch our local database to the dataset hub every 60 seconds.\nWe will use the [AdvancedPythonScheduler](https://apscheduler.readthedocs.io/en/3.x/) to handle the scheduling.\nHowever, this is not the only task scheduling library available. Feel free to use whatever you are comfortable with.\n\nThe function to back up our data will look like this:\n\n```python\nfrom apscheduler.schedulers.background import BackgroundScheduler\n\ndef backup_db():\n    shutil.copyfile(DB_FILE, \"./data/reviews.db\")\n    db = sqlite3.connect(DB_FILE)\n    reviews = db.execute(\"SELECT * FROM reviews\").fetchall()\n    pd.DataFrame(reviews).to_csv(\"./data/reviews.csv\", index=False)\n    print(\"updating db\")\n    repo.push_to_hub(blocking=False, commit_message=f\"Updating data at {datetime.datetime.now()}\")\n\n\nscheduler = BackgroundScheduler()\nscheduler.add_job(func=backup_db, trigger=\"interval\", seconds=60)\nscheduler.start()\n```\n\n## Step 4 (Bonus) - Deployment to HuggingFace Spaces\n\nYou can use the HuggingFace [Spaces](https://huggingface.co/spaces) platform to deploy this application for free ‚ú®\n\nIf you haven't used Spaces before, follow the previous guide [here](/using_hugging_face_integrations).\nYou will have to use the `HUB_TOKEN` environment variable as a secret in the Guides.\n\n## Conclusion\n\nCongratulations! You know how to run background tasks from your gradio app on a schedule ‚è≤Ô∏è.\n\nCheckout the application running on Spaces [here](https://huggingface.co/spaces/freddyaboulton/gradio-google-forms).\nThe complete code is [here](https://huggingface.co/spaces/freddyaboulton/gradio-google-forms/blob/main/app.py)\n",tags:["TASKS","SCHEDULED","TABULAR","DATA"],spaces:["https://huggingface.co/spaces/freddyaboulton/gradio-google-forms"],url:"/guides/running-background-tasks/",contributor:null},{name:"running-gradio-on-your-web-server-with-nginx",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:62,pretty_name:"Running Gradio On Your Web Server With Nginx",content:"# Running a Gradio App on your Web Server with Nginx\n\n\n\n## Introduction\n\nGradio is a Python library that allows you to quickly create customizable web apps for your machine learning models and data processing pipelines. Gradio apps can be deployed on [Hugging Face Spaces](https://hf.space) for free.\n\nIn some cases though, you might want to deploy a Gradio app on your own web server. You might already be using [Nginx](https://www.nginx.com/), a highly performant web server, to serve your website (say `https://www.example.com`), and you want to attach Gradio to a specific subpath on your website (e.g. `https://www.example.com/gradio-demo`).\n\nIn this Guide, we will guide you through the process of running a Gradio app behind Nginx on your own web server to achieve this.\n\n**Prerequisites**\n\n1. A Linux web server with [Nginx installed](https://www.nginx.com/blog/setting-up-nginx/) and [Gradio installed](/quickstart)\n2. A working Gradio app saved as a python file on your web server\n\n## Editing your Nginx configuration file\n\n1. Start by editing the Nginx configuration file on your web server. By default, this is located at: `/etc/nginx/nginx.conf`\n\nIn the `http` block, add the following line to include server block configurations from a separate file:\n\n```bash\ninclude /etc/nginx/sites-enabled/*;\n```\n\n2. Create a new file in the `/etc/nginx/sites-available` directory (create the directory if it does not already exist), using a filename that represents your app, for example: `sudo nano /etc/nginx/sites-available/my_gradio_app`\n\n3. Paste the following into your file editor:\n\n```bash\nserver {\n    listen 80;\n    server_name example.com www.example.com;  # Change this to your domain name\n\n    location /gradio-demo/ {  # Change this if you'd like to server your Gradio app on a different path\n        proxy_pass http://127.0.0.1:7860/; # Change this if your Gradio app will be running on a different port\n        proxy_buffering off;\n        proxy_redirect off;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-Host $host;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n```\n\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> Setting the `X-Forwarded-Host` and `X-Forwarded-Proto` headers is important as Gradio uses these, in conjunction with the `root_path` parameter discussed below, to construct the public URL that your app is being served on. Gradio uses the public URL to fetch various static assets. If these headers are not set, your Gradio app may load in a broken state.\u003C/p>\n\n## Run your Gradio app on your web server\n\n1. Before you launch your Gradio app, you'll need to set the `root_path` to be the same as the subpath that you specified in your nginx configuration. This is necessary for Gradio to run on any subpath besides the root of the domain.\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> You can also provide a complete URL for `root_path` (beginning with `http` or `https`) in which case the `root_path` is treated as an absolute URL instead of a URL suffix (but in this case, you'll need to update the `root_path` if the domain changes).\u003C/p>\n\nHere's a simple example of a Gradio app with a custom `root_path` corresponding to the Nginx configuration above.\n\n```python\nimport gradio as gr\nimport time\n\ndef test(x):\ntime.sleep(4)\nreturn x\n\ngr.Interface(test, \"textbox\", \"textbox\").queue().launch(root_path=\"/gradio-demo\")\n```\n\n2. Start a `tmux` session by typing `tmux` and pressing enter (optional)\n\nIt's recommended that you run your Gradio app in a `tmux` session so that you can keep it running in the background easily\n\n3. Then, start your Gradio app. Simply type in `python` followed by the name of your Gradio python file. By default, your app will run on `localhost:7860`, but if it starts on a different port, you will need to update the nginx configuration file above.\n\n## Restart Nginx\n\n1. If you are in a tmux session, exit by typing CTRL+B (or CMD+B), followed by the \"D\" key.\n\n2. Finally, restart nginx by running `sudo systemctl restart nginx`.\n\nAnd that's it! If you visit `https://example.com/gradio-demo` on your browser, you should see your Gradio app running there\n\n",tags:["DEPLOYMENT","WEB SERVER","NGINX"],spaces:[],url:"/guides/running-gradio-on-your-web-server-with-nginx/",contributor:null},{name:"setting-up-a-demo-for-maximum-performance",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:63,pretty_name:"Setting Up A Demo For Maximum Performance",content:"# Setting Up a Demo for Maximum Performance\n\n\n\nLet's say that your Gradio demo goes _viral_ on social media -- you have lots of users trying it out simultaneously, and you want to provide your users with the best possible experience or, in other words, minimize the amount of time that each user has to wait in the queue to see their prediction.\n\nHow can you configure your Gradio demo to handle the most traffic? In this Guide, we dive into some of the parameters of Gradio's `.queue()` method as well as some other related parameters, and discuss how to set these parameters in a way that allows you to serve lots of users simultaneously with minimal latency.\n\nThis is an advanced guide, so make sure you know the basics of Gradio already, such as [how to create and launch a Gradio Interface](https://gradio.app/guides/quickstart/). Most of the information in this Guide is relevant whether you are hosting your demo on [Hugging Face Spaces](https://hf.space) or on your own server.\n\n## Overview of Gradio's Queueing System\n\nBy default, every Gradio demo includes a built-in queuing system that scales to thousands of requests. When a user of your app submits a request (i.e. submits an input to your function), Gradio adds the request to the queue, and requests are processed in order, generally speaking (this is not exactly true, as discussed below). When the user's request has finished processing, the Gradio server returns the result back to the user using server-side events (SSE). The SSE protocol has several advantages over simply using HTTP POST requests: \n\n(1) They do not time out -- most browsers raise a timeout error if they do not get a response to a POST request after a short period of time (e.g. 1 min). This can be a problem if your inference function takes longer than 1 minute to run or if many people are trying out your demo at the same time, resulting in increased latency.\n\n(2) They allow the server to send multiple updates to the frontend. This means, for example, that the server can send a real-time ETA of how long your prediction will take to complete.\n\nTo configure the queue, simply call the `.queue()` method before launching an `Interface`, `TabbedInterface`, `ChatInterface` or any `Blocks`. Here's an example:\n\n```py\nimport gradio as gr\n\napp = gr.Interface(lambda x:x, \"image\", \"image\")\napp.queue()  # \u003C-- Sets up a queue with default parameters\napp.launch()\n```\n\n**How Requests are Processed from the Queue**\n\nWhen a Gradio server is launched, a pool of threads is used to execute requests from the queue. By default, the maximum size of this thread pool is `40` (which is the default inherited from FastAPI, on which the Gradio server is based). However, this does *not* mean that 40 requests are always processed in parallel from the queue. \n\nInstead, Gradio uses a **single-function-single-worker** model by default. This means that each worker thread is only assigned a single function from among all of the functions that could be part of your Gradio app. This ensures that you do not see, for example, out-of-memory errors, due to multiple workers calling a machine learning model at the same time. Suppose you have 3 functions in your Gradio app: A, B, and C. And you see the following sequence of 7 requests come in from users using your app:\n\n```\n1 2 3 4 5 6 7\n-------------\nA B A A C B A\n```\n\nInitially, 3 workers will get dispatched to handle requests 1, 2, and 5 (corresponding to functions: A, B, C). As soon as any of these workers finish, they will start processing the next function in the queue of the same function type, e.g. the worker that finished processing request 1 will start processing request 3, and so on.\n\nIf you want to change this behavior, there are several parameters that can be used to configure the queue and help reduce latency. Let's go through them one-by-one.\n\n\n### The `default_concurrency_limit` parameter in `queue()`\n\nThe first parameter we will explore is the `default_concurrency_limit` parameter in `queue()`. This controls how many workers can execute the same event. By default, this is set to `1`, but you can set it to a higher integer: `2`, `10`, or even `None` (in the last case, there is no limit besides the total number of available workers). \n\nThis is useful, for example, if your Gradio app does not call any resource-intensive functions. If your app only queries external APIs, then you can set the `default_concurrency_limit` much higher. Increasing this parameter can **linearly multiply the capacity of your server to handle requests**.\n\nSo why not set this parameter much higher all the time? Keep in mind that since requests are processed in parallel, each request will consume memory to store the data and weights for processing. This means that you might get out-of-memory errors if you increase the `default_concurrency_limit` too high. You may also start to get diminishing returns if the `default_concurrency_limit` is too high because of costs of switching between different worker threads.\n\n**Recommendation**: Increase the `default_concurrency_limit` parameter as high as you can while you continue to see performance gains or until you hit memory limits on your machine. You can [read about Hugging Face Spaces machine specs here](https://huggingface.co/docs/hub/spaces-overview).\n\n\n### The `concurrency_limit` parameter in events\n\nYou can also set the number of requests that can be processed in parallel for each event individually. These take priority over the  `default_concurrency_limit` parameter described previously.\n\nTo do this, set the `concurrency_limit` parameter of any event listener, e.g. `btn.click(..., concurrency_limit=20)` or in the `Interface` or `ChatInterface` classes: e.g. `gr.Interface(..., concurrency_limit=20)`. By default, this parameter is set to the global `default_concurrency_limit`.\n\n\n### The `max_threads` parameter in `launch()`\n\nIf your demo uses non-async functions, e.g. `def` instead of `async def`, they will be run in a threadpool. This threadpool has a size of 40 meaning that only 40 threads can be created to run your non-async functions. If you are running into this limit, you can increase the threadpool size with `max_threads`. The default value is 40.\u003Cp class='tip'>\u003Cstrong>‚úçÔ∏è Tip:\u003C/strong> You should use async functions whenever possible to increase the number of concurrent requests your app can handle. Quick functions that are not CPU-bound are good candidates to be written as `async`. This [guide](https://fastapi.tiangolo.com/async/) is a good primer on the concept.\u003C/p>\n\n\n### The `max_size` parameter in `queue()`\n\nA more blunt way to reduce the wait times is simply to prevent too many people from joining the queue in the first place. You can set the maximum number of requests that the queue processes using the `max_size` parameter of `queue()`. If a request arrives when the queue is already of the maximum size, it will not be allowed to join the queue and instead, the user will receive an error saying that the queue is full and to try again. By default, `max_size=None`, meaning that there is no limit to the number of users that can join the queue.\n\nParadoxically, setting a `max_size` can often improve user experience because it prevents users from being dissuaded by very long queue wait times. Users who are more interested and invested in your demo will keep trying to join the queue, and will be able to get their results faster.\n\n**Recommendation**: For a better user experience, set a `max_size` that is reasonable given your expectations of how long users might be willing to wait for a prediction.\n\n### The `max_batch_size` parameter in events\n\nAnother way to increase the parallelism of your Gradio demo is to write your function so that it can accept **batches** of inputs. Most deep learning models can process batches of samples more efficiently than processing individual samples.\n\nIf you write your function to process a batch of samples, Gradio will automatically batch incoming requests together and pass them into your function as a batch of samples. You need to set `batch` to `True` (by default it is `False`) and set a `max_batch_size` (by default it is `4`) based on the maximum number of samples your function is able to handle. These two parameters can be passed into `gr.Interface()` or to an event in Blocks such as `.click()`.\n\nWhile setting a batch is conceptually similar to having workers process requests in parallel, it is often _faster_ than setting the `concurrency_count` for deep learning models. The downside is that you might need to adapt your function a little bit to accept batches of samples instead of individual samples.\n\nHere's an example of a function that does _not_ accept a batch of inputs -- it processes a single input at a time:\n\n```py\nimport time\n\ndef trim_words(word, length):\n    return word[:int(length)]\n\n```\n\nHere's the same function rewritten to take in a batch of samples:\n\n```py\nimport time\n\ndef trim_words(words, lengths):\n    trimmed_words = []\n    for w, l in zip(words, lengths):\n        trimmed_words.append(w[:int(l)])\n    return [trimmed_words]\n\n```\n\nThe second function can be used with `batch=True` and an appropriate `max_batch_size` parameter.\n\n**Recommendation**: If possible, write your function to accept batches of samples, and then set `batch` to `True` and the `max_batch_size` as high as possible based on your machine's memory limits.\n\n## Upgrading your Hardware (GPUs, TPUs, etc.)\n\nIf you have done everything above, and your demo is still not fast enough, you can upgrade the hardware that your model is running on. Changing the model from running on CPUs to running on GPUs will usually provide a 10x-50x increase in inference time for deep learning models.\n\nIt is particularly straightforward to upgrade your Hardware on Hugging Face Spaces. Simply click on the \"Settings\" tab in your Space and choose the Space Hardware you'd like.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/spaces-gpu-settings.png)\n\nWhile you might need to adapt portions of your machine learning inference code to run on a GPU (here's a [handy guide](https://cnvrg.io/pytorch-cuda/) if you are using PyTorch), Gradio is completely agnostic to the choice of hardware and will work completely fine if you use it with CPUs, GPUs, TPUs, or any other hardware!\n\nNote: your GPU memory is different than your CPU memory, so if you upgrade your hardware,\nyou might need to adjust the value of the `default_concurrency_limit` parameter described above.\n\n## Conclusion\n\nCongratulations! You know how to set up a Gradio demo for maximum performance. Good luck on your next viral demo!\n",tags:["CONCURRENCY","LATENCY","PERFORMANCE"],spaces:[],url:"/guides/setting-up-a-demo-for-maximum-performance/",contributor:null},{name:"theming-guide",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:64,pretty_name:"Theming Guide",content:"# Theming\n\n\n\n## Introduction\n\nGradio features a built-in theming engine that lets you customize the look and feel of your app. You can choose from a variety of themes, or create your own. To do so, pass the `theme=` kwarg to the `Blocks` or `Interface` constructor. For example:\n\n```python\nwith gr.Blocks(theme=gr.themes.Soft()) as demo:\n    ...\n```\n\n\u003Cdiv class=\"wrapper\">\n\u003Ciframe\n\tsrc=\"https://gradio-theme-soft.hf.space?__theme=light\"\n\tframeborder=\"0\"\n>\u003C/iframe>\n\u003C/div>\n\nGradio comes with a set of prebuilt themes which you can load from `gr.themes.*`. These are:\n\n- `gr.themes.Base()`\n- `gr.themes.Default()`\n- `gr.themes.Glass()`\n- `gr.themes.Monochrome()`\n- `gr.themes.Soft()`\n\nEach of these themes set values for hundreds of CSS variables. You can use prebuilt themes as a starting point for your own custom themes, or you can create your own themes from scratch. Let's take a look at each approach.\n\n## Using the Theme Builder\n\nThe easiest way to build a theme is using the Theme Builder. To launch the Theme Builder locally, run the following code:\n\n```python\nimport gradio as gr\n\ngr.themes.builder()\n```\n\n\u003Cgradio-app space='gradio/theme_builder'>\u003C/gradio-app>\n\nYou can use the Theme Builder running on Spaces above, though it runs much faster when you launch it locally via `gr.themes.builder()`.\n\nAs you edit the values in the Theme Builder, the app will preview updates in real time. You can download the code to generate the theme you've created so you can use it in any Gradio app.\n\nIn the rest of the guide, we will cover building themes programmatically.\n\n## Extending Themes via the Constructor\n\nAlthough each theme has hundreds of CSS variables, the values for most these variables are drawn from 8 core variables which can be set through the constructor of each prebuilt theme. Modifying these 8 arguments allows you to quickly change the look and feel of your app.\n\n### Core Colors\n\nThe first 3 constructor arguments set the colors of the theme and are `gradio.themes.Color` objects. Internally, these Color objects hold brightness values for the palette of a single hue, ranging from 50, 100, 200..., 800, 900, 950. Other CSS variables are derived from these 3 colors.\n\nThe 3 color constructor arguments are:\n\n- `primary_hue`: This is the color draws attention in your theme. In the default theme, this is set to `gradio.themes.colors.orange`.\n- `secondary_hue`: This is the color that is used for secondary elements in your theme. In the default theme, this is set to `gradio.themes.colors.blue`.\n- `neutral_hue`: This is the color that is used for text and other neutral elements in your theme. In the default theme, this is set to `gradio.themes.colors.gray`.\n\nYou could modify these values using their string shortcuts, such as\n\n```python\nwith gr.Blocks(theme=gr.themes.Default(primary_hue=\"red\", secondary_hue=\"pink\")) as demo:\n    ...\n```\n\nor you could use the `Color` objects directly, like this:\n\n```python\nwith gr.Blocks(theme=gr.themes.Default(primary_hue=gr.themes.colors.red, secondary_hue=gr.themes.colors.pink)) as demo:\n    ...\n```\n\n\u003Cdiv class=\"wrapper\">\n\u003Ciframe\n\tsrc=\"https://gradio-theme-extended-step-1.hf.space?__theme=light\"\n\tframeborder=\"0\"\n>\u003C/iframe>\n\u003C/div>\n\nPredefined colors are:\n\n- `slate`\n- `gray`\n- `zinc`\n- `neutral`\n- `stone`\n- `red`\n- `orange`\n- `amber`\n- `yellow`\n- `lime`\n- `green`\n- `emerald`\n- `teal`\n- `cyan`\n- `sky`\n- `blue`\n- `indigo`\n- `violet`\n- `purple`\n- `fuchsia`\n- `pink`\n- `rose`\n\nYou could also create your own custom `Color` objects and pass them in.\n\n### Core Sizing\n\nThe next 3 constructor arguments set the sizing of the theme and are `gradio.themes.Size` objects. Internally, these Size objects hold pixel size values that range from `xxs` to `xxl`. Other CSS variables are derived from these 3 sizes.\n\n- `spacing_size`: This sets the padding within and spacing between elements. In the default theme, this is set to `gradio.themes.sizes.spacing_md`.\n- `radius_size`: This sets the roundedness of corners of elements. In the default theme, this is set to `gradio.themes.sizes.radius_md`.\n- `text_size`: This sets the font size of text. In the default theme, this is set to `gradio.themes.sizes.text_md`.\n\nYou could modify these values using their string shortcuts, such as\n\n```python\nwith gr.Blocks(theme=gr.themes.Default(spacing_size=\"sm\", radius_size=\"none\")) as demo:\n    ...\n```\n\nor you could use the `Size` objects directly, like this:\n\n```python\nwith gr.Blocks(theme=gr.themes.Default(spacing_size=gr.themes.sizes.spacing_sm, radius_size=gr.themes.sizes.radius_none)) as demo:\n    ...\n```\n\n\u003Cdiv class=\"wrapper\">\n\u003Ciframe\n\tsrc=\"https://gradio-theme-extended-step-2.hf.space?__theme=light\"\n\tframeborder=\"0\"\n>\u003C/iframe>\n\u003C/div>\n\nThe predefined size objects are:\n\n- `radius_none`\n- `radius_sm`\n- `radius_md`\n- `radius_lg`\n- `spacing_sm`\n- `spacing_md`\n- `spacing_lg`\n- `text_sm`\n- `text_md`\n- `text_lg`\n\nYou could also create your own custom `Size` objects and pass them in.\n\n### Core Fonts\n\nThe final 2 constructor arguments set the fonts of the theme. You can pass a list of fonts to each of these arguments to specify fallbacks. If you provide a string, it will be loaded as a system font. If you provide a `gradio.themes.GoogleFont`, the font will be loaded from Google Fonts.\n\n- `font`: This sets the primary font of the theme. In the default theme, this is set to `gradio.themes.GoogleFont(\"Source Sans Pro\")`.\n- `font_mono`: This sets the monospace font of the theme. In the default theme, this is set to `gradio.themes.GoogleFont(\"IBM Plex Mono\")`.\n\nYou could modify these values such as the following:\n\n```python\nwith gr.Blocks(theme=gr.themes.Default(font=[gr.themes.GoogleFont(\"Inconsolata\"), \"Arial\", \"sans-serif\"])) as demo:\n    ...\n```\n\n\u003Cdiv class=\"wrapper\">\n\u003Ciframe\n\tsrc=\"https://gradio-theme-extended-step-3.hf.space?__theme=light\"\n\tframeborder=\"0\"\n>\u003C/iframe>\n\u003C/div>\n\n## Extending Themes via `.set()`\n\nYou can also modify the values of CSS variables after the theme has been loaded. To do so, use the `.set()` method of the theme object to get access to the CSS variables. For example:\n\n```python\ntheme = gr.themes.Default(primary_hue=\"blue\").set(\n    loader_color=\"#FF0000\",\n    slider_color=\"#FF0000\",\n)\n\nwith gr.Blocks(theme=theme) as demo:\n    ...\n```\n\nIn the example above, we've set the `loader_color` and `slider_color` variables to `#FF0000`, despite the overall `primary_color` using the blue color palette. You can set any CSS variable that is defined in the theme in this manner.\n\nYour IDE type hinting should help you navigate these variables. Since there are so many CSS variables, let's take a look at how these variables are named and organized.\n\n### CSS Variable Naming Conventions\n\nCSS variable names can get quite long, like `button_primary_background_fill_hover_dark`! However they follow a common naming convention that makes it easy to understand what they do and to find the variable you're looking for. Separated by underscores, the variable name is made up of:\n\n1. The target element, such as `button`, `slider`, or `block`.\n2. The target element type or sub-element, such as `button_primary`, or `block_label`.\n3. The property, such as `button_primary_background_fill`, or `block_label_border_width`.\n4. Any relevant state, such as `button_primary_background_fill_hover`.\n5. If the value is different in dark mode, the suffix `_dark`. For example, `input_border_color_focus_dark`.\n\nOf course, many CSS variable names are shorter than this, such as `table_border_color`, or `input_shadow`.\n\n### CSS Variable Organization\n\nThough there are hundreds of CSS variables, they do not all have to have individual values. They draw their values by referencing a set of core variables and referencing each other. This allows us to only have to modify a few variables to change the look and feel of the entire theme, while also getting finer control of individual elements that we may want to modify.\n\n#### Referencing Core Variables\n\nTo reference one of the core constructor variables, precede the variable name with an asterisk. To reference a core color, use the `*primary_`, `*secondary_`, or `*neutral_` prefix, followed by the brightness value. For example:\n\n```python\ntheme = gr.themes.Default(primary_hue=\"blue\").set(\n    button_primary_background_fill=\"*primary_200\",\n    button_primary_background_fill_hover=\"*primary_300\",\n)\n```\n\nIn the example above, we've set the `button_primary_background_fill` and `button_primary_background_fill_hover` variables to `*primary_200` and `*primary_300`. These variables will be set to the 200 and 300 brightness values of the blue primary color palette, respectively.\n\nSimilarly, to reference a core size, use the `*spacing_`, `*radius_`, or `*text_` prefix, followed by the size value. For example:\n\n```python\ntheme = gr.themes.Default(radius_size=\"md\").set(\n    button_primary_border_radius=\"*radius_xl\",\n)\n```\n\nIn the example above, we've set the `button_primary_border_radius` variable to `*radius_xl`. This variable will be set to the `xl` setting of the medium radius size range.\n\n#### Referencing Other Variables\n\nVariables can also reference each other. For example, look at the example below:\n\n```python\ntheme = gr.themes.Default().set(\n    button_primary_background_fill=\"#FF0000\",\n    button_primary_background_fill_hover=\"#FF0000\",\n    button_primary_border=\"#FF0000\",\n)\n```\n\nHaving to set these values to a common color is a bit tedious. Instead, we can reference the `button_primary_background_fill` variable in the `button_primary_background_fill_hover` and `button_primary_border` variables, using a `*` prefix.\n\n```python\ntheme = gr.themes.Default().set(\n    button_primary_background_fill=\"#FF0000\",\n    button_primary_background_fill_hover=\"*button_primary_background_fill\",\n    button_primary_border=\"*button_primary_background_fill\",\n)\n```\n\nNow, if we change the `button_primary_background_fill` variable, the `button_primary_background_fill_hover` and `button_primary_border` variables will automatically update as well.\n\nThis is particularly useful if you intend to share your theme - it makes it easy to modify the theme without having to change every variable.\n\nNote that dark mode variables automatically reference each other. For example:\n\n```python\ntheme = gr.themes.Default().set(\n    button_primary_background_fill=\"#FF0000\",\n    button_primary_background_fill_dark=\"#AAAAAA\",\n    button_primary_border=\"*button_primary_background_fill\",\n    button_primary_border_dark=\"*button_primary_background_fill_dark\",\n)\n```\n\n`button_primary_border_dark` will draw its value from `button_primary_background_fill_dark`, because dark mode always draw from the dark version of the variable.\n\n## Creating a Full Theme\n\nLet's say you want to create a theme from scratch! We'll go through it step by step - you can also see the source of prebuilt themes in the gradio source repo for reference - [here's the source](https://github.com/gradio-app/gradio/blob/main/gradio/themes/monochrome.py) for the Monochrome theme.\n\nOur new theme class will inherit from `gradio.themes.Base`, a theme that sets a lot of convenient defaults. Let's make a simple demo that creates a dummy theme called Seafoam, and make a simple app that uses it.\n\n```python\nimport gradio as gr\nfrom gradio.themes.base import Base\nimport time\n\nclass Seafoam(Base):\n    pass\n\nseafoam = Seafoam()\n\nwith gr.Blocks(theme=seafoam) as demo:\n    textbox = gr.Textbox(label=\"Name\")\n    slider = gr.Slider(label=\"Count\", minimum=0, maximum=100, step=1)\n    with gr.Row():\n        button = gr.Button(\"Submit\", variant=\"primary\")\n        clear = gr.Button(\"Clear\")\n    output = gr.Textbox(label=\"Output\")\n\n    def repeat(name, count):\n        time.sleep(3)\n        return name * count\n    \n    button.click(repeat, [textbox, slider], output)\n\ndemo.launch()\n```\n\n\u003Cdiv class=\"wrapper\">\n\u003Ciframe\n\tsrc=\"https://gradio-theme-new-step-1.hf.space?__theme=light\"\n\tframeborder=\"0\"\n>\u003C/iframe>\n\u003C/div>\n\nThe Base theme is very barebones, and uses `gr.themes.Blue` as it primary color - you'll note the primary button and the loading animation are both blue as a result. Let's change the defaults core arguments of our app. We'll overwrite the constructor and pass new defaults for the core constructor arguments.\n\nWe'll use `gr.themes.Emerald` as our primary color, and set secondary and neutral hues to `gr.themes.Blue`. We'll make our text larger using `text_lg`. We'll use `Quicksand` as our default font, loaded from Google Fonts.\n\n```python\nfrom __future__ import annotations\nfrom typing import Iterable\nimport gradio as gr\nfrom gradio.themes.base import Base\nfrom gradio.themes.utils import colors, fonts, sizes\nimport time\n\n\nclass Seafoam(Base):\n    def __init__(\n        self,\n        *,\n        primary_hue: colors.Color | str = colors.emerald,\n        secondary_hue: colors.Color | str = colors.blue,\n        neutral_hue: colors.Color | str = colors.gray,\n        spacing_size: sizes.Size | str = sizes.spacing_md,\n        radius_size: sizes.Size | str = sizes.radius_md,\n        text_size: sizes.Size | str = sizes.text_lg,\n        font: fonts.Font\n        | str\n        | Iterable[fonts.Font | str] = (\n            fonts.GoogleFont(\"Quicksand\"),\n            \"ui-sans-serif\",\n            \"sans-serif\",\n        ),\n        font_mono: fonts.Font\n        | str\n        | Iterable[fonts.Font | str] = (\n            fonts.GoogleFont(\"IBM Plex Mono\"),\n            \"ui-monospace\",\n            \"monospace\",\n        ),\n    ):\n        super().__init__(\n            primary_hue=primary_hue,\n            secondary_hue=secondary_hue,\n            neutral_hue=neutral_hue,\n            spacing_size=spacing_size,\n            radius_size=radius_size,\n            text_size=text_size,\n            font=font,\n            font_mono=font_mono,\n        )\n\n\nseafoam = Seafoam()\n\nwith gr.Blocks(theme=seafoam) as demo:\n    textbox = gr.Textbox(label=\"Name\")\n    slider = gr.Slider(label=\"Count\", minimum=0, maximum=100, step=1)\n    with gr.Row():\n        button = gr.Button(\"Submit\", variant=\"primary\")\n        clear = gr.Button(\"Clear\")\n    output = gr.Textbox(label=\"Output\")\n\n    def repeat(name, count):\n        time.sleep(3)\n        return name * count\n\n    button.click(repeat, [textbox, slider], output)\n\ndemo.launch()\n\n```\n\n\u003Cdiv class=\"wrapper\">\n\u003Ciframe\n\tsrc=\"https://gradio-theme-new-step-2.hf.space?__theme=light\"\n\tframeborder=\"0\"\n>\u003C/iframe>\n\u003C/div>\n\nSee how the primary button and the loading animation are now green? These CSS variables are tied to the `primary_hue` variable.\n\nLet's modify the theme a bit more directly. We'll call the `set()` method to overwrite CSS variable values explicitly. We can use any CSS logic, and reference our core constructor arguments using the `*` prefix.\n\n```python\nfrom __future__ import annotations\nfrom typing import Iterable\nimport gradio as gr\nfrom gradio.themes.base import Base\nfrom gradio.themes.utils import colors, fonts, sizes\nimport time\n\n\nclass Seafoam(Base):\n    def __init__(\n        self,\n        *,\n        primary_hue: colors.Color | str = colors.emerald,\n        secondary_hue: colors.Color | str = colors.blue,\n        neutral_hue: colors.Color | str = colors.blue,\n        spacing_size: sizes.Size | str = sizes.spacing_md,\n        radius_size: sizes.Size | str = sizes.radius_md,\n        text_size: sizes.Size | str = sizes.text_lg,\n        font: fonts.Font\n        | str\n        | Iterable[fonts.Font | str] = (\n            fonts.GoogleFont(\"Quicksand\"),\n            \"ui-sans-serif\",\n            \"sans-serif\",\n        ),\n        font_mono: fonts.Font\n        | str\n        | Iterable[fonts.Font | str] = (\n            fonts.GoogleFont(\"IBM Plex Mono\"),\n            \"ui-monospace\",\n            \"monospace\",\n        ),\n    ):\n        super().__init__(\n            primary_hue=primary_hue,\n            secondary_hue=secondary_hue,\n            neutral_hue=neutral_hue,\n            spacing_size=spacing_size,\n            radius_size=radius_size,\n            text_size=text_size,\n            font=font,\n            font_mono=font_mono,\n        )\n        super().set(\n            body_background_fill=\"repeating-linear-gradient(45deg, *primary_200, *primary_200 10px, *primary_50 10px, *primary_50 20px)\",\n            body_background_fill_dark=\"repeating-linear-gradient(45deg, *primary_800, *primary_800 10px, *primary_900 10px, *primary_900 20px)\",\n            button_primary_background_fill=\"linear-gradient(90deg, *primary_300, *secondary_400)\",\n            button_primary_background_fill_hover=\"linear-gradient(90deg, *primary_200, *secondary_300)\",\n            button_primary_text_color=\"white\",\n            button_primary_background_fill_dark=\"linear-gradient(90deg, *primary_600, *secondary_800)\",\n            slider_color=\"*secondary_300\",\n            slider_color_dark=\"*secondary_600\",\n            block_title_text_weight=\"600\",\n            block_border_width=\"3px\",\n            block_shadow=\"*shadow_drop_lg\",\n            button_shadow=\"*shadow_drop_lg\",\n            button_large_padding=\"32px\",\n        )\n\n\nseafoam = Seafoam()\n\nwith gr.Blocks(theme=seafoam) as demo:\n    textbox = gr.Textbox(label=\"Name\")\n    slider = gr.Slider(label=\"Count\", minimum=0, maximum=100, step=1)\n    with gr.Row():\n        button = gr.Button(\"Submit\", variant=\"primary\")\n        clear = gr.Button(\"Clear\")\n    output = gr.Textbox(label=\"Output\")\n\n    def repeat(name, count):\n        time.sleep(3)\n        return name * count\n\n    button.click(repeat, [textbox, slider], output)\n\ndemo.launch()\n\n```\n\n\u003Cdiv class=\"wrapper\">\n\u003Ciframe\n\tsrc=\"https://gradio-theme-new-step-3.hf.space?__theme=light\"\n\tframeborder=\"0\"\n>\u003C/iframe>\n\u003C/div>\n\nLook how fun our theme looks now! With just a few variable changes, our theme looks completely different.\n\nYou may find it helpful to explore the [source code of the other prebuilt themes](https://github.com/gradio-app/gradio/blob/main/gradio/themes) to see how they modified the base theme. You can also find your browser's Inspector useful to select elements from the UI and see what CSS variables are being used in the styles panel.\n\n## Sharing Themes\n\nOnce you have created a theme, you can upload it to the HuggingFace Hub to let others view it, use it, and build off of it!\n\n### Uploading a Theme\n\nThere are two ways to upload a theme, via the theme class instance or the command line. We will cover both of them with the previously created `seafoam` theme.\n\n- Via the class instance\n\nEach theme instance has a method called `push_to_hub` we can use to upload a theme to the HuggingFace hub.\n\n```python\nseafoam.push_to_hub(repo_name=\"seafoam\",\n                    version=\"0.0.1\",\n\t\t\t\t\thf_token=\"\u003Ctoken>\")\n```\n\n- Via the command line\n\nFirst save the theme to disk\n\n```python\nseafoam.dump(filename=\"seafoam.json\")\n```\n\nThen use the `upload_theme` command:\n\n```bash\nupload_theme\\\n\"seafoam.json\"\\\n\"seafoam\"\\\n--version \"0.0.1\"\\\n--hf_token \"\u003Ctoken>\"\n```\n\nIn order to upload a theme, you must have a HuggingFace account and pass your [Access Token](https://huggingface.co/docs/huggingface_hub/quick-start#login)\nas the `hf_token` argument. However, if you log in via the [HuggingFace command line](https://huggingface.co/docs/huggingface_hub/quick-start#login) (which comes installed with `gradio`),\nyou can omit the `hf_token` argument.\n\nThe `version` argument lets you specify a valid [semantic version](https://www.geeksforgeeks.org/introduction-semantic-versioning/) string for your theme.\nThat way your users are able to specify which version of your theme they want to use in their apps. This also lets you publish updates to your theme without worrying\nabout changing how previously created apps look. The `version` argument is optional. If omitted, the next patch version is automatically applied.\n\n### Theme Previews\n\nBy calling `push_to_hub` or `upload_theme`, the theme assets will be stored in a [HuggingFace space](https://huggingface.co/docs/hub/spaces-overview).\n\nThe theme preview for our seafoam theme is here: [seafoam preview](https://huggingface.co/spaces/gradio/seafoam).\n\n\u003Cdiv class=\"wrapper\">\n\u003Ciframe\n\tsrc=\"https://gradio-seafoam.hf.space?__theme=light\"\n\tframeborder=\"0\"\n>\u003C/iframe>\n\u003C/div>\n\n### Discovering Themes\n\nThe [Theme Gallery](https://huggingface.co/spaces/gradio/theme-gallery) shows all the public gradio themes. After publishing your theme,\nit will automatically show up in the theme gallery after a couple of minutes.\n\nYou can sort the themes by the number of likes on the space and from most to least recently created as well as toggling themes between light and dark mode.\n\n\u003Cdiv class=\"wrapper\">\n\u003Ciframe\n\tsrc=\"https://gradio-theme-gallery.static.hf.space\"\n\tframeborder=\"0\"\n>\u003C/iframe>\n\u003C/div>\n\n### Downloading\n\nTo use a theme from the hub, use the `from_hub` method on the `ThemeClass` and pass it to your app:\n\n```python\nmy_theme = gr.Theme.from_hub(\"gradio/seafoam\")\n\nwith gr.Blocks(theme=my_theme) as demo:\n    ....\n```\n\nYou can also pass the theme string directly to `Blocks` or `Interface` (`gr.Blocks(theme=\"gradio/seafoam\")`)\n\nYou can pin your app to an upstream theme version by using semantic versioning expressions.\n\nFor example, the following would ensure the theme we load from the `seafoam` repo was between versions `0.0.1` and `0.1.0`:\n\n```python\nwith gr.Blocks(theme=\"gradio/seafoam@>=0.0.1,\u003C0.1.0\") as demo:\n    ....\n```\n\nEnjoy creating your own themes! If you make one you're proud of, please share it with the world by uploading it to the hub!\nIf you tag us on [Twitter](https://twitter.com/gradio) we can give your theme a shout out!\n\n\u003Cstyle>\n.wrapper {\n    position: relative;\n    padding-bottom: 56.25%;\n    padding-top: 25px;\n    height: 0;\n}\n.wrapper iframe {\n    position: absolute;\n    top: 0;\n    left: 0;\n    width: 100%;\n    height: 100%;\n}\n\u003C/style>\n",tags:["THEMES"],spaces:[],url:"/guides/theming-guide/",contributor:null},{name:"using-flagging",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:65,pretty_name:"Using Flagging",content:"# Using Flagging\n\n\n\n\n## Introduction\n\nWhen you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these \"hard\" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust.\n\nGradio simplifies the collection of this data by including a **Flag** button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`.\n\n## The **Flag** button in `gradio.Interface`\n\nFlagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a button marked **Flag**. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file.\n\nThere are [four parameters](https://gradio.app/docs/interface#initialization) in `gradio.Interface` that control how flagging works. We will go over them in greater detail.\n\n- `allow_flagging`: this parameter can be set to either `\"manual\"` (default), `\"auto\"`, or `\"never\"`.\n  - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked.\n  - `auto`: users will not see a button to flag, but every sample will be flagged automatically.\n  - `never`: users will not see a button to flag, and no sample will be flagged.\n- `flagging_options`: this parameter can be either `None` (default) or a list of strings.\n  - If `None`, then the user simply clicks on the **Flag** button and no additional options are shown.\n  - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\"Incorrect\", \"Ambiguous\"]`, then buttons labeled **Flag as Incorrect** and **Flag as Ambiguous** appear. This only applies if `allow_flagging` is `\"manual\"`.\n  - The chosen option is then logged along with the input and output.\n- `flagging_dir`: this parameter takes a string.\n  - It represents what to name the directory where flagged data is stored.\n- `flagging_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\n  - Using this parameter allows you to write custom code that gets run when the flag button is clicked\n  - By default, this is set to an instance of `gr.CSVLogger`\n  - One example is setting it to an instance of `gr.HuggingFaceDatasetSaver` which can allow you to pipe any flagged data into a HuggingFace Dataset. (See more below.)\n\n## What happens to flagged data?\n\nWithin the directory provided by the `flagging_dir` argument, a CSV file will log the flagged data.\n\nHere's an example: The code below creates the calculator interface embedded below it:\n\n```python\nimport gradio as gr\n\n\ndef calculator(num1, operation, num2):\n    if operation == \"add\":\n        return num1 + num2\n    elif operation == \"subtract\":\n        return num1 - num2\n    elif operation == \"multiply\":\n        return num1 * num2\n    elif operation == \"divide\":\n        return num1 / num2\n\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\"\n)\n\niface.launch()\n```\n\n\u003Cgradio-app space=\"gradio/calculator-flag-basic/\">\u003C/gradio-app>\n\nWhen you click the flag button above, the directory where the interface was launched will include a new flagged subfolder, with a csv file inside it. This csv file includes all the data that was flagged.\n\n```directory\n+-- flagged/\n|   +-- logs.csv\n```\n\n_flagged/logs.csv_\n\n```csv\nnum1,operation,num2,Output,timestamp\n5,add,7,12,2022-01-31 11:40:51.093412\n6,subtract,1.5,4.5,2022-01-31 03:25:32.023542\n```\n\nIf the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example an `image` input to `image` output interface will create the following structure.\n\n```directory\n+-- flagged/\n|   +-- logs.csv\n|   +-- image/\n|   |   +-- 0.png\n|   |   +-- 1.png\n|   +-- Output/\n|   |   +-- 0.png\n|   |   +-- 1.png\n```\n\n_flagged/logs.csv_\n\n```csv\nim,Output timestamp\nim/0.png,Output/0.png,2022-02-04 19:49:58.026963\nim/1.png,Output/1.png,2022-02-02 10:40:51.093412\n```\n\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging_options` argument of Interface. Users will have to select one of these choices when flagging, and the option will be saved as an additional column to the CSV.\n\nIf we go back to the calculator example, the following code will create the interface embedded below it.\n\n```python\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"]\n)\n\niface.launch()\n```\n\n\u003Cgradio-app space=\"gradio/calculator-flagging-options/\">\u003C/gradio-app>\n\nWhen users click the flag button, the csv file will now include a column indicating the selected option.\n\n_flagged/logs.csv_\n\n```csv\nnum1,operation,num2,Output,flag,timestamp\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\n```\n\n## The HuggingFaceDatasetSaver Callback\n\nSometimes, saving the data to a local CSV file doesn't make sense. For example, on Hugging Face\nSpaces, developers typically don't have access to the underlying ephemeral machine hosting the Gradio\ndemo. That's why, by default, flagging is turned off in Hugging Face Space. However,\nyou may want to do something else with the flagged data.\n\nWe've made this super easy with the `flagging_callback` parameter.\n\nFor example, below we're going to pipe flagged data from our calculator example into a Hugging Face Dataset, e.g. so that we can build a \"crowd-sourced\" dataset:\n\n```python\nimport os\n\nHF_TOKEN = os.getenv('HF_TOKEN')\nhf_writer = gr.HuggingFaceDatasetSaver(HF_TOKEN, \"crowdsourced-calculator-demo\")\n\niface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    description=\"Check out the crowd-sourced dataset at: [https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)\",\n    allow_flagging=\"manual\",\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"],\n    flagging_callback=hf_writer\n)\n\niface.launch()\n```\n\nNotice that we define our own\ninstance of `gradio.HuggingFaceDatasetSaver` using our Hugging Face token and\nthe name of a dataset we'd like to save samples to. In addition, we also set `allow_flagging=\"manual\"`\nbecause on Hugging Face Spaces, `allow_flagging` is set to `\"never\"` by default. Here's our demo:\n\n\u003Cgradio-app space=\"gradio/calculator-flagging-crowdsourced/\">\u003C/gradio-app>\n\nYou can now see all the examples flagged above in this [public Hugging Face dataset](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo).\n\n![flagging callback hf](https://github.com/gradio-app/gradio/blob/main/guides/assets/flagging-callback-hf.png?raw=true)\n\nWe created the `gradio.HuggingFaceDatasetSaver` class, but you can pass your own custom class as long as it inherits from `FLaggingCallback` defined in [this file](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py). If you create a cool callback, contribute it to the repo!\n\n## Flagging with Blocks\n\nWhat about if you are using `gradio.Blocks`? On one hand, you have even more flexibility\nwith Blocks -- you can write whatever Python code you want to run when a button is clicked,\nand assign that using the built-in events in Blocks.\n\nAt the same time, you might want to use an existing `FlaggingCallback` to avoid writing extra code.\nThis requires two steps:\n\n1. You have to run your callback's `.setup()` somewhere in the code prior to the\n   first time you flag data\n2. When the flagging button is clicked, then you trigger the callback's `.flag()` method,\n   making sure to collect the arguments correctly and disabling the typical preprocessing.\n\nHere is an example with an image sepia filter Blocks demo that lets you flag\ndata using the default `CSVLogger`:\n\n```python\nimport numpy as np\nimport gradio as gr\n\ndef sepia(input_img, strength):\n    sepia_filter = strength * np.array(\n        [[0.393, 0.769, 0.189], [0.349, 0.686, 0.168], [0.272, 0.534, 0.131]]\n    ) + (1-strength) * np.identity(3)\n    sepia_img = input_img.dot(sepia_filter.T)\n    sepia_img /= sepia_img.max()\n    return sepia_img\n\ncallback = gr.CSVLogger()\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            img_input = gr.Image()\n            strength = gr.Slider(0, 1, 0.5)\n        img_output = gr.Image()\n    with gr.Row():\n        btn = gr.Button(\"Flag\")\n        \n    # This needs to be called at some point prior to the first call to callback.flag()\n    callback.setup([img_input, strength, img_output], \"flagged_data_points\")\n\n    img_input.change(sepia, [img_input, strength], img_output)\n    strength.change(sepia, [img_input, strength], img_output)\n    \n    # We can choose which components to flag -- in this case, we'll flag all of them\n    btn.click(lambda *args: callback.flag(args), [img_input, strength, img_output], None, preprocess=False)\n\ndemo.launch()\n\n```\n\u003Cgradio-app space='gradio/blocks_flag'>\u003C/gradio-app>\n\n## Privacy\n\nImportant Note: please make sure your users understand when the data they submit is being saved, and what you plan on doing with it. This is especially important when you use `allow_flagging=auto` (when all of the data submitted through the demo is being flagged)\n\n### That's all! Happy building :)\n",tags:["FLAGGING","DATA"],spaces:["https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced","https://huggingface.co/spaces/gradio/calculator-flagging-options","https://huggingface.co/spaces/gradio/calculator-flag-basic"],url:"/guides/using-flagging/",contributor:null},{name:"wrapping-layouts",category:"other-tutorials",pretty_category:"Other Tutorials",guide_index:null,absolute_index:66,pretty_name:"Wrapping Layouts",content:"# Wrapping Layouts\n\n\n\n## Introduction\n\nGradio features [blocks](https://www.gradio.app/docs/blocks) to easily layout applications. To use this feature, you need to stack or nest layout components and create a hierarchy with them. This isn't difficult to implement and maintain for small projects, but after the project gets more complex, this component hierarchy becomes difficult to maintain and reuse.\n\nIn this guide, we are going to explore how we can wrap the layout classes to create more maintainable and easy-to-read applications without sacrificing flexibility.\n\n## Example\n\nWe are going to follow the implementation from this Huggingface Space example:\n\n\u003Cgradio-app\nspace=\"WoWoWoWololo/wrapping-layouts\">\n\u003C/gradio-app>\n\n## Implementation\n\nThe wrapping utility has two important classes. The first one is the ```LayoutBase``` class and the other one is the ```Application``` class.\n\nWe are going to look at the ```render``` and ```attach_event``` functions of them for brevity. You can look at the full implementation from [the example code](https://huggingface.co/spaces/WoWoWoWololo/wrapping-layouts/blob/main/app.py).\n\nSo let's start with the ```LayoutBase``` class.\n\n### LayoutBase Class\n\n1. Render Function\n\n    Let's look at the ```render``` function in the ```LayoutBase``` class:\n\n```python\n# other LayoutBase implementations\n\ndef render(self) -> None:\n    with self.main_layout:\n        for renderable in self.renderables:\n            renderable.render()\n\n    self.main_layout.render()\n```\nThis is a little confusing at first but if you consider the default implementation you can understand it easily.\nLet's look at an example:\n\nIn the default implementation, this is what we're doing:\n\n```python\nwith Row():\n    left_textbox = Textbox(value=\"left_textbox\")\n    right_textbox = Textbox(value=\"right_textbox\")\n```\n\nNow, pay attention to the Textbox variables. These variables' ```render``` parameter is true by default. So as we use the ```with``` syntax and create these variables, they are calling the ```render``` function under the ```with``` syntax.\n\nWe know the render function is called in the constructor with the implementation from the ```gradio.blocks.Block``` class:\n\n```python\nclass Block:\n    # constructor parameters are omitted for brevity\n    def __init__(self, ...):\n        # other assign functions \n\n        if render:\n            self.render()\n```\n\nSo our implementation looks like this:\n\n```python\n# self.main_layout -> Row()\nwith self.main_layout:\n    left_textbox.render()\n    right_textbox.render()\n```\n\nWhat this means is by calling the components' render functions under the ```with``` syntax, we are actually simulating the default implementation.\n\nSo now let's consider two nested ```with```s to see how the outer one works. For this, let's expand our example with the ```Tab``` component:\n\n```python\nwith Tab():\n    with Row():\n        first_textbox = Textbox(value=\"first_textbox\")\n        second_textbox = Textbox(value=\"second_textbox\")\n```\n\nPay attention to the Row and Tab components this time. We have created the Textbox variables above and added them to Row with the ```with``` syntax. Now we need to add the Row component to the Tab component. You can see that the Row component is created with default parameters, so its render parameter is true, that's why the render function is going to be executed under the Tab component's ```with``` syntax.\n\nTo mimic this implementation, we need to call the ```render``` function of the ```main_layout``` variable after the ```with``` syntax of the ```main_layout``` variable.\n\nSo the implementation looks like this:\n\n```python\nwith tab_main_layout:\n    with row_main_layout:\n        first_textbox.render()\n        second_textbox.render()\n\n    row_main_layout.render()\n\ntab_main_layout.render()\n```\n\nThe default implementation and our implementation are the same, but we are using the render function ourselves. So it requires a little work.\n\nNow, let's take a look at the ```attach_event``` function.\n\n2. Attach Event Function\n\n    The function is left as not implemented because it is specific to the class, so each class has to implement its `attach_event` function.\n\n```python\n    # other LayoutBase implementations\n\n    def attach_event(self, block_dict: Dict[str, Block]) -> None:\n        raise NotImplementedError\n```\n\nCheck out the ```block_dict``` variable in the ```Application``` class's ```attach_event``` function.\n\n### Application Class\n\n1. Render Function\n\n```python\n    # other Application implementations\n\n    def _render(self):\n        with self.app:\n            for child in self.children:\n                child.render()\n\n        self.app.render()\n```\n\nFrom the explanation of the ```LayoutBase``` class's ```render``` function, we can understand the ```child.render``` part.\n\nSo let's look at the bottom part, why are we calling the ```app``` variable's ```render``` function? It's important to call this function because if we look at the implementation in the ```gradio.blocks.Blocks``` class, we can see that it is adding the components and event functions into the root component. To put it another way, it is creating and structuring the gradio application.\n\n2. Attach Event Function\n\n    Let's see how we can attach events to components:\n\n```python\n    # other Application implementations\n\n    def _attach_event(self):\n        block_dict: Dict[str, Block] = {}\n\n        for child in self.children:\n            block_dict.update(child.global_children_dict)\n\n        with self.app:\n            for child in self.children:\n                try:\n                    child.attach_event(block_dict=block_dict)\n                except NotImplementedError:\n                    print(f\"{child.name}'s attach_event is not implemented\")\n```\n\nYou can see why the ```global_children_list``` is used in the ```LayoutBase``` class from the example code. With this, all the components in the application are gathered into one dictionary, so the component can access all the components with their names.\n\nThe ```with``` syntax is used here again to attach events to components. If we look at the ```__exit__``` function in the ```gradio.blocks.Blocks``` class, we can see that it is calling the ```attach_load_events``` function which is used for setting event triggers to components. So we have to use the ```with``` syntax to trigger the ```__exit__``` function.\n\nOf course, we can call ```attach_load_events``` without using the ```with``` syntax, but the function needs a ```Context.root_block```, and it is set in the ```__enter__``` function. So we used the ```with``` syntax here rather than calling the function ourselves.\n\n## Conclusion\n\nIn this guide, we saw\n\n- How we can wrap the layouts\n- How components are rendered\n- How we can structure our application with wrapped layout classes\n\nBecause the classes used in this guide are used for demonstration purposes, they may still not be totally optimized or modular. But that would make the guide much longer!\n\nI hope this guide helps you gain another view of the layout classes and gives you an idea about how you can use them for your needs. See the full implementation of our example [here](https://huggingface.co/spaces/WoWoWoWololo/wrapping-layouts/blob/main/app.py).\n",tags:["LAYOUTS"],spaces:[],url:"/guides/wrapping-layouts/",contributor:null}]}],total_guides:67,COLOR_SETS:["green","yellow","red","blue","pink","purple","green","yellow","red","blue","pink","purple"]},"uses":{"params":["version"]}}];

					Promise.all([
						import("./_app/immutable/entry/start.DeGkUWpD.js"),
						import("./_app/immutable/entry/app.zeLHmYJV.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 2, 4],
							data,
							form: null,
							error: null
						});
					});
				}
   </script>
  </div>
 </body>
</html>
